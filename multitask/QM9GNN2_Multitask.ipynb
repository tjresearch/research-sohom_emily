{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.backend import mean, square\n",
    "\n",
    "from spektral.datasets import qm9\n",
    "from spektral.layers import EdgeConditionedConv, GlobalAttentionPool\n",
    "from spektral.utils import label_to_one_hot\n",
    "\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QM9 dataset.\n",
      "Reading SDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133885/133885 [00:38<00:00, 3471.19it/s]\n"
     ]
    }
   ],
   "source": [
    "A_complete, X_complete, E_complete, y_complete = qm9.load_data(\n",
    "    return_type='numpy',\n",
    "    nf_keys='atomic_num',\n",
    "    ef_keys='type',\n",
    "    self_loops=True,\n",
    "    amount=None # Set to None to train on whole dataset\n",
    ")\n",
    "# one-hot labeling of atoms\n",
    "uniq_X = np.unique(X_complete)\n",
    "X_complete = label_to_one_hot(X_complete, uniq_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, X, E = list(), list(), list()\n",
    "y = y_complete.sample(10000)\n",
    "for index, row in y.iterrows():\n",
    "    A.append(A_complete[index])\n",
    "    X.append(X_complete[index])\n",
    "    E.append(E_complete[index])\n",
    "A = np.stack(A, axis=0)\n",
    "X = np.stack(X, axis=0)\n",
    "E = np.stack(E, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = list(y.columns)[1:]\n",
    "num_tasks = len(tasks)\n",
    "y_list = []\n",
    "for task in tasks:\n",
    "    y_list.append(y[[task]].values)\n",
    "    \n",
    "key_to_index = dict(zip(tasks, range(len(tasks))))\n",
    "key_to_mean = dict()\n",
    "key_to_std = dict()\n",
    "for task in tasks:\n",
    "    key_to_mean[task] = np.mean(y[[task]].values)\n",
    "    key_to_std[task] = np.std(y[[task]].values)\n",
    "\n",
    "# Transforms the output values to have mean 0 and variance 1\n",
    "for i in range(len(y_list)):\n",
    "    y_list[i] = StandardScaler().fit_transform(y_list[i]).reshape(-1, y_list[-1].shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [['A', 'B', 'alpha'], \n",
    "            ['C', 'r2', 'u0'],\n",
    "            ['zpve', 'g298', 'cv'],\n",
    "            ['lumo', 'u298', 'h298'],\n",
    "            ['mu', 'homo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[-2]           # Number of nodes in the graphs\n",
    "F = X.shape[-1]           # Node features dimensionality\n",
    "S = E.shape[-1]           # Edge features dimensionality\n",
    "n_out = y_list[0].shape[-1]    # Dimensionality of the target\n",
    "learning_rate = 1e-3      # Learning rate for SGD\n",
    "epochs = 25               # Number of training epochs # formerly 25\n",
    "batch_size = 64           # Batch size\n",
    "es_patience = 5           # Patience fot early stopping\n",
    "soft = False\n",
    "soft_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train, A_test, \\\n",
    "X_train, X_test, \\\n",
    "E_train, E_test, \\\n",
    "*y_train_test_list = train_test_split(A, X, E, *y_list, test_size = 0.1)\n",
    "\n",
    "y_train_list = y_train_test_list[::2]\n",
    "y_test_list = y_train_test_list[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_in = Input(shape=(N, F))\n",
    "A_in = Input(shape=(N, N))\n",
    "E_in = Input(shape=(N, N, S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_task_model():\n",
    "    gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "    gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    pool = GlobalAttentionPool(256)(gc2)\n",
    "    dense = Dense(256, activation='relu')(pool)\n",
    "    output = Dense(n_out)(dense)\n",
    "    return Model(inputs=[X_in, A_in, E_in], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hard_parameter_sharing_model(num_tasks=1):\n",
    "    gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "    gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    pool = GlobalAttentionPool(256)(gc2)\n",
    "    dense_list = [Dense(256, activation='relu')(pool) for i in range(num_tasks)]\n",
    "    output_list = [Dense(n_out)(dense_layer) for dense_layer in dense_list]\n",
    "    return Model(inputs=[X_in, A_in, E_in], outputs=output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soft_paramter_sharing_model_and_loss(soft_weight, num_tasks=1):\n",
    "    gc1_list = [EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in]) for i in range(num_tasks)]\n",
    "    gc2_list = [EdgeConditionedConv(128, activation='relu')([gc1_layer, A_in, E_in]) for gc1_layer in gc1_list]\n",
    "    pool_list = [GlobalAttentionPool(256)(gc2_layer) for gc2_layer in gc2_list]\n",
    "    dense_list = [Dense(256, activation='relu')(pool_layer) for pool_layer in pool_list]\n",
    "    output_list = [Dense(n_out)(dense_layer) for dense_layer in dense_list]\n",
    "    model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "    \n",
    "    def loss(y_true, y_pred):\n",
    "        avg_layer_diff = 0\n",
    "        for i in range(len(dense_list)):\n",
    "            for j in range(i):\n",
    "                avg_layer_diff += mean(square(dense_list[i]-dense_list[j]))\n",
    "        avg_layer_diff /= len(dense_list)\n",
    "        return mean(square(y_pred - y_true)) + soft_weight*avg_layer_diff\n",
    "    \n",
    "    return Model(inputs=[X_in, A_in, E_in], outputs=output_list), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_filename(tasks):\n",
    "    tasks_str = \"\".join(sorted(tasks))\n",
    "    return path.join('demo_models', tasks_str + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_helper_filename(task):\n",
    "    return path.join('demo_models', task + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_multitask_model(tasks, y_train_list):\n",
    "    model = create_hard_parameter_sharing_model(len(tasks))\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "    es_callback = EarlyStopping(monitor='val_loss', patience=es_patience)\n",
    "    training_set = [y_train_list[key_to_index[task]] for task in tasks]\n",
    "    model.fit([X_train, A_train, E_train],\n",
    "             training_set,\n",
    "             batch_size=batch_size,\n",
    "             validation_split=0.1,\n",
    "             epochs=epochs,\n",
    "             callbacks=[es_callback])\n",
    "    model.save_weights(generate_filename(tasks))\n",
    "    for task in tasks:\n",
    "        helper_file = generate_helper_filename(task)\n",
    "        with open(helper_file, 'w') as file:\n",
    "            print(key_to_mean[task], file=file)\n",
    "            print(key_to_std[task], file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_multitask_model(tasks, y_test_list):\n",
    "    model = create_hard_parameter_sharing_model(len(tasks))\n",
    "    model.load_weights(generate_model_filename(tasks))\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "    testing_set = [y_test_list[key_to_index[task]] for task in tasks]\n",
    "    eval_results = model.evaluate([X_test, A_test, E_test], testing_set, batch_size=batch_size)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_property(prop, mol_id):\n",
    "    for cluster in clusters:\n",
    "        if prop in cluster:\n",
    "            model = create_hard_parameter_sharing_model(len(cluster))\n",
    "            model.load_weights(generate_model_filename(cluster))\n",
    "            model.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "            predictions = model.predict([[X_complete[mol_id-1]], [A_complete[mol_id-1]], [E_complete[mol_id-1]]])\n",
    "            mean, std = 0, 1\n",
    "            with open(generate_helper_filename(prop), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                mean = float(lines[0].strip())\n",
    "                std = float(lines[1].strip())\n",
    "            prediction = mean + std * predictions[1 + cluster.index(prop)]\n",
    "            return prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    for cluster in clusters:\n",
    "        train_and_save_multitask_model(cluster, y_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "1000/1000 [==============================] - 3s 3ms/step\n",
      "[0.7527519102096558, 0.42298720276355745, 0.271389740228653, 0.05837496376037598]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    print(load_and_evaluate_multitask_model(['A', 'B', 'alpha'], y_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.059586\n",
      "5.20531\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    print(calculate_property('A', 13333))\n",
    "    print(y_complete.loc[13333 - 1, 'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam(lr=learning_rate)\n",
    "# if soft:\n",
    "#     model, loss = create_soft_paramter_sharing_model_and_loss(X_in, A_in, E_in, soft_weight)\n",
    "#     model.compile(optimizer=optimizer, loss=loss)\n",
    "# else:\n",
    "#     model = create_hard_paramter_sharing_model(X_in, A_in, E_in)\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# es_callback = EarlyStopping(monitor='val_loss', patience=es_patience)\n",
    "\n",
    "# model.fit([X_train, A_train, E_train],\n",
    "#           y_train_list,\n",
    "#           batch_size=batch_size,\n",
    "#           validation_split=0.1,\n",
    "#           epochs=epochs,\n",
    "#           callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Evaluating model.')\n",
    "# eval_results = model.evaluate([X_test, A_test, E_test],\n",
    "#                               y_test_list,\n",
    "#                               batch_size=batch_size)\n",
    "# print('Done.\\n'\n",
    "#       'Test loss: {}'.format(eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model.predict([X_test, A_test, E_test])\n",
    "\n",
    "# if num_tasks == 1:\n",
    "#     preds = np.transpose(preds)\n",
    "\n",
    "# for i in range(num_tasks):\n",
    "#     plt.figure()\n",
    "#     plt.scatter(preds[i], y_test_list[i], alpha=0.3)\n",
    "#     plt.plot()\n",
    "#     plt.title(tasks[i])\n",
    "#     plt.xlabel('Predicted')\n",
    "#     plt.ylabel('Actual')\n",
    "#     # plt.savefig('graphs/' + '11_5_'+tasks[i]+'_multitask')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
