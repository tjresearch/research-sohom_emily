{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os import path\n",
    "from time import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.backend import mean, square\n",
    "\n",
    "from spektral.datasets import qm9\n",
    "from spektral.layers import EdgeConditionedConv, GlobalSumPool, GlobalAttentionPool\n",
    "from spektral.utils import label_to_one_hot\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(amount=None):\n",
    "    A_all, X_all, E_all, y_all = qm9.load_data(return_type='numpy',\n",
    "                               nf_keys='atomic_num',\n",
    "                               ef_keys='type',\n",
    "                               self_loops=True,\n",
    "                               amount=amount) # None for entire dataset\n",
    "    # Preprocessing\n",
    "    X_uniq = np.unique(X_all)\n",
    "    X_uniq = X_uniq[X_uniq != 0]\n",
    "    E_uniq = np.unique(E_all)\n",
    "    E_uniq = E_uniq[E_uniq != 0]\n",
    "    \n",
    "    X_all = label_to_one_hot(X_all, X_uniq)\n",
    "    E_all = label_to_one_hot(E_all, E_uniq)\n",
    "    \n",
    "    return A_all, X_all, E_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_data(sample_size, A_all, X_all, E_all, y_all):\n",
    "    indices = np.random.choice(X_all.shape[0], sample_size, replace=False)\n",
    "    A = A_all[indices, :, :]\n",
    "    X = X_all[indices, :, :]\n",
    "    E = E_all[indices, :, :, :]\n",
    "    y = y_all.iloc[indices, :].copy()\n",
    "    \n",
    "    return A, X, E, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(y):\n",
    "    task_to_scaler = dict()\n",
    "    for task in list(y.columns)[1:]:\n",
    "        scaler = PowerTransformer()\n",
    "        y.loc[:, task] = scaler.fit_transform(y[[task]])\n",
    "        task_to_scaler[task] = scaler\n",
    "    return task_to_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_params(*, A, X, E):\n",
    "    N = X.shape[-2]       # Number of nodes in the graphs\n",
    "    F = X[0].shape[-1]    # Dimension of node features\n",
    "    S = E[0].shape[-1]    # Dimension of edge features\n",
    "    \n",
    "    return N, F, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tensors(*, A, X, E):\n",
    "    N, F, S = get_shape_params(A=A, X=X, E=E)\n",
    "    X_in = Input(shape=(N, F))\n",
    "    A_in = Input(shape=(N, N))\n",
    "    E_in = Input(shape=(N, N, S))\n",
    "    \n",
    "    return X_in, A_in, E_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_task_model(*, A, X, E, learning_rate=1e-3, loss='mse'):\n",
    "    X_in, A_in, E_in = get_input_tensors(A=A, X=X, E=E)\n",
    "\n",
    "    gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "    gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    pool = GlobalAttentionPool(256)(gc2)\n",
    "    dense = Dense(256, activation='relu')(pool)\n",
    "    output = Dense(1)(dense)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=[X_in, A_in, E_in], outputs=output)\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hard_sharing_model(*, A, X, E, num_tasks, \n",
    "                             learning_rate=1e-3, loss='mse'):\n",
    "    X_in, A_in, E_in = get_input_tensors(A=A, X=X, E=E)\n",
    "\n",
    "    gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "    gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    pool = GlobalAttentionPool(256)(gc2)\n",
    "    dense_list = [Dense(256, activation='relu')(pool) \n",
    "                  for i in range(num_tasks)]\n",
    "    output_list = [Dense(1)(dense_layer) for dense_layer in dense_list]\n",
    "\n",
    "    model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_soft_sharing_model(*, A, X, E, num_tasks, share_param, \n",
    "                             learning_rate=1e-3, loss='mse'):\n",
    "    X_in, A_in, E_in = get_input_tensors(A=A, X=X, E=E)\n",
    "\n",
    "    gc1_list = [EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in]) \n",
    "                for i in range(num_tasks)]\n",
    "    gc2_list = [EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in]) \n",
    "                for gc1 in gc1_list]\n",
    "    pool_list = [GlobalAttentionPool(256)(gc2) for gc2 in gc2_list]\n",
    "    dense_list = [Dense(256, activation='relu')(pool) for pool in pool_list]\n",
    "    output_list = [Dense(1)(dense) for dense in dense_list]\n",
    "\n",
    "    def loss(y_actual, y_pred):\n",
    "        avg_layer_diff = 0\n",
    "        for i, j in itertools.combinations(range(num_tasks), 2):\n",
    "            for gc in [gc1_list, gc2_list]:\n",
    "                diff = gc[i].trainable_weights - gc[j].trainable_weights\n",
    "                avg_layer_diff += mean(square(diff))\n",
    "        avg_layer_diff /= (num_tasks)*(num_tasks-1)/2  \n",
    "        return mean(square(y_actual - y_pred)) + share_param*avg_layer_diff\n",
    "\n",
    "    model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_filename(tasks, folder_path='demo_models'):\n",
    "    filename = \"\".join(sorted(tasks))\n",
    "    return path.join(folder_path, f'{filename}.h5')\n",
    "\n",
    "def generate_task_scaler_filename(task, folder_path='demo_models'):\n",
    "    return path.join(folder_path, f'{task}_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tasks, task_to_scaler):\n",
    "    model.save_weights(generate_model_filename(tasks))\n",
    "    for task in tasks:\n",
    "        scaler_filename = generate_task_scaler_filename(task)\n",
    "        with open(scaler_filename, 'wb') as f:\n",
    "            scaler = task_to_scaler[task]\n",
    "            pickle.dump(obj=scaler, file=f)\n",
    "\n",
    "def load_hard_sharing_model(*, A, X, E, tasks, task_to_scaler=dict()):\n",
    "    model = build_hard_sharing_model(A=A, X=X, E=E, num_tasks=len(tasks))\n",
    "    model.load_weights(generate_model_filename(tasks))\n",
    "    for task in tasks:\n",
    "        if task not in task_to_scaler:\n",
    "            with open(generate_task_scaler_filename(task), 'rb') as f:\n",
    "                task_to_scaler[task] = pickle.load(f)\n",
    "    return model, task_to_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_property(prop, mol_id, clusters, *, X_all, A_all, E_all, \n",
    "                     model=None, task_to_scaler=dict()):\n",
    "    cluster = [c for c in clusters if prop in c][0]\n",
    "    if model is None:\n",
    "        model, task_to_scaler = load_hard_sharing_model(\n",
    "            A=A_all, X=X_all, E=E_all, tasks=cluster, \n",
    "            task_to_scaler=task_to_scaler\n",
    "        )\n",
    "    i = mol_id - 1\n",
    "\n",
    "    # convert shape for batch mode\n",
    "    def wrap(a):\n",
    "        return a.reshape([1] + list(a.shape))\n",
    "    x = list(map(wrap, [X_all[i], A_all[i], E_all[i]]))\n",
    "\n",
    "    cluster_prediction = model.predict(x)       \n",
    "    prediction = cluster_prediction[cluster.index(prop)]\n",
    "    prediction = task_to_scaler[prop].inverse_transform(prediction)\n",
    "    return prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():    \n",
    "    A_all, X_all, E_all, y_all = load_data()\n",
    "    N, F, S = get_shape_params(A=A_all, X=X_all, E=E_all)\n",
    "    # n_out = y_all.shape[-1]   # Dimension of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals(): \n",
    "    A, X, E, y = sample_from_data(1000, A_all, X_all, E_all, y_all)\n",
    "    task_to_scaler = standardize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [['A', 'B', 'alpha'], \n",
    "            ['C', 'r2', 'u0'],\n",
    "            ['zpve', 'g298', 'cv'],\n",
    "            ['lumo', 'u298', 'h298'],\n",
    "            ['mu', 'homo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():     \n",
    "    A_train, A_test, \\\n",
    "        X_train, X_test, \\\n",
    "        E_train, E_test, \\\n",
    "        y_train, y_test = train_test_split(A, X, E, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    print('begin training models')\n",
    "    for cluster in clusters:\n",
    "        print(f'training {cluster}')\n",
    "        model = build_hard_sharing_model(\n",
    "            A=A_train, X=X_train, E=E_train, num_tasks=len(cluster)\n",
    "        )\n",
    "        y_train_cluster = np.hsplit(y_train[cluster].values, len(cluster))\n",
    "        model.fit(x=[X_train, A_train, E_train], \n",
    "                  y=y_train_cluster,\n",
    "                  batch_size=32,\n",
    "                  validation_split=0.1,\n",
    "                  epochs=25)\n",
    "        save_model(model, cluster, task_to_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():   \n",
    "    for cluster in clusters:\n",
    "        model, task_to_scaler = load_hard_sharing_model(\n",
    "            A=A_test, X=X_test, E=E_test, tasks=cluster, \n",
    "            task_to_scaler=task_to_scaler\n",
    "        )\n",
    "        y_test_cluster = np.hsplit(y_test[cluster].values, len(cluster))\n",
    "        model_loss = model.evaluate(x=[X_test, A_test, E_test],\n",
    "                                    y=y_test_cluster)\n",
    "        print(f\"Test loss on {cluster}: {model_loss}\")\n",
    "        \n",
    "        cluster_pred = model.predict([X_test, A_test, E_test])\n",
    "        for prop, batch_pred in zip(cluster, cluster_pred):\n",
    "            batch_pred = task_to_scaler[prop].inverse_transform(batch_pred)\n",
    "            errors = list()\n",
    "            for index, pred in zip(y_test.index.values, batch_pred):\n",
    "                actual = y_all.loc[index, prop]\n",
    "                err = abs((pred-actual)/actual)\n",
    "                errors.append(err[0])\n",
    "            print(f'Avg error of {prop} is {sum(errors)/len(errors):.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
