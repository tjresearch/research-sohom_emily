{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run QM9GNN2_Multitask.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_hyperparameter_search(trials=50):\n",
    "    A_all, X_all, E_all, y_all = load_data(amount=None, mode=mode)\n",
    "    for _ in trials:\n",
    "        mode = 'batch'\n",
    "        conv = 'ecc'\n",
    "        batch_size = random.choice([16, 32, 64])\n",
    "        learning_rate = random.choice([1e-2, 1e-3, 1e-4])\n",
    "        epochs = random.choice([20, 30, 40])\n",
    "        layer_sizes = [random.choice([16, 32, 64, 128, 256, 512]), \n",
    "               random.choice([16, 32, 64, 128, 256, 512]), \n",
    "               random.choice([16, 32, 64, 128, 256, 512]), \n",
    "               random.choice([64, 128, 256, 512])]\n",
    "        num_sampled = 30000\n",
    "        scaler = random.choice(['power_transformer', 'standard_scaler'])\n",
    "        loss_fn = random.choice([MeanAbsoluteError(), MeanSquaredError()])\n",
    "        soft_sharing = random.random() < 0.5\n",
    "        share_param = random.choice([1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3])\n",
    "    \n",
    "        A, X, E, y = sample_from_data(30000,\n",
    "                                     A_all,\n",
    "                                     X_all,\n",
    "                                     E_all,\n",
    "                                     y_all,\n",
    "                                     mode=mode)\n",
    "        task_to_scaler = standardize(y, scaler=scaler)\n",
    "        \n",
    "        cluster = ['r2', 'cv']\n",
    "        A_train, A_test, \\\n",
    "        X_train, X_test, \\\n",
    "        E_train, E_test, \\\n",
    "        y_train, y_test = train_test_split(A, X, E, y, test_size=0.1)\n",
    "        \n",
    "        if soft_sharing:\n",
    "            model, _ = build_soft_sharing_model(A=A_train,\n",
    "                                                X=X_train,\n",
    "                                                E=E_train,\n",
    "                                                num_tasks=len(cluster),\n",
    "                                                share_param=share_param\n",
    "                                                mode=mode,\n",
    "                                                conv=conv,\n",
    "                                                layer_sizes=layer_sizes)\n",
    "        else:\n",
    "            model, _ = build_hard_sharing_model(A=A_train,\n",
    "                                                X=X_train,\n",
    "                                                E=E_train,\n",
    "                                                num_tasks=len(cluster),\n",
    "                                                mode=mode,\n",
    "                                                conv=conv,\n",
    "                                                layer_sizes=layer_sizes)\n",
    "        \n",
    "        stream = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
    "        summary = stream.getvalue()\n",
    "        \n",
    "        params = {'mode': mode,\n",
    "                 'conv': conv,\n",
    "                 'batch_size': batch_size,\n",
    "                 'epochs': epochs,\n",
    "                 'num_sampled': num_sampled,\n",
    "                 'learning_rate': learning_rate,\n",
    "                 'cluster': cluster,\n",
    "                 'hard_sharing': not soft_sharing,\n",
    "                 'share_param': share_param,\n",
    "                 'model_summary': summary,\n",
    "                 'loss_fn': type(loss_fn).__name__,\n",
    "                 'optimizer': type(optimizer).__name__}\n",
    "        model_data = ModelData(params=params)\n",
    "        \n",
    "        y_train_cluster = np.hsplit(y_train[cluster].values, len(cluster))\n",
    "        model.compile(optimizer=optimizer, \n",
    "                      loss=loss_fn)\n",
    "        model.fit(x=[X_train, A_train, E_train], \n",
    "                  y=y_train_cluster,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_split=0.1,\n",
    "                  epochs=epochs,\n",
    "                  callbacks=[model_data.loss_logger])\n",
    "        \n",
    "        y_test_cluster = np.hsplit(y_test[cluster].values, len(cluster))\n",
    "        model_loss = model.evaluate(x=[X_test, A_test, E_test],\n",
    "                                    y=y_test_cluster)\n",
    "        model_data.test_loss = model_loss\n",
    "        cluster_pred = model.predict([X_test, A_test, E_test])\n",
    "\n",
    "        for prop, batch_pred in zip(cluster, cluster_pred):\n",
    "            batch_pred = task_to_scaler[prop].inverse_transform(batch_pred)\n",
    "            errors = list()\n",
    "            for index, pred in zip(y_test.index.values, batch_pred):\n",
    "                actual = y_all.loc[index, prop]\n",
    "                model_data.add_test(prop, actual, pred[0])\n",
    "        \n",
    "        dirname = 'model_data'\n",
    "        dt_string = self.timestamp.strftime('%d-%m-%Y_%H-%M-%S')\n",
    "        filename = path.join(dirname, 'hyperparam_search' + dt_string + '.pkl')\n",
    "        model_data.serialize(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    random_hyperparameter_search(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
