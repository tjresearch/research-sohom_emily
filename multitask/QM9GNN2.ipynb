{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.backend import mean, square\n",
    "\n",
    "from spektral.datasets import qm9\n",
    "from spektral.layers import EdgeConditionedConv, GlobalAttentionPool\n",
    "from spektral.utils import label_to_one_hot\n",
    "\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QM9 dataset.\n",
      "Reading SDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133885/133885 [00:38<00:00, 3520.21it/s]\n"
     ]
    }
   ],
   "source": [
    "A_complete, X_complete, E_complete, y_complete = qm9.load_data(return_type='numpy',\n",
    "                           nf_keys='atomic_num',\n",
    "                           ef_keys='type',\n",
    "                           self_loops=True,\n",
    "                           amount=None)  # Set to None to train on whole dataset\n",
    "# one-hot labeling of atoms\n",
    "uniq_X = np.unique(X_complete)\n",
    "X_complete = label_to_one_hot(X_complete, uniq_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mol_id</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>mu</th>\n",
       "      <th>alpha</th>\n",
       "      <th>homo</th>\n",
       "      <th>lumo</th>\n",
       "      <th>gap</th>\n",
       "      <th>r2</th>\n",
       "      <th>zpve</th>\n",
       "      <th>u0</th>\n",
       "      <th>u298</th>\n",
       "      <th>h298</th>\n",
       "      <th>g298</th>\n",
       "      <th>cv</th>\n",
       "      <th>u0_atom</th>\n",
       "      <th>u298_atom</th>\n",
       "      <th>h298_atom</th>\n",
       "      <th>g298_atom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>gdb_1</td>\n",
       "      <td>157.71180</td>\n",
       "      <td>157.709970</td>\n",
       "      <td>157.706990</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13.21</td>\n",
       "      <td>-0.3877</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.5048</td>\n",
       "      <td>35.3641</td>\n",
       "      <td>0.044749</td>\n",
       "      <td>-40.478930</td>\n",
       "      <td>-40.476062</td>\n",
       "      <td>-40.475117</td>\n",
       "      <td>-40.498597</td>\n",
       "      <td>6.469</td>\n",
       "      <td>-395.999595</td>\n",
       "      <td>-398.643290</td>\n",
       "      <td>-401.014647</td>\n",
       "      <td>-372.471772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>gdb_2</td>\n",
       "      <td>293.60975</td>\n",
       "      <td>293.541110</td>\n",
       "      <td>191.393970</td>\n",
       "      <td>1.6256</td>\n",
       "      <td>9.46</td>\n",
       "      <td>-0.2570</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.3399</td>\n",
       "      <td>26.1563</td>\n",
       "      <td>0.034358</td>\n",
       "      <td>-56.525887</td>\n",
       "      <td>-56.523026</td>\n",
       "      <td>-56.522082</td>\n",
       "      <td>-56.544961</td>\n",
       "      <td>6.316</td>\n",
       "      <td>-276.861363</td>\n",
       "      <td>-278.620271</td>\n",
       "      <td>-280.399259</td>\n",
       "      <td>-259.338802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>gdb_3</td>\n",
       "      <td>799.58812</td>\n",
       "      <td>437.903860</td>\n",
       "      <td>282.945450</td>\n",
       "      <td>1.8511</td>\n",
       "      <td>6.31</td>\n",
       "      <td>-0.2928</td>\n",
       "      <td>0.0687</td>\n",
       "      <td>0.3615</td>\n",
       "      <td>19.0002</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>-76.404702</td>\n",
       "      <td>-76.401867</td>\n",
       "      <td>-76.400922</td>\n",
       "      <td>-76.422349</td>\n",
       "      <td>6.002</td>\n",
       "      <td>-213.087624</td>\n",
       "      <td>-213.974294</td>\n",
       "      <td>-215.159658</td>\n",
       "      <td>-201.407171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>gdb_4</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>35.610036</td>\n",
       "      <td>35.610036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.28</td>\n",
       "      <td>-0.2845</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.3351</td>\n",
       "      <td>59.5248</td>\n",
       "      <td>0.026841</td>\n",
       "      <td>-77.308427</td>\n",
       "      <td>-77.305527</td>\n",
       "      <td>-77.304583</td>\n",
       "      <td>-77.327429</td>\n",
       "      <td>8.574</td>\n",
       "      <td>-385.501997</td>\n",
       "      <td>-387.237686</td>\n",
       "      <td>-389.016047</td>\n",
       "      <td>-365.800724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>gdb_5</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>44.593883</td>\n",
       "      <td>44.593883</td>\n",
       "      <td>2.8937</td>\n",
       "      <td>12.99</td>\n",
       "      <td>-0.3604</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>48.7476</td>\n",
       "      <td>0.016601</td>\n",
       "      <td>-93.411888</td>\n",
       "      <td>-93.409370</td>\n",
       "      <td>-93.408425</td>\n",
       "      <td>-93.431246</td>\n",
       "      <td>6.278</td>\n",
       "      <td>-301.820534</td>\n",
       "      <td>-302.906752</td>\n",
       "      <td>-304.091489</td>\n",
       "      <td>-288.720028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133880</td>\n",
       "      <td>gdb_133881</td>\n",
       "      <td>3.59483</td>\n",
       "      <td>2.198990</td>\n",
       "      <td>1.904230</td>\n",
       "      <td>1.6637</td>\n",
       "      <td>69.37</td>\n",
       "      <td>-0.2254</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>0.2842</td>\n",
       "      <td>760.7472</td>\n",
       "      <td>0.127406</td>\n",
       "      <td>-400.633868</td>\n",
       "      <td>-400.628599</td>\n",
       "      <td>-400.627654</td>\n",
       "      <td>-400.663098</td>\n",
       "      <td>23.658</td>\n",
       "      <td>-1603.983913</td>\n",
       "      <td>-1614.898804</td>\n",
       "      <td>-1623.788097</td>\n",
       "      <td>-1492.819438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133881</td>\n",
       "      <td>gdb_133882</td>\n",
       "      <td>3.65648</td>\n",
       "      <td>2.142370</td>\n",
       "      <td>1.904390</td>\n",
       "      <td>1.2976</td>\n",
       "      <td>69.52</td>\n",
       "      <td>-0.2393</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.3002</td>\n",
       "      <td>762.6354</td>\n",
       "      <td>0.127495</td>\n",
       "      <td>-400.629713</td>\n",
       "      <td>-400.624444</td>\n",
       "      <td>-400.623500</td>\n",
       "      <td>-400.658942</td>\n",
       "      <td>23.697</td>\n",
       "      <td>-1601.376613</td>\n",
       "      <td>-1612.291504</td>\n",
       "      <td>-1621.181424</td>\n",
       "      <td>-1490.211511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133882</td>\n",
       "      <td>gdb_133883</td>\n",
       "      <td>3.67118</td>\n",
       "      <td>2.143140</td>\n",
       "      <td>1.895010</td>\n",
       "      <td>1.2480</td>\n",
       "      <td>73.60</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>0.0720</td>\n",
       "      <td>0.2953</td>\n",
       "      <td>780.3553</td>\n",
       "      <td>0.140458</td>\n",
       "      <td>-380.753918</td>\n",
       "      <td>-380.748619</td>\n",
       "      <td>-380.747675</td>\n",
       "      <td>-380.783148</td>\n",
       "      <td>23.972</td>\n",
       "      <td>-1667.045429</td>\n",
       "      <td>-1678.830048</td>\n",
       "      <td>-1688.312964</td>\n",
       "      <td>-1549.143391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133883</td>\n",
       "      <td>gdb_133884</td>\n",
       "      <td>3.52845</td>\n",
       "      <td>2.151310</td>\n",
       "      <td>1.865820</td>\n",
       "      <td>1.9576</td>\n",
       "      <td>77.40</td>\n",
       "      <td>-0.2122</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.3003</td>\n",
       "      <td>803.1904</td>\n",
       "      <td>0.152222</td>\n",
       "      <td>-364.720374</td>\n",
       "      <td>-364.714974</td>\n",
       "      <td>-364.714030</td>\n",
       "      <td>-364.749650</td>\n",
       "      <td>24.796</td>\n",
       "      <td>-1794.600439</td>\n",
       "      <td>-1807.210860</td>\n",
       "      <td>-1817.286772</td>\n",
       "      <td>-1670.349892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133884</td>\n",
       "      <td>gdb_133885</td>\n",
       "      <td>3.64015</td>\n",
       "      <td>2.217640</td>\n",
       "      <td>1.937930</td>\n",
       "      <td>0.8626</td>\n",
       "      <td>69.48</td>\n",
       "      <td>-0.2316</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.3058</td>\n",
       "      <td>756.3557</td>\n",
       "      <td>0.127862</td>\n",
       "      <td>-400.633052</td>\n",
       "      <td>-400.627892</td>\n",
       "      <td>-400.626948</td>\n",
       "      <td>-400.662186</td>\n",
       "      <td>23.434</td>\n",
       "      <td>-1603.471865</td>\n",
       "      <td>-1614.455155</td>\n",
       "      <td>-1623.345075</td>\n",
       "      <td>-1492.247150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133885 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mol_id          A           B           C      mu  alpha    homo  \\\n",
       "0            gdb_1  157.71180  157.709970  157.706990  0.0000  13.21 -0.3877   \n",
       "1            gdb_2  293.60975  293.541110  191.393970  1.6256   9.46 -0.2570   \n",
       "2            gdb_3  799.58812  437.903860  282.945450  1.8511   6.31 -0.2928   \n",
       "3            gdb_4    0.00000   35.610036   35.610036  0.0000  16.28 -0.2845   \n",
       "4            gdb_5    0.00000   44.593883   44.593883  2.8937  12.99 -0.3604   \n",
       "...            ...        ...         ...         ...     ...    ...     ...   \n",
       "133880  gdb_133881    3.59483    2.198990    1.904230  1.6637  69.37 -0.2254   \n",
       "133881  gdb_133882    3.65648    2.142370    1.904390  1.2976  69.52 -0.2393   \n",
       "133882  gdb_133883    3.67118    2.143140    1.895010  1.2480  73.60 -0.2233   \n",
       "133883  gdb_133884    3.52845    2.151310    1.865820  1.9576  77.40 -0.2122   \n",
       "133884  gdb_133885    3.64015    2.217640    1.937930  0.8626  69.48 -0.2316   \n",
       "\n",
       "          lumo     gap        r2      zpve          u0        u298  \\\n",
       "0       0.1171  0.5048   35.3641  0.044749  -40.478930  -40.476062   \n",
       "1       0.0829  0.3399   26.1563  0.034358  -56.525887  -56.523026   \n",
       "2       0.0687  0.3615   19.0002  0.021375  -76.404702  -76.401867   \n",
       "3       0.0506  0.3351   59.5248  0.026841  -77.308427  -77.305527   \n",
       "4       0.0191  0.3796   48.7476  0.016601  -93.411888  -93.409370   \n",
       "...        ...     ...       ...       ...         ...         ...   \n",
       "133880  0.0588  0.2842  760.7472  0.127406 -400.633868 -400.628599   \n",
       "133881  0.0608  0.3002  762.6354  0.127495 -400.629713 -400.624444   \n",
       "133882  0.0720  0.2953  780.3553  0.140458 -380.753918 -380.748619   \n",
       "133883  0.0881  0.3003  803.1904  0.152222 -364.720374 -364.714974   \n",
       "133884  0.0742  0.3058  756.3557  0.127862 -400.633052 -400.627892   \n",
       "\n",
       "              h298        g298      cv      u0_atom    u298_atom    h298_atom  \\\n",
       "0       -40.475117  -40.498597   6.469  -395.999595  -398.643290  -401.014647   \n",
       "1       -56.522082  -56.544961   6.316  -276.861363  -278.620271  -280.399259   \n",
       "2       -76.400922  -76.422349   6.002  -213.087624  -213.974294  -215.159658   \n",
       "3       -77.304583  -77.327429   8.574  -385.501997  -387.237686  -389.016047   \n",
       "4       -93.408425  -93.431246   6.278  -301.820534  -302.906752  -304.091489   \n",
       "...            ...         ...     ...          ...          ...          ...   \n",
       "133880 -400.627654 -400.663098  23.658 -1603.983913 -1614.898804 -1623.788097   \n",
       "133881 -400.623500 -400.658942  23.697 -1601.376613 -1612.291504 -1621.181424   \n",
       "133882 -380.747675 -380.783148  23.972 -1667.045429 -1678.830048 -1688.312964   \n",
       "133883 -364.714030 -364.749650  24.796 -1794.600439 -1807.210860 -1817.286772   \n",
       "133884 -400.626948 -400.662186  23.434 -1603.471865 -1614.455155 -1623.345075   \n",
       "\n",
       "          g298_atom  \n",
       "0       -372.471772  \n",
       "1       -259.338802  \n",
       "2       -201.407171  \n",
       "3       -365.800724  \n",
       "4       -288.720028  \n",
       "...             ...  \n",
       "133880 -1492.819438  \n",
       "133881 -1490.211511  \n",
       "133882 -1549.143391  \n",
       "133883 -1670.349892  \n",
       "133884 -1492.247150  \n",
       "\n",
       "[133885 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, X, E = list(), list(), list()\n",
    "y = y_complete.sample(10000)\n",
    "for index, row in y.iterrows():\n",
    "    A.append(A_complete[index])\n",
    "    X.append(X_complete[index])\n",
    "    E.append(E_complete[index])\n",
    "A = np.stack(A, axis=0)\n",
    "X = np.stack(X, axis=0)\n",
    "E = np.stack(E, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = list(y.columns)[1:]\n",
    "num_tasks = len(tasks)\n",
    "y_list = []\n",
    "for task in tasks:\n",
    "    y_list.append(y[[task]].values)\n",
    "    \n",
    "key_to_index = dict(zip(tasks, range(len(tasks))))\n",
    "key_to_mean = dict()\n",
    "key_to_std = dict()\n",
    "for task in tasks:\n",
    "    key_to_mean[task] = np.mean(y[[task]].values)\n",
    "    key_to_std[task] = np.std(y[[task]].values)\n",
    "\n",
    "# Transforms the output values to have mean 0 and variance 1\n",
    "for i in range(len(y_list)):\n",
    "    y_list[i] = StandardScaler().fit_transform(y_list[i]).reshape(-1, y_list[-1].shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [['A', 'B', 'alpha'], \n",
    "               ['C', 'r2', 'u0'],\n",
    "               ['zpve', 'g298', 'cv'],\n",
    "               ['lumo', 'u298', 'h298'],\n",
    "               ['mu', 'homo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[-2]           # Number of nodes in the graphs\n",
    "F = X.shape[-1]           # Node features dimensionality\n",
    "S = E.shape[-1]           # Edge features dimensionality\n",
    "n_out = y_list[0].shape[-1]    # Dimensionality of the target\n",
    "learning_rate = 1e-3      # Learning rate for SGD\n",
    "epochs = 25               # Number of training epochs # formerly 25\n",
    "batch_size = 64           # Batch size\n",
    "es_patience = 5           # Patience fot early stopping\n",
    "soft = False\n",
    "soft_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train, A_test, \\\n",
    "X_train, X_test, \\\n",
    "E_train, E_test, \\\n",
    "*y_train_test_list = train_test_split(A, X, E, *y_list, test_size = 0.1)\n",
    "\n",
    "y_train_list = y_train_test_list[::2]\n",
    "y_test_list = y_train_test_list[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_in = Input(shape=(N, F))\n",
    "A_in = Input(shape=(N, N))\n",
    "E_in = Input(shape=(N, N, S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_task_model():\n",
    "    gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "    gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    pool = GlobalAttentionPool(256)(gc2)\n",
    "    dense = Dense(256, activation='relu')(pool)\n",
    "    output = Dense(n_out)(dense)\n",
    "    return Model(inputs=[X_in, A_in, E_in], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hard_parameter_sharing_model(num_tasks=1):\n",
    "    gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "    gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    pool = GlobalAttentionPool(256)(gc2)\n",
    "    dense_list = [Dense(256, activation='relu')(pool) for i in range(num_tasks)]\n",
    "    output_list = [Dense(n_out)(dense_layer) for dense_layer in dense_list]\n",
    "    return Model(inputs=[X_in, A_in, E_in], outputs=output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soft_paramter_sharing_model_and_loss(soft_weight, num_tasks=1):\n",
    "    gc1_list = [EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in]) for i in range(num_tasks)]\n",
    "    gc2_list = [EdgeConditionedConv(128, activation='relu')([gc1_layer, A_in, E_in]) for gc1_layer in gc1_list]\n",
    "    pool_list = [GlobalAttentionPool(256)(gc2_layer) for gc2_layer in gc2_list]\n",
    "    dense_list = [Dense(256, activation='relu')(pool_layer) for pool_layer in pool_list]\n",
    "    output_list = [Dense(n_out)(dense_layer) for dense_layer in dense_list]\n",
    "    model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "    \n",
    "    def loss(y_true, y_pred):\n",
    "        avg_layer_diff = 0\n",
    "        for i in range(len(dense_list)):\n",
    "            for j in range(i):\n",
    "                avg_layer_diff += mean(square(dense_list[i]-dense_list[j]))\n",
    "        avg_layer_diff /= len(dense_list)\n",
    "        return mean(square(y_pred - y_true)) + soft_weight*avg_layer_diff\n",
    "    \n",
    "    return Model(inputs=[X_in, A_in, E_in], outputs=output_list), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filename(tasks):\n",
    "    tasks_str = \"\".join(sorted(tasks))\n",
    "    return path.join('demo_models', tasks_str + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_helper_filename(task):\n",
    "    return path.join('demo_models', task + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_multitask_model(tasks, y_train_list):\n",
    "    model = create_hard_parameter_sharing_model(len(tasks))\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "    es_callback = EarlyStopping(monitor='val_loss', patience=es_patience)\n",
    "    training_set = [y_train_list[key_to_index[task]] for task in tasks]\n",
    "    model.fit([X_train, A_train, E_train],\n",
    "             training_set,\n",
    "             batch_size=batch_size,\n",
    "             validation_split=0.1,\n",
    "             epochs=epochs,\n",
    "             callbacks=[es_callback])\n",
    "    model.save_weights(generate_filename(tasks))\n",
    "    for task in tasks:\n",
    "        helper_file = generate_helper_filename(task)\n",
    "        with open(helper_file, 'w') as file:\n",
    "            print(key_to_mean[task], file=file)\n",
    "            print(key_to_std[task], file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_model(tasks, y_test_list):\n",
    "    model = create_hard_parameter_sharing_model(len(tasks))\n",
    "    model.load_weights(generate_filename(tasks))\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "    testing_set = [y_test_list[key_to_index[task]] for task in tasks]\n",
    "    eval_results = model.evaluate([X_test, A_test, E_test], testing_set, batch_size=batch_size)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_property(prop, mol_id):\n",
    "    for cluster in clusters:\n",
    "        if prop in cluster:\n",
    "            model = create_hard_parameter_sharing_model(len(cluster))\n",
    "            model.load_weights(generate_filename(cluster))\n",
    "            model.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "            predictions = model.predict([[X_complete[mol_id-1]], [A_complete[mol_id-1]], [E_complete[mol_id-1]]])\n",
    "            mean, std = 0, 1\n",
    "            with open(generate_helper_filename(prop), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                mean = float(lines[0].strip())\n",
    "                std = float(lines[1].strip())\n",
    "            prediction = mean + std * predictions[1 + cluster.index(prop)]\n",
    "            return prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/25\n",
      "8100/8100 [==============================] - 38s 5ms/step - loss: 2.4934 - dense_4_loss: 0.9974 - dense_5_loss: 0.9390 - dense_6_loss: 0.5570 - val_loss: 1.8669 - val_dense_4_loss: 0.7883 - val_dense_5_loss: 0.7776 - val_dense_6_loss: 0.3010\n",
      "Epoch 2/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.7017 - dense_4_loss: 0.7766 - dense_5_loss: 0.7079 - dense_6_loss: 0.2171 - val_loss: 1.5615 - val_dense_4_loss: 0.7969 - val_dense_5_loss: 0.5926 - val_dense_6_loss: 0.1719\n",
      "Epoch 3/25\n",
      "8100/8100 [==============================] - 37s 5ms/step - loss: 1.5212 - dense_4_loss: 0.7486 - dense_5_loss: 0.6078 - dense_6_loss: 0.1648 - val_loss: 1.3886 - val_dense_4_loss: 0.7609 - val_dense_5_loss: 0.5088 - val_dense_6_loss: 0.1189\n",
      "Epoch 4/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.4283 - dense_4_loss: 0.7368 - dense_5_loss: 0.5603 - dense_6_loss: 0.1311 - val_loss: 1.2996 - val_dense_4_loss: 0.7234 - val_dense_5_loss: 0.4641 - val_dense_6_loss: 0.1121\n",
      "Epoch 5/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.3449 - dense_4_loss: 0.7166 - dense_5_loss: 0.5225 - dense_6_loss: 0.1058 - val_loss: 1.3508 - val_dense_4_loss: 0.7093 - val_dense_5_loss: 0.5223 - val_dense_6_loss: 0.1192\n",
      "Epoch 6/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.3374 - dense_4_loss: 0.7121 - dense_5_loss: 0.5180 - dense_6_loss: 0.1073 - val_loss: 1.3013 - val_dense_4_loss: 0.7698 - val_dense_5_loss: 0.4434 - val_dense_6_loss: 0.0880\n",
      "Epoch 7/25\n",
      "8100/8100 [==============================] - 36s 4ms/step - loss: 1.2947 - dense_4_loss: 0.7106 - dense_5_loss: 0.4897 - dense_6_loss: 0.0944 - val_loss: 1.3016 - val_dense_4_loss: 0.7182 - val_dense_5_loss: 0.4967 - val_dense_6_loss: 0.0868\n",
      "Epoch 8/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.3014 - dense_4_loss: 0.7020 - dense_5_loss: 0.4947 - dense_6_loss: 0.1047 - val_loss: 1.2205 - val_dense_4_loss: 0.7069 - val_dense_5_loss: 0.4383 - val_dense_6_loss: 0.0753\n",
      "Epoch 9/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.2547 - dense_4_loss: 0.6970 - dense_5_loss: 0.4673 - dense_6_loss: 0.0904 - val_loss: 1.2472 - val_dense_4_loss: 0.7100 - val_dense_5_loss: 0.4509 - val_dense_6_loss: 0.0863\n",
      "Epoch 10/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.2130 - dense_4_loss: 0.6828 - dense_5_loss: 0.4507 - dense_6_loss: 0.0795 - val_loss: 1.2683 - val_dense_4_loss: 0.7736 - val_dense_5_loss: 0.4160 - val_dense_6_loss: 0.0786\n",
      "Epoch 11/25\n",
      "8100/8100 [==============================] - 37s 5ms/step - loss: 1.2652 - dense_4_loss: 0.7118 - dense_5_loss: 0.4575 - dense_6_loss: 0.0959 - val_loss: 1.3675 - val_dense_4_loss: 0.7191 - val_dense_5_loss: 0.4618 - val_dense_6_loss: 0.1866\n",
      "Epoch 12/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.1863 - dense_4_loss: 0.6785 - dense_5_loss: 0.4301 - dense_6_loss: 0.0777 - val_loss: 1.4721 - val_dense_4_loss: 0.7235 - val_dense_5_loss: 0.6251 - val_dense_6_loss: 0.1235\n",
      "Epoch 13/25\n",
      "8100/8100 [==============================] - 36s 4ms/step - loss: 1.2392 - dense_4_loss: 0.6773 - dense_5_loss: 0.4569 - dense_6_loss: 0.1051 - val_loss: 1.2085 - val_dense_4_loss: 0.7220 - val_dense_5_loss: 0.4070 - val_dense_6_loss: 0.0795\n",
      "Epoch 14/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.1784 - dense_4_loss: 0.6721 - dense_5_loss: 0.4233 - dense_6_loss: 0.0830 - val_loss: 1.1587 - val_dense_4_loss: 0.6872 - val_dense_5_loss: 0.4058 - val_dense_6_loss: 0.0657\n",
      "Epoch 15/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.1549 - dense_4_loss: 0.6611 - dense_5_loss: 0.4090 - dense_6_loss: 0.0848 - val_loss: 1.1106 - val_dense_4_loss: 0.6552 - val_dense_5_loss: 0.3826 - val_dense_6_loss: 0.0728\n",
      "Epoch 16/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.1573 - dense_4_loss: 0.6444 - dense_5_loss: 0.4239 - dense_6_loss: 0.0890 - val_loss: 1.1354 - val_dense_4_loss: 0.6556 - val_dense_5_loss: 0.4084 - val_dense_6_loss: 0.0715\n",
      "Epoch 17/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.1385 - dense_4_loss: 0.6314 - dense_5_loss: 0.4067 - dense_6_loss: 0.1004 - val_loss: 1.0753 - val_dense_4_loss: 0.6107 - val_dense_5_loss: 0.3811 - val_dense_6_loss: 0.0835\n",
      "Epoch 18/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.0869 - dense_4_loss: 0.6286 - dense_5_loss: 0.3751 - dense_6_loss: 0.0831 - val_loss: 1.1262 - val_dense_4_loss: 0.6605 - val_dense_5_loss: 0.3886 - val_dense_6_loss: 0.0772\n",
      "Epoch 19/25\n",
      "8100/8100 [==============================] - 36s 4ms/step - loss: 1.0738 - dense_4_loss: 0.6059 - dense_5_loss: 0.3782 - dense_6_loss: 0.0896 - val_loss: 1.0492 - val_dense_4_loss: 0.6367 - val_dense_5_loss: 0.3498 - val_dense_6_loss: 0.0627\n",
      "Epoch 20/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.0539 - dense_4_loss: 0.5984 - dense_5_loss: 0.3745 - dense_6_loss: 0.0810 - val_loss: 1.0021 - val_dense_4_loss: 0.5896 - val_dense_5_loss: 0.3522 - val_dense_6_loss: 0.0603\n",
      "Epoch 21/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.0072 - dense_4_loss: 0.5726 - dense_5_loss: 0.3625 - dense_6_loss: 0.0722 - val_loss: 0.9907 - val_dense_4_loss: 0.5705 - val_dense_5_loss: 0.3275 - val_dense_6_loss: 0.0926\n",
      "Epoch 22/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.0570 - dense_4_loss: 0.5774 - dense_5_loss: 0.3747 - dense_6_loss: 0.1048 - val_loss: 1.0168 - val_dense_4_loss: 0.5831 - val_dense_5_loss: 0.3677 - val_dense_6_loss: 0.0661\n",
      "Epoch 23/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.0230 - dense_4_loss: 0.5693 - dense_5_loss: 0.3677 - dense_6_loss: 0.0860 - val_loss: 0.9226 - val_dense_4_loss: 0.5420 - val_dense_5_loss: 0.3202 - val_dense_6_loss: 0.0604\n",
      "Epoch 24/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.9397 - dense_4_loss: 0.5427 - dense_5_loss: 0.3257 - dense_6_loss: 0.0712 - val_loss: 0.8940 - val_dense_4_loss: 0.5271 - val_dense_5_loss: 0.3085 - val_dense_6_loss: 0.0585\n",
      "Epoch 25/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.9650 - dense_4_loss: 0.5487 - dense_5_loss: 0.3389 - dense_6_loss: 0.0774 - val_loss: 0.9233 - val_dense_4_loss: 0.5417 - val_dense_5_loss: 0.3202 - val_dense_6_loss: 0.0615\n",
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/25\n",
      "8100/8100 [==============================] - 37s 5ms/step - loss: 2.0708 - dense_10_loss: 0.8172 - dense_11_loss: 0.8983 - dense_12_loss: 0.3552 - val_loss: 1.5351 - val_dense_10_loss: 0.5811 - val_dense_11_loss: 0.8725 - val_dense_12_loss: 0.0815\n",
      "Epoch 2/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.3701 - dense_10_loss: 0.5872 - dense_11_loss: 0.7126 - dense_12_loss: 0.0703 - val_loss: 1.2481 - val_dense_10_loss: 0.4755 - val_dense_11_loss: 0.7383 - val_dense_12_loss: 0.0343\n",
      "Epoch 3/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.0096 - dense_10_loss: 0.4546 - dense_11_loss: 0.5247 - dense_12_loss: 0.0303 - val_loss: 0.8475 - val_dense_10_loss: 0.3377 - val_dense_11_loss: 0.4790 - val_dense_12_loss: 0.0308\n",
      "Epoch 4/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8458 - dense_10_loss: 0.3772 - dense_11_loss: 0.4457 - dense_12_loss: 0.0229 - val_loss: 0.7795 - val_dense_10_loss: 0.3364 - val_dense_11_loss: 0.4311 - val_dense_12_loss: 0.0120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.7348 - dense_10_loss: 0.3331 - dense_11_loss: 0.3875 - dense_12_loss: 0.0141 - val_loss: 0.7036 - val_dense_10_loss: 0.2827 - val_dense_11_loss: 0.4118 - val_dense_12_loss: 0.0091\n",
      "Epoch 6/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.7249 - dense_10_loss: 0.3243 - dense_11_loss: 0.3869 - dense_12_loss: 0.0137 - val_loss: 0.6311 - val_dense_10_loss: 0.2657 - val_dense_11_loss: 0.3479 - val_dense_12_loss: 0.0176\n",
      "Epoch 7/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.6322 - dense_10_loss: 0.2844 - dense_11_loss: 0.3383 - dense_12_loss: 0.0095 - val_loss: 0.5794 - val_dense_10_loss: 0.2430 - val_dense_11_loss: 0.3294 - val_dense_12_loss: 0.0070\n",
      "Epoch 8/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.6328 - dense_10_loss: 0.2842 - dense_11_loss: 0.3403 - dense_12_loss: 0.0083 - val_loss: 0.5972 - val_dense_10_loss: 0.2499 - val_dense_11_loss: 0.3365 - val_dense_12_loss: 0.0108\n",
      "Epoch 9/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.6345 - dense_10_loss: 0.2874 - dense_11_loss: 0.3399 - dense_12_loss: 0.0072 - val_loss: 0.7408 - val_dense_10_loss: 0.3709 - val_dense_11_loss: 0.3620 - val_dense_12_loss: 0.0079\n",
      "Epoch 10/25\n",
      "8100/8100 [==============================] - 36s 4ms/step - loss: 0.5892 - dense_10_loss: 0.2704 - dense_11_loss: 0.3124 - dense_12_loss: 0.0063 - val_loss: 0.5528 - val_dense_10_loss: 0.2399 - val_dense_11_loss: 0.3093 - val_dense_12_loss: 0.0036\n",
      "Epoch 11/25\n",
      "8100/8100 [==============================] - 37s 5ms/step - loss: 0.6148 - dense_10_loss: 0.2786 - dense_11_loss: 0.3298 - dense_12_loss: 0.0065 - val_loss: 0.7349 - val_dense_10_loss: 0.3350 - val_dense_11_loss: 0.3917 - val_dense_12_loss: 0.0082\n",
      "Epoch 12/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.5649 - dense_10_loss: 0.2554 - dense_11_loss: 0.3002 - dense_12_loss: 0.0093 - val_loss: 0.5376 - val_dense_10_loss: 0.2336 - val_dense_11_loss: 0.3005 - val_dense_12_loss: 0.0034\n",
      "Epoch 13/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.5282 - dense_10_loss: 0.2428 - dense_11_loss: 0.2802 - dense_12_loss: 0.0051 - val_loss: 0.5851 - val_dense_10_loss: 0.2592 - val_dense_11_loss: 0.3222 - val_dense_12_loss: 0.0036\n",
      "Epoch 14/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.5274 - dense_10_loss: 0.2414 - dense_11_loss: 0.2779 - dense_12_loss: 0.0081 - val_loss: 0.5847 - val_dense_10_loss: 0.2512 - val_dense_11_loss: 0.3254 - val_dense_12_loss: 0.0081\n",
      "Epoch 15/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.5140 - dense_10_loss: 0.2392 - dense_11_loss: 0.2701 - dense_12_loss: 0.0046 - val_loss: 0.5646 - val_dense_10_loss: 0.2384 - val_dense_11_loss: 0.3230 - val_dense_12_loss: 0.0032\n",
      "Epoch 16/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.5779 - dense_10_loss: 0.2631 - dense_11_loss: 0.3026 - dense_12_loss: 0.0122 - val_loss: 0.4915 - val_dense_10_loss: 0.2114 - val_dense_11_loss: 0.2769 - val_dense_12_loss: 0.0032\n",
      "Epoch 17/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.5099 - dense_10_loss: 0.2379 - dense_11_loss: 0.2668 - dense_12_loss: 0.0052 - val_loss: 0.5044 - val_dense_10_loss: 0.2129 - val_dense_11_loss: 0.2826 - val_dense_12_loss: 0.0089\n",
      "Epoch 18/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.4599 - dense_10_loss: 0.2160 - dense_11_loss: 0.2395 - dense_12_loss: 0.0044 - val_loss: 0.5502 - val_dense_10_loss: 0.2333 - val_dense_11_loss: 0.3137 - val_dense_12_loss: 0.0032\n",
      "Epoch 19/25\n",
      "8100/8100 [==============================] - 36s 4ms/step - loss: 0.4901 - dense_10_loss: 0.2294 - dense_11_loss: 0.2565 - dense_12_loss: 0.0042 - val_loss: 0.4459 - val_dense_10_loss: 0.2031 - val_dense_11_loss: 0.2403 - val_dense_12_loss: 0.0026\n",
      "Epoch 20/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.4344 - dense_10_loss: 0.2070 - dense_11_loss: 0.2236 - dense_12_loss: 0.0038 - val_loss: 0.4454 - val_dense_10_loss: 0.1933 - val_dense_11_loss: 0.2493 - val_dense_12_loss: 0.0028\n",
      "Epoch 21/25\n",
      "8100/8100 [==============================] - 36s 4ms/step - loss: 0.4552 - dense_10_loss: 0.2147 - dense_11_loss: 0.2363 - dense_12_loss: 0.0042 - val_loss: 0.4508 - val_dense_10_loss: 0.2007 - val_dense_11_loss: 0.2471 - val_dense_12_loss: 0.0030\n",
      "Epoch 22/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.4650 - dense_10_loss: 0.2158 - dense_11_loss: 0.2454 - dense_12_loss: 0.0039 - val_loss: 0.6108 - val_dense_10_loss: 0.2705 - val_dense_11_loss: 0.3369 - val_dense_12_loss: 0.0034\n",
      "Epoch 23/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.4958 - dense_10_loss: 0.2257 - dense_11_loss: 0.2615 - dense_12_loss: 0.0085 - val_loss: 0.4562 - val_dense_10_loss: 0.2061 - val_dense_11_loss: 0.2454 - val_dense_12_loss: 0.0048\n",
      "Epoch 24/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.4359 - dense_10_loss: 0.2062 - dense_11_loss: 0.2250 - dense_12_loss: 0.0048 - val_loss: 0.4369 - val_dense_10_loss: 0.2011 - val_dense_11_loss: 0.2313 - val_dense_12_loss: 0.0045\n",
      "Epoch 25/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.4152 - dense_10_loss: 0.1969 - dense_11_loss: 0.2145 - dense_12_loss: 0.0038 - val_loss: 0.4950 - val_dense_10_loss: 0.2258 - val_dense_11_loss: 0.2670 - val_dense_12_loss: 0.0022\n",
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/25\n",
      "8100/8100 [==============================] - 38s 5ms/step - loss: 1.8244 - dense_16_loss: 0.5045 - dense_17_loss: 0.4302 - dense_18_loss: 0.8897 - val_loss: 1.2551 - val_dense_16_loss: 0.3156 - val_dense_17_loss: 0.1452 - val_dense_18_loss: 0.7943\n",
      "Epoch 2/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.9860 - dense_16_loss: 0.2861 - dense_17_loss: 0.0476 - dense_18_loss: 0.6523 - val_loss: 0.7455 - val_dense_16_loss: 0.1960 - val_dense_17_loss: 0.0430 - val_dense_18_loss: 0.5066\n",
      "Epoch 3/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.6477 - dense_16_loss: 0.1908 - dense_17_loss: 0.0472 - dense_18_loss: 0.4096 - val_loss: 0.4482 - val_dense_16_loss: 0.1734 - val_dense_17_loss: 0.0206 - val_dense_18_loss: 0.2542\n",
      "Epoch 4/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.2771 - dense_16_loss: 0.1081 - dense_17_loss: 0.0128 - dense_18_loss: 0.1562 - val_loss: 0.2602 - val_dense_16_loss: 0.0843 - val_dense_17_loss: 0.0183 - val_dense_18_loss: 0.1577\n",
      "Epoch 5/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.2005 - dense_16_loss: 0.0844 - dense_17_loss: 0.0086 - dense_18_loss: 0.1075 - val_loss: 0.1654 - val_dense_16_loss: 0.0691 - val_dense_17_loss: 0.0082 - val_dense_18_loss: 0.0881\n",
      "Epoch 6/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1903 - dense_16_loss: 0.0815 - dense_17_loss: 0.0084 - dense_18_loss: 0.1003 - val_loss: 0.1482 - val_dense_16_loss: 0.0688 - val_dense_17_loss: 0.0062 - val_dense_18_loss: 0.0733\n",
      "Epoch 7/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1585 - dense_16_loss: 0.0689 - dense_17_loss: 0.0070 - dense_18_loss: 0.0826 - val_loss: 0.1444 - val_dense_16_loss: 0.0557 - val_dense_17_loss: 0.0060 - val_dense_18_loss: 0.0828\n",
      "Epoch 8/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1344 - dense_16_loss: 0.0604 - dense_17_loss: 0.0058 - dense_18_loss: 0.0683 - val_loss: 0.1142 - val_dense_16_loss: 0.0514 - val_dense_17_loss: 0.0045 - val_dense_18_loss: 0.0583\n",
      "Epoch 9/25\n",
      "8100/8100 [==============================] - 37s 5ms/step - loss: 0.1275 - dense_16_loss: 0.0564 - dense_17_loss: 0.0056 - dense_18_loss: 0.0655 - val_loss: 0.1381 - val_dense_16_loss: 0.0525 - val_dense_17_loss: 0.0106 - val_dense_18_loss: 0.0750\n",
      "Epoch 10/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1365 - dense_16_loss: 0.0596 - dense_17_loss: 0.0057 - dense_18_loss: 0.0712 - val_loss: 0.1123 - val_dense_16_loss: 0.0455 - val_dense_17_loss: 0.0079 - val_dense_18_loss: 0.0589\n",
      "Epoch 11/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1352 - dense_16_loss: 0.0558 - dense_17_loss: 0.0058 - dense_18_loss: 0.0736 - val_loss: 0.1086 - val_dense_16_loss: 0.0427 - val_dense_17_loss: 0.0040 - val_dense_18_loss: 0.0619\n",
      "Epoch 12/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1107 - dense_16_loss: 0.0486 - dense_17_loss: 0.0051 - dense_18_loss: 0.0570 - val_loss: 0.0902 - val_dense_16_loss: 0.0375 - val_dense_17_loss: 0.0046 - val_dense_18_loss: 0.0481\n",
      "Epoch 13/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1338 - dense_16_loss: 0.0577 - dense_17_loss: 0.0074 - dense_18_loss: 0.0687 - val_loss: 0.1159 - val_dense_16_loss: 0.0400 - val_dense_17_loss: 0.0051 - val_dense_18_loss: 0.0708\n",
      "Epoch 14/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1065 - dense_16_loss: 0.0452 - dense_17_loss: 0.0045 - dense_18_loss: 0.0568 - val_loss: 0.1819 - val_dense_16_loss: 0.0937 - val_dense_17_loss: 0.0032 - val_dense_18_loss: 0.0850\n",
      "Epoch 15/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1151 - dense_16_loss: 0.0493 - dense_17_loss: 0.0040 - dense_18_loss: 0.0618 - val_loss: 0.0909 - val_dense_16_loss: 0.0358 - val_dense_17_loss: 0.0050 - val_dense_18_loss: 0.0501\n",
      "Epoch 16/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.0981 - dense_16_loss: 0.0412 - dense_17_loss: 0.0049 - dense_18_loss: 0.0519 - val_loss: 0.1383 - val_dense_16_loss: 0.0514 - val_dense_17_loss: 0.0071 - val_dense_18_loss: 0.0798\n",
      "Epoch 17/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.0959 - dense_16_loss: 0.0404 - dense_17_loss: 0.0044 - dense_18_loss: 0.0510 - val_loss: 0.1205 - val_dense_16_loss: 0.0490 - val_dense_17_loss: 0.0034 - val_dense_18_loss: 0.0681\n",
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/25\n",
      "8100/8100 [==============================] - 37s 5ms/step - loss: 1.3790 - dense_22_loss: 0.5674 - dense_23_loss: 0.4044 - dense_24_loss: 0.4073 - val_loss: 0.5972 - val_dense_22_loss: 0.4458 - val_dense_23_loss: 0.0764 - val_dense_24_loss: 0.0750\n",
      "Epoch 2/25\n",
      "8100/8100 [==============================] - 36s 4ms/step - loss: 0.4605 - dense_22_loss: 0.3857 - dense_23_loss: 0.0376 - dense_24_loss: 0.0372 - val_loss: 0.3525 - val_dense_22_loss: 0.3202 - val_dense_23_loss: 0.0164 - val_dense_24_loss: 0.0159\n",
      "Epoch 3/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.3481 - dense_22_loss: 0.3237 - dense_23_loss: 0.0123 - dense_24_loss: 0.0121 - val_loss: 0.3118 - val_dense_22_loss: 0.2985 - val_dense_23_loss: 0.0067 - val_dense_24_loss: 0.0067\n",
      "Epoch 4/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.2909 - dense_22_loss: 0.2753 - dense_23_loss: 0.0078 - dense_24_loss: 0.0078 - val_loss: 0.4293 - val_dense_22_loss: 0.4176 - val_dense_23_loss: 0.0059 - val_dense_24_loss: 0.0059\n",
      "Epoch 5/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.2745 - dense_22_loss: 0.2625 - dense_23_loss: 0.0060 - dense_24_loss: 0.0060 - val_loss: 0.2529 - val_dense_22_loss: 0.2323 - val_dense_23_loss: 0.0101 - val_dense_24_loss: 0.0105\n",
      "Epoch 6/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.2500 - dense_22_loss: 0.2368 - dense_23_loss: 0.0066 - dense_24_loss: 0.0066 - val_loss: 0.2495 - val_dense_22_loss: 0.2340 - val_dense_23_loss: 0.0077 - val_dense_24_loss: 0.0078\n",
      "Epoch 7/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.2388 - dense_22_loss: 0.2266 - dense_23_loss: 0.0061 - dense_24_loss: 0.0061 - val_loss: 0.2168 - val_dense_22_loss: 0.2024 - val_dense_23_loss: 0.0071 - val_dense_24_loss: 0.0073\n",
      "Epoch 8/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.2180 - dense_22_loss: 0.2050 - dense_23_loss: 0.0065 - dense_24_loss: 0.0065 - val_loss: 0.2080 - val_dense_22_loss: 0.1993 - val_dense_23_loss: 0.0043 - val_dense_24_loss: 0.0044\n",
      "Epoch 9/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.2165 - dense_22_loss: 0.2073 - dense_23_loss: 0.0046 - dense_24_loss: 0.0046 - val_loss: 0.1959 - val_dense_22_loss: 0.1890 - val_dense_23_loss: 0.0034 - val_dense_24_loss: 0.0035\n",
      "Epoch 10/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.2072 - dense_22_loss: 0.1983 - dense_23_loss: 0.0045 - dense_24_loss: 0.0045 - val_loss: 0.2001 - val_dense_22_loss: 0.1930 - val_dense_23_loss: 0.0036 - val_dense_24_loss: 0.0036\n",
      "Epoch 11/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1918 - dense_22_loss: 0.1828 - dense_23_loss: 0.0045 - dense_24_loss: 0.0045 - val_loss: 0.2530 - val_dense_22_loss: 0.2341 - val_dense_23_loss: 0.0095 - val_dense_24_loss: 0.0095\n",
      "Epoch 12/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1810 - dense_22_loss: 0.1727 - dense_23_loss: 0.0042 - dense_24_loss: 0.0042 - val_loss: 0.1619 - val_dense_22_loss: 0.1566 - val_dense_23_loss: 0.0026 - val_dense_24_loss: 0.0027\n",
      "Epoch 13/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1771 - dense_22_loss: 0.1684 - dense_23_loss: 0.0043 - dense_24_loss: 0.0044 - val_loss: 0.1843 - val_dense_22_loss: 0.1749 - val_dense_23_loss: 0.0046 - val_dense_24_loss: 0.0048\n",
      "Epoch 14/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1687 - dense_22_loss: 0.1581 - dense_23_loss: 0.0053 - dense_24_loss: 0.0053 - val_loss: 0.1749 - val_dense_22_loss: 0.1665 - val_dense_23_loss: 0.0042 - val_dense_24_loss: 0.0042\n",
      "Epoch 15/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1574 - dense_22_loss: 0.1493 - dense_23_loss: 0.0040 - dense_24_loss: 0.0040 - val_loss: 0.1514 - val_dense_22_loss: 0.1444 - val_dense_23_loss: 0.0035 - val_dense_24_loss: 0.0036\n",
      "Epoch 16/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1523 - dense_22_loss: 0.1421 - dense_23_loss: 0.0051 - dense_24_loss: 0.0051 - val_loss: 0.1493 - val_dense_22_loss: 0.1345 - val_dense_23_loss: 0.0076 - val_dense_24_loss: 0.0072\n",
      "Epoch 17/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1513 - dense_22_loss: 0.1415 - dense_23_loss: 0.0049 - dense_24_loss: 0.0049 - val_loss: 0.1419 - val_dense_22_loss: 0.1343 - val_dense_23_loss: 0.0039 - val_dense_24_loss: 0.0038\n",
      "Epoch 18/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1529 - dense_22_loss: 0.1432 - dense_23_loss: 0.0049 - dense_24_loss: 0.0048 - val_loss: 0.1453 - val_dense_22_loss: 0.1366 - val_dense_23_loss: 0.0044 - val_dense_24_loss: 0.0043\n",
      "Epoch 19/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1484 - dense_22_loss: 0.1367 - dense_23_loss: 0.0059 - dense_24_loss: 0.0058 - val_loss: 0.1428 - val_dense_22_loss: 0.1340 - val_dense_23_loss: 0.0044 - val_dense_24_loss: 0.0045\n",
      "Epoch 20/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1407 - dense_22_loss: 0.1315 - dense_23_loss: 0.0047 - dense_24_loss: 0.0046 - val_loss: 0.1724 - val_dense_22_loss: 0.1247 - val_dense_23_loss: 0.0239 - val_dense_24_loss: 0.0238\n",
      "Epoch 21/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1410 - dense_22_loss: 0.1304 - dense_23_loss: 0.0054 - dense_24_loss: 0.0053 - val_loss: 0.1432 - val_dense_22_loss: 0.1304 - val_dense_23_loss: 0.0064 - val_dense_24_loss: 0.0064\n",
      "Epoch 22/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1321 - dense_22_loss: 0.1230 - dense_23_loss: 0.0046 - dense_24_loss: 0.0045 - val_loss: 0.1316 - val_dense_22_loss: 0.1276 - val_dense_23_loss: 0.0019 - val_dense_24_loss: 0.0020\n",
      "Epoch 23/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1387 - dense_22_loss: 0.1280 - dense_23_loss: 0.0055 - dense_24_loss: 0.0053 - val_loss: 0.1367 - val_dense_22_loss: 0.1205 - val_dense_23_loss: 0.0082 - val_dense_24_loss: 0.0081\n",
      "Epoch 24/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1358 - dense_22_loss: 0.1248 - dense_23_loss: 0.0055 - dense_24_loss: 0.0054 - val_loss: 0.1439 - val_dense_22_loss: 0.1233 - val_dense_23_loss: 0.0107 - val_dense_24_loss: 0.0099\n",
      "Epoch 25/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.1271 - dense_22_loss: 0.1193 - dense_23_loss: 0.0040 - dense_24_loss: 0.0039 - val_loss: 0.1177 - val_dense_22_loss: 0.1137 - val_dense_23_loss: 0.0020 - val_dense_24_loss: 0.0020\n",
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/25\n",
      "8100/8100 [==============================] - 37s 5ms/step - loss: 1.4595 - dense_27_loss: 0.7345 - dense_28_loss: 0.7250 - val_loss: 1.2617 - val_dense_27_loss: 0.7071 - val_dense_28_loss: 0.5546\n",
      "Epoch 2/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.1910 - dense_27_loss: 0.6540 - dense_28_loss: 0.5370 - val_loss: 1.1569 - val_dense_27_loss: 0.6777 - val_dense_28_loss: 0.4792\n",
      "Epoch 3/25\n",
      "8100/8100 [==============================] - 36s 4ms/step - loss: 1.1081 - dense_27_loss: 0.6358 - dense_28_loss: 0.4723 - val_loss: 1.0930 - val_dense_27_loss: 0.6722 - val_dense_28_loss: 0.4207\n",
      "Epoch 4/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.0474 - dense_27_loss: 0.6267 - dense_28_loss: 0.4207 - val_loss: 1.0416 - val_dense_27_loss: 0.6482 - val_dense_28_loss: 0.3933\n",
      "Epoch 5/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 1.0118 - dense_27_loss: 0.6161 - dense_28_loss: 0.3957 - val_loss: 1.0353 - val_dense_27_loss: 0.6569 - val_dense_28_loss: 0.3785\n",
      "Epoch 6/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.9861 - dense_27_loss: 0.6140 - dense_28_loss: 0.3721 - val_loss: 1.0647 - val_dense_27_loss: 0.6561 - val_dense_28_loss: 0.4086\n",
      "Epoch 7/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.9532 - dense_27_loss: 0.6010 - dense_28_loss: 0.3522 - val_loss: 0.9745 - val_dense_27_loss: 0.6416 - val_dense_28_loss: 0.3329\n",
      "Epoch 8/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.9393 - dense_27_loss: 0.5953 - dense_28_loss: 0.3440 - val_loss: 0.9664 - val_dense_27_loss: 0.6553 - val_dense_28_loss: 0.3111\n",
      "Epoch 9/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.9371 - dense_27_loss: 0.5948 - dense_28_loss: 0.3423 - val_loss: 1.0183 - val_dense_27_loss: 0.6541 - val_dense_28_loss: 0.3642\n",
      "Epoch 10/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.9070 - dense_27_loss: 0.5832 - dense_28_loss: 0.3238 - val_loss: 0.9903 - val_dense_27_loss: 0.6259 - val_dense_28_loss: 0.3644\n",
      "Epoch 11/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.9082 - dense_27_loss: 0.5863 - dense_28_loss: 0.3219 - val_loss: 0.9200 - val_dense_27_loss: 0.6109 - val_dense_28_loss: 0.3091\n",
      "Epoch 12/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8842 - dense_27_loss: 0.5770 - dense_28_loss: 0.3072 - val_loss: 0.9459 - val_dense_27_loss: 0.6312 - val_dense_28_loss: 0.3146\n",
      "Epoch 13/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8856 - dense_27_loss: 0.5752 - dense_28_loss: 0.3103 - val_loss: 0.9455 - val_dense_27_loss: 0.6598 - val_dense_28_loss: 0.2857\n",
      "Epoch 14/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8656 - dense_27_loss: 0.5697 - dense_28_loss: 0.2960 - val_loss: 0.9406 - val_dense_27_loss: 0.6151 - val_dense_28_loss: 0.3254\n",
      "Epoch 15/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8582 - dense_27_loss: 0.5605 - dense_28_loss: 0.2977 - val_loss: 0.9164 - val_dense_27_loss: 0.6069 - val_dense_28_loss: 0.3095\n",
      "Epoch 16/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8504 - dense_27_loss: 0.5542 - dense_28_loss: 0.2962 - val_loss: 0.9890 - val_dense_27_loss: 0.5990 - val_dense_28_loss: 0.3901\n",
      "Epoch 17/25\n",
      "8100/8100 [==============================] - 39s 5ms/step - loss: 0.8493 - dense_27_loss: 0.5501 - dense_28_loss: 0.2992 - val_loss: 0.8832 - val_dense_27_loss: 0.5908 - val_dense_28_loss: 0.2924\n",
      "Epoch 18/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8291 - dense_27_loss: 0.5479 - dense_28_loss: 0.2812 - val_loss: 0.9179 - val_dense_27_loss: 0.5978 - val_dense_28_loss: 0.3202\n",
      "Epoch 19/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8287 - dense_27_loss: 0.5387 - dense_28_loss: 0.2901 - val_loss: 0.8913 - val_dense_27_loss: 0.5973 - val_dense_28_loss: 0.2940\n",
      "Epoch 20/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8186 - dense_27_loss: 0.5435 - dense_28_loss: 0.2751 - val_loss: 0.8640 - val_dense_27_loss: 0.5673 - val_dense_28_loss: 0.2966\n",
      "Epoch 21/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8127 - dense_27_loss: 0.5344 - dense_28_loss: 0.2783 - val_loss: 0.8847 - val_dense_27_loss: 0.6107 - val_dense_28_loss: 0.2740\n",
      "Epoch 22/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8233 - dense_27_loss: 0.5401 - dense_28_loss: 0.2832 - val_loss: 0.8665 - val_dense_27_loss: 0.5912 - val_dense_28_loss: 0.2753\n",
      "Epoch 23/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8037 - dense_27_loss: 0.5257 - dense_28_loss: 0.2780 - val_loss: 0.8730 - val_dense_27_loss: 0.5810 - val_dense_28_loss: 0.2920\n",
      "Epoch 24/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.8124 - dense_27_loss: 0.5242 - dense_28_loss: 0.2882 - val_loss: 0.8716 - val_dense_27_loss: 0.5878 - val_dense_28_loss: 0.2839\n",
      "Epoch 25/25\n",
      "8100/8100 [==============================] - 35s 4ms/step - loss: 0.7779 - dense_27_loss: 0.5171 - dense_28_loss: 0.2608 - val_loss: 0.8230 - val_dense_27_loss: 0.5619 - val_dense_28_loss: 0.2611\n"
     ]
    }
   ],
   "source": [
    "for cluster in clusters:\n",
    "    train_and_save_multitask_model(cluster, y_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/step\n",
      "[0.7329079527854919, 0.36025442719459533, 0.30761341547966004, 0.06504011046886445]\n"
     ]
    }
   ],
   "source": [
    "print(load_and_evaluate_model(['A', 'B', 'alpha'], y_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.059586\n",
      "5.20531\n"
     ]
    }
   ],
   "source": [
    "print(calculate_property('A', 13333))\n",
    "print(y_complete.loc[13333 - 1, 'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam(lr=learning_rate)\n",
    "# if soft:\n",
    "#     model, loss = create_soft_paramter_sharing_model_and_loss(X_in, A_in, E_in, soft_weight)\n",
    "#     model.compile(optimizer=optimizer, loss=loss)\n",
    "# else:\n",
    "#     model = create_hard_paramter_sharing_model(X_in, A_in, E_in)\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# es_callback = EarlyStopping(monitor='val_loss', patience=es_patience)\n",
    "\n",
    "# model.fit([X_train, A_train, E_train],\n",
    "#           y_train_list,\n",
    "#           batch_size=batch_size,\n",
    "#           validation_split=0.1,\n",
    "#           epochs=epochs,\n",
    "#           callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Evaluating model.')\n",
    "# eval_results = model.evaluate([X_test, A_test, E_test],\n",
    "#                               y_test_list,\n",
    "#                               batch_size=batch_size)\n",
    "# print('Done.\\n'\n",
    "#       'Test loss: {}'.format(eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model.predict([X_test, A_test, E_test])\n",
    "\n",
    "# if num_tasks == 1:\n",
    "#     preds = np.transpose(preds)\n",
    "\n",
    "# for i in range(num_tasks):\n",
    "#     plt.figure()\n",
    "#     plt.scatter(preds[i], y_test_list[i], alpha=0.3)\n",
    "#     plt.plot()\n",
    "#     plt.title(tasks[i])\n",
    "#     plt.xlabel('Predicted')\n",
    "#     plt.ylabel('Actual')\n",
    "#     # plt.savefig('graphs/' + '11_5_'+tasks[i]+'_multitask')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
