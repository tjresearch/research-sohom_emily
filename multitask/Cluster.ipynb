{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run QM9GNN2_Multitask.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_task_filename(task):\n",
    "    return path.join('single_task_trained_models', f'{task}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_all, X_all, E_all, y_all = load_data()\n",
    "A, X, E, y = sample_from_data(10000, A_all, X_all, E_all, y_all)\n",
    "_ = standardize(y)\n",
    "A_train, A_test, \\\n",
    "    X_train, X_test, \\\n",
    "    E_train, E_test, \\\n",
    "    y_train, y_test = train_test_split(A, X, E, y, test_size=0.1)\n",
    "tasks = [t for c in clusters for t in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_single_task_models(tasks, *, A, X, E):\n",
    "    for task in tasks:\n",
    "        print(f'learning {task}')\n",
    "        model = build_single_task_model(A=A, X=X, E=E)\n",
    "        model.fit([X_train, A_train, E_train],\n",
    "                 y_train[[task]].values,\n",
    "                 batch_size=32,\n",
    "                 epochs=5)\n",
    "        model.save_weights(generate_single_task_filename(task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transfer_coefficient(learned, transferred, y_transferred_test, A_test, X_test, E_test):\n",
    "    learned_model = build_single_task_model(A=A_test, X=X_test, E=E_test)\n",
    "    transferred_model = build_single_task_model(A=A_test, X=X_test, E=E_test)\n",
    "    learned_model.load_weights(generate_single_task_filename(learned))\n",
    "    transferred_model.load_weights(generate_single_task_filename(transferred))\n",
    "    learned_layers = learned_model.get_weights()\n",
    "    transferred_layers = transferred_model.get_weights()\n",
    "    # 10 is a hard-coded architecture-dependent value\n",
    "    SPLIT = -2\n",
    "    transferred_layers = learned_layers[:SPLIT] + transferred_layers[SPLIT:]\n",
    "    transferred_model.set_weights(transferred_layers)\n",
    "    transferred_model.compile(optimizer=Adam(lr=1e-3), loss='mae')\n",
    "    \n",
    "    eval_results = transferred_model.evaluate([X_test, A_test, E_test],\n",
    "                                             y_transferred_test,\n",
    "                                              batch_size=32)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transfer_coefficients(tasks, A_test, X_test, E_test, y_test):\n",
    "    df = pd.DataFrame(index=tasks, columns=tasks, dtype=np.float32)\n",
    "    for learned, transferred in itertools.product(tasks[:2], repeat=2):\n",
    "        print(f'transferring {learned} to {transferred}')\n",
    "        transfer_coef = calculate_transfer_coefficient(learned, \n",
    "                                                       transferred, \n",
    "                                                       y_test[[transferred]].values, \n",
    "                                                       A_test, \n",
    "                                                       X_test, \n",
    "                                                       E_test)\n",
    "        print(transfer_coef)\n",
    "        df.loc[learned, transferred] = transfer_coef\n",
    "\n",
    "    with open('cross_task_transfer_coefs.pkl', 'wb') as f:\n",
    "        pickle.dump(obj=df, file=f)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transfer_coefficients():\n",
    "    with open('cross_task_transfer_coefs.pkl', 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_from_similarity_matrix(df, num_clusters):\n",
    "    mat = df.to_numpy()\n",
    "    df[:] = (mat+mat.T)/2\n",
    "    sc = SpectralClustering(n_clusters=num_clusters, affinity='precomputed')\n",
    "    labels = sc.fit_predict(df.to_numpy())\n",
    "    d = {c:[] for c in labels}\n",
    "    for c, t in zip(labels, df.columns):\n",
    "        d[c].append(t)\n",
    "    clusters = [v for k, v in d.items()]\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save_single_task_models(tasks, A=A_train, X=X_train, E=E_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_transfer_coefficients(tasks, A_test, X_test, E_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_transfer_coefficients()\n",
    "print(cluster_from_similarity_matrix(df, 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
