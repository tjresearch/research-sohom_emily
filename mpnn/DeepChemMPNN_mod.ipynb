{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "RDKit WARNING: [08:53:24] Enabling RDKit 2019.09.3 jupyter extensions\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.utils import ScaffoldGenerator\n",
    "from deepchem.utils.save import log\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import OrderedDict\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f08b2511e90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(2)\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scaffold(smiles, include_chirality=False):\n",
    "    \"\"\"Compute the Bemis-Murcko scaffold for a SMILES string.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    engine = ScaffoldGenerator(include_chirality=include_chirality)\n",
    "    scaffold = engine.get_scaffold(mol)\n",
    "    return scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataset,\n",
    "          frac_train=.80,\n",
    "          frac_valid=.10,\n",
    "          frac_test=.10,\n",
    "          log_every_n=1000):\n",
    "    \"\"\"\n",
    "    Splits internal compounds into train/validation/test by scaffold.\n",
    "    \"\"\"\n",
    "    np.testing.assert_almost_equal(frac_train + frac_valid + frac_test, 1.)\n",
    "    scaffolds = {}\n",
    "    log(\"About to generate scaffolds\", True)\n",
    "    data_len = len(dataset)\n",
    "\n",
    "    for ind, smiles in enumerate(dataset):\n",
    "        if ind % log_every_n == 0:\n",
    "            log(\"Generating scaffold %d/%d\" % (ind, data_len), True)\n",
    "        scaffold = generate_scaffold(smiles)\n",
    "        if scaffold not in scaffolds:\n",
    "            scaffolds[scaffold] = [ind]\n",
    "        else:\n",
    "            scaffolds[scaffold].append(ind)\n",
    "\n",
    "    scaffolds = {key: sorted(value) for key, value in scaffolds.items()}\n",
    "    scaffold_sets = [\n",
    "        scaffold_set\n",
    "        for (scaffold, scaffold_set) in sorted(\n",
    "            scaffolds.items(), key=lambda x: (len(x[1]), x[1][0]), reverse=True)\n",
    "    ]\n",
    "    train_cutoff = frac_train * len(dataset)\n",
    "    valid_cutoff = (frac_train + frac_valid) * len(dataset)\n",
    "    train_inds, valid_inds, test_inds = [], [], []\n",
    "    log(\"About to sort in scaffold sets\", True)\n",
    "    for scaffold_set in scaffold_sets:\n",
    "        if len(train_inds) + len(scaffold_set) > train_cutoff:\n",
    "            if len(train_inds) + len(valid_inds) + len(scaffold_set) > valid_cutoff:\n",
    "                test_inds += scaffold_set    \n",
    "            else:\n",
    "                valid_inds += scaffold_set\n",
    "        else:\n",
    "            train_inds += scaffold_set\n",
    "    return train_inds, valid_inds, test_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donkey_load_dataset(filename, whiten=False):\n",
    "    f = open(filename, 'r')\n",
    "    features = []\n",
    "    labels = []\n",
    "    tracer = 0\n",
    "    for line in f:\n",
    "        if tracer == 0:\n",
    "            tracer += 1\n",
    "            continue\n",
    "        splits =  line[:-1].split(',')\n",
    "        \n",
    "        #### THIS HAS BEEN MODIFIED FROM SOURCE\n",
    "        # features.append(splits[-1])\n",
    "        # labels.append(float(splits[-2]))\n",
    "        features.append(splits[-2])\n",
    "        labels.append(float(splits[-1]))\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels, dtype='float32').reshape(-1, 1)\n",
    "\n",
    "    train_ind, val_ind, test_ins = split(features)\n",
    "\n",
    "    train_features = np.take(features, train_ind)\n",
    "    train_labels = np.take(labels, train_ind)\n",
    "    val_features = np.take(features, val_ind)\n",
    "    val_labels = np.take(labels, val_ind)\n",
    "\n",
    "    return train_features, train_labels, val_features, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "az_ppb_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# DATASET = 'az_ppb.csv'\n",
    "DATASET = 'az_ppb_clean.csv'\n",
    "print(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3\n",
    "BATCH_SIZE = 48\n",
    "MAXITER = 40000\n",
    "LIMIT = 0\n",
    "LR = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = nn.Linear(150, 128)\n",
    "U = {0: nn.Linear(156, 75), 1: nn.Linear(156, 75), 2: nn.Linear(156, 75)}\n",
    "V = {0: nn.Linear(75, 75), 1: nn.Linear(75, 75), 2: nn.Linear(75, 75)}\n",
    "E = nn.Linear(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by .8 every 5 epochs\"\"\"\n",
    "    lr = LR * (0.9 ** (epoch // 10))\n",
    "    print('new lr [%.5f]' % lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_features, train_labels, val_features, val_labels = donkey_load_dataset(DATASET)\n",
    "    \n",
    "    train_labels = train_labels.reshape(-1, 1)\n",
    "    val_labels = val_labels.reshape(-1, 1)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(train_labels)\n",
    "    train_labels = scaler.transform(train_labels)\n",
    "    val_labels = scaler.transform(val_labels)\n",
    "\n",
    "    train_labels = Variable(torch.FloatTensor(train_labels), requires_grad=False)\n",
    "    val_labels = Variable(torch.FloatTensor(val_labels), requires_grad=False)\n",
    "\n",
    "    return train_features, train_labels, val_features, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout(h, h2):\n",
    "    catted_reads = map(lambda x: torch.cat([h[x[0]], h2[x[1]]], 1), zip(h2.keys(), h.keys()))\n",
    "    activated_reads = map(lambda x: F.selu( R(x) ), catted_reads)\n",
    "    readout = Variable(torch.zeros(1, 128))\n",
    "    for read in activated_reads:\n",
    "        readout = readout + read\n",
    "    return F.tanh(readout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_pass(g, h, k):\n",
    "    for v in g.keys():\n",
    "        neighbors = g[v]\n",
    "        for neighbor in neighbors:\n",
    "            e_vw = neighbor[0] # feature variable\n",
    "            w = neighbor[1]\n",
    "\n",
    "            m_w = V[k](h[w])\n",
    "            m_e_vw = E(e_vw)\n",
    "            reshaped = torch.cat( (h[v], m_w, m_e_vw), 1)\n",
    "            h[v] = F.selu(U[k](reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_multigraph(smile):\n",
    "    g = OrderedDict({})\n",
    "    h = OrderedDict({})\n",
    "    molecule = Chem.MolFromSmiles(smile)\n",
    "    for i in range(0, molecule.GetNumAtoms()):\n",
    "        atom_i = molecule.GetAtomWithIdx(i)\n",
    "        h[i] = Variable(torch.FloatTensor(dc.feat.graph_features.atom_features(atom_i))).view(1, 75)\n",
    "        for j in range(0, molecule.GetNumAtoms()):\n",
    "            e_ij = molecule.GetBondBetweenAtoms(i, j)\n",
    "            if e_ij != None:\n",
    "                \n",
    "                ### THIS HAS BEEN MODIFIED FROM SOURCE\n",
    "#                 e_ij =  map(lambda x: 1 if x == True else 0, \n",
    "#                             dc.feat.graph_features.bond_features(e_ij)) # ADDED edge feat\n",
    "                e_ij = [*map(lambda x: 1 if x == True else 0, \n",
    "                           dc.feat.graph_features.bond_features(e_ij))]\n",
    "    \n",
    "                e_ij = Variable(torch.FloatTensor(e_ij).view(1, 6))\n",
    "                atom_j = molecule.GetAtomWithIdx(j)\n",
    "                if i not in g:\n",
    "                    g[i] = []\n",
    "                g[i].append( (e_ij, j) )\n",
    "    return g, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to generate scaffolds\n",
      "Generating scaffold 0/1614\n",
      "Generating scaffold 1000/1614\n",
      "About to sort in scaffold sets\n"
     ]
    }
   ],
   "source": [
    "train_smiles, train_labels, val_smiles, val_labels = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(128, 1)\n",
    "params = [{'params': R.parameters()},\n",
    "         {'params': U[0].parameters()},\n",
    "         {'params': U[1].parameters()},\n",
    "         {'params': U[2].parameters()},\n",
    "         {'params': E.parameters()},\n",
    "         {'params': V[0].parameters()},\n",
    "         {'params': V[1].parameters()},\n",
    "         {'params': V[2].parameters()},\n",
    "         {'params': linear.parameters()}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "No loop matching the specified signature and casting was found for ufunc add",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-904fe1fc4664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mr2_val_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hats_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mr2_val_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hats_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mtrain_loss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/psi4conda/envs/deepchem/lib/python3.6/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mpearsonr\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   3517\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3519\u001b[0;31m     \u001b[0mxmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3520\u001b[0m     \u001b[0mymean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/psi4conda/envs/deepchem/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mTypeError\u001b[0m: No loop matching the specified signature and casting was found for ufunc add"
     ]
    }
   ],
   "source": [
    "num_epoch = 0\n",
    "optimizer = optim.Adam(params, lr=LR, weight_decay=1e-4)\n",
    "for i in range(0, MAXITER):\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = Variable(torch.zeros(1, 1))\n",
    "    y_hats_train = []\n",
    "    for j in range(0, BATCH_SIZE):\n",
    "        sample_index = random.randint(0, len(train_smiles) - 2)\n",
    "        smile = train_smiles[sample_index]\n",
    "        g, h = construct_multigraph(smile) # TODO: cache this\n",
    "\n",
    "        g2, h2 = construct_multigraph(smile)\n",
    "\n",
    "        for k in range(0, T):\n",
    "            message_pass(g, h, k)\n",
    "\n",
    "        x = readout(h, h2)\n",
    "        #x = F.selu( fc(x) )\n",
    "        y_hat = linear(x)\n",
    "        y = train_labels[sample_index]\n",
    "\n",
    "        y_hats_train.append(y_hat)\n",
    "\n",
    "        error = (y_hat - y)*(y_hat - y) / Variable(torch.FloatTensor([BATCH_SIZE])).view(1, 1)\n",
    "        train_loss = train_loss + error\n",
    "\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % int(len(train_smiles) / BATCH_SIZE) == 0:\n",
    "        val_loss = Variable(torch.zeros(1, 1), requires_grad=False)\n",
    "        y_hats_val = []\n",
    "        for j in range(0, len(val_smiles)):\n",
    "            g, h = construct_multigraph(val_smiles[j])\n",
    "            g2, h2 = construct_multigraph(val_smiles[j])\n",
    "\n",
    "            for k in range(0, T):\n",
    "                message_pass(g, h, k)\n",
    "\n",
    "            x = readout(h, h2)\n",
    "            #x = F.selu( fc(x) )\n",
    "            y_hat = linear(x)\n",
    "            y = val_labels[j]\n",
    "\n",
    "            y_hats_val.append(y_hat)\n",
    "\n",
    "            error = (y_hat - y)*(y_hat - y) / Variable(torch.FloatTensor([len(val_smiles)])).view(1, 1)\n",
    "            val_loss = val_loss + error\n",
    "\n",
    "    ## MODIFYING FROM SOURCE\n",
    "#     y_hats_val = np.array(map(lambda x: x.data.numpy(), y_hats_val))\n",
    "#     y_val = np.array(map(lambda x: x.data.numpy(), val_labels))\n",
    "    y_hats_val = np.array(list(map(lambda x: x.data.numpy(), y_hats_val)), dtype='float64')\n",
    "    y_val = np.array(list(map(lambda x: x.data.numpy(), val_labels)), dtype='float64')\n",
    "\n",
    "    y_hats_val = y_hats_val.reshape(-1, 1)\n",
    "    y_val = y_val.reshape(-1, 1)\n",
    "\n",
    "    r2_val_old = r2_score(y_val, y_hats_val)\n",
    "    r2_val_new = pearsonr(y_val, y_hats_val)[0]**2\n",
    "\n",
    "    train_loss_ = train_loss.data.numpy()[0]\n",
    "    val_loss_ = val_loss.data.numpy()[0]\n",
    "    print('epoch [%i/%i] train_loss [%f] val_loss [%f] r2_val_old [%.4f], r2_val_new [%.4f]' \\\n",
    "                  % (num_epoch, 100, train_loss_, val_loss_, r2_val_old, r2_val_new))\n",
    "    num_epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
