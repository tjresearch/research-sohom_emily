{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.backend import mean, square\n",
    "\n",
    "from spektral.datasets import qm9\n",
    "from spektral.layers import EdgeConditionedConv, GlobalSumPool, GlobalAttentionPool\n",
    "from spektral.utils import label_to_one_hot\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A_all, X_all, E_all, y_all = qm9.load_data(return_type='numpy',\n",
    "                           nf_keys='atomic_num',\n",
    "                           ef_keys='type',\n",
    "                           self_loops=True,\n",
    "                           amount=2000) # chnage this to None to load entire dataset\n",
    "# Preprocessing\n",
    "X_uniq = np.unique(X_all)\n",
    "X_uniq = X_uniq[X_uniq != 0]\n",
    "E_uniq = np.unique(E_all)\n",
    "E_uniq = E_uniq[E_uniq != 0]\n",
    "\n",
    "X_all = label_to_one_hot(X_all, X_uniq)\n",
    "E_all = label_to_one_hot(E_all, E_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N = X_all.shape[-2]       # Number of nodes in the graphs\n",
    "F = X_all[0].shape[-1]    # Dimension of node features\n",
    "S = E_all[0].shape[-1]    # Dimension of edge features\n",
    "n_out = y_all.shape[-1]   # Dimension of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we don't want to train only on the lightest molecules\n",
    "# we randomly sample from the dataset\n",
    "indices = np.random.choice(X_all.shape[0], 1000, replace=False)\n",
    "X = X_all[indices, :, :]\n",
    "A = A_all[indices, :, :]\n",
    "E = E_all[indices, :, :, :]\n",
    "y = y_all.iloc[indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the means and stddevs here allows us \n",
    "# to normalize our data \n",
    "# TODO: shouldn't we store only the mean/stddev for the training data?\n",
    "task_to_scaler = dict()\n",
    "for task in list(y.columns)[1:]:\n",
    "    scaler = StandardScaler()\n",
    "    y.task = scaler.fit_transform(y[[task]])\n",
    "    task_to_scaler[task] = scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [['A', 'B', 'alpha'], \n",
    "            ['C', 'r2', 'u0'],\n",
    "            ['zpve', 'g298', 'cv'],\n",
    "            ['lumo', 'u298', 'h298'],\n",
    "            ['mu', 'homo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train, A_test, \\\n",
    "    X_train, X_test, \\\n",
    "    E_train, E_test, \\\n",
    "    y_train, y_test = train_test_split(A, X, E, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_task_model(*, N, F, S):\n",
    "  X_in = Input(shape=(N, F))\n",
    "  A_in = Input(shape=(N, N))\n",
    "  E_in = Input(shape=(N, N, S))\n",
    "\n",
    "  gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "  gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "  pool = GlobalAttentionPool(256)(gc2)\n",
    "  dense = Dense(256, activation='relu')(pool)\n",
    "  output = Dense(1)(dense_layer)\n",
    "\n",
    "  # Build model\n",
    "  model = Model(inputs=[X_in, A_in, E_in], outputs=output)\n",
    "  optimizer = Adam(lr=learning_rate)\n",
    "  model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hard_sharing_model(*, N, F, S, num_tasks):\n",
    "  X_in = Input(shape=(N, F))\n",
    "  A_in = Input(shape=(N, N))\n",
    "  E_in = Input(shape=(N, N, S))\n",
    "\n",
    "  gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "  gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "  pool = GlobalAttentionPool(256)(gc2)\n",
    "  dense_list = [Dense(256, activation='relu')(pool) for i in range(num_tasks)]\n",
    "  output_list = [Dense(1)(dense_layer) for dense_layer in dense_list]\n",
    "\n",
    "  # Build model\n",
    "  model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "  optimizer = Adam(lr=learning_rate)\n",
    "  model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_soft_sharing_model(*, N, F, S, num_tasks, share_param):\n",
    "  X_in = Input(shape=(N, F))\n",
    "  A_in = Input(shape=(N, N))\n",
    "  E_in = Input(shape=(N, N, S))\n",
    "\n",
    "  gc1_list = [EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in]) for i in range(num_tasks)]\n",
    "  gc2_list = [EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in]) for gc1 in gc1_list]\n",
    "  pool_list = [GlobalAttentionPool(256)(gc2) for gc2 in gc2_list]\n",
    "  dense_list = [Dense(256, activation='relu')(pool) for pool in pool_list]\n",
    "  output_list = [Dense(1)(dense) for dense in dense_list]\n",
    "\n",
    "  def loss(y_actual, y_pred):\n",
    "    avg_layer_diff = 0\n",
    "    for i in range(num_tasks):\n",
    "      for j in range(i):\n",
    "        for gc in [gc1_list, gc2_list]:\n",
    "          avg_layer_diff += mean(square(gc[i].trainable_weights - gc[j].trainable_weights))\n",
    "    avg_layer_diff /= (num_tasks)*(num_tasks-1)/2  \n",
    "    return mean(square(y_actual - y_pred)) + share_param*avg_layer_diff\n",
    "\n",
    "  # Build model\n",
    "  model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "  optimizer = Adam(lr=learning_rate)\n",
    "  model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOLDER_PATH = '/content/drive/My Drive/Colab Notebooks/demo_models'\n",
    "FOLDER_PATH = 'demo_models'\n",
    "\n",
    "def generate_model_filename(tasks):\n",
    "  filename = \"\".join(sorted(tasks))\n",
    "  return path.join(FOLDER_PATH, filename + '.h5')\n",
    "  # return filename + '.h5'\n",
    "\n",
    "def generate_task_scaler_filename(task):\n",
    "  return path.join(FOLDER_PATH, task + '.txt')\n",
    "  # return task + '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tasks):\n",
    "  model.save_weights(generate_model_filename(tasks))\n",
    "  for task in tasks:\n",
    "    scaler_filename = generate_task_scaler_filename(task)\n",
    "    with open(scaler_filename, 'w') as f:\n",
    "      print(task_to_scaler[task].mean_[0], file=f)\n",
    "      print(task_to_scaler[task].scale_[0], file=f)\n",
    "\n",
    "def load_hard_sharing_model(*, N, F, S, tasks):\n",
    "  model = build_hard_sharing_model(N=N, F=F, S=S, num_tasks=len(tasks))\n",
    "  model.load_weights(generate_model_filename(tasks))\n",
    "  task_to_scaler = dict()\n",
    "  for task in tasks:\n",
    "    with open(generate_task_scaler_filename(task), 'r') as f:\n",
    "      lines = f.readlines()\n",
    "      scaler = StandardScaler()\n",
    "      scaler.mean_ = float(lines[0].strip())\n",
    "      scaler.scale_ = float(lines[1].strip())\n",
    "      task_to_scaler[task] = scaler\n",
    "  return model, task_to_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_property(prop, mol_id, clusters, N=N, F=F, S=S):\n",
    "  for cluster in clusters:\n",
    "    if prop in cluster:\n",
    "      model, task_to_scaler = load_hard_sharing_model(N=N, F=F, S=S, tasks=cluster)\n",
    "      i = mol_id - 1\n",
    "      print(\"i\", i)\n",
    "      print('X', X_all[i].shape)\n",
    "      print('A', A_all[i].shape)\n",
    "      print('E', E_all[i].shape)\n",
    "\n",
    "      # convert shape for batch mode\n",
    "      def wrap(a):\n",
    "        return a.reshape([1] + list(a.shape))\n",
    "      x = list(map(wrap, [X_all[i], A_all[i], E_all[i]]))\n",
    "\n",
    "      cluster_prediction = model.predict(x)\n",
    "      prediction = cluster_prediction[cluster.index(prop)]\n",
    "      prediction = task_to_scaler[prop].inverse_transform(prediction)\n",
    "      print(prediction)\n",
    "      return prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in clusters[:1]:\n",
    "  model = build_hard_sharing_model(N=N, F=F, S=S, num_tasks=len(cluster))\n",
    "  model.fit(x=[X_train, A_train, E_train], \n",
    "            y=y_train[cluster].values,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.1,\n",
    "            epochs=3)\n",
    "#   save_model(model, cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, _ = load_hard_sharing_model(N=N, F=F, S=S, tasks=clusters[0])\n",
    "model_loss = model.evaluate(x=[X_test, A_test, E_test],\n",
    "                            y=y_test[cluster].values)\n",
    "print(f\"Test loss: {model_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('spektral_dev': conda)",
   "language": "python",
   "name": "python37764bitspektraldevcondabfd33a3dc2d448368ade01afb4015ef2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}