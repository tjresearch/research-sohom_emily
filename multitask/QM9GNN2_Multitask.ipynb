{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "from os import path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, LogCosh\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from tensorflow.keras.backend import mean, square\n",
    "\n",
    "from spektral.datasets import qm9\n",
    "from spektral.layers import EdgeConditionedConv, GINConv, GatedGraphConv\n",
    "from spektral.layers import ops, GlobalSumPool, GlobalAttentionPool\n",
    "from spektral.utils import batch_iterator, numpy_to_disjoint\n",
    "from spektral.utils import label_to_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(amount=None, mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")\n",
    "    \n",
    "    A_all, X_all, E_all, y_all = qm9.load_data(return_type='numpy',\n",
    "                               nf_keys='atomic_num',\n",
    "                               ef_keys='type',\n",
    "                               self_loops=True,\n",
    "                               amount=amount) # None for entire dataset\n",
    "    # Preprocessing\n",
    "    if mode == 'batch':\n",
    "        X_uniq = np.unique(X_all)\n",
    "        X_uniq = X_uniq[X_uniq != 0]\n",
    "        E_uniq = np.unique(E_all)\n",
    "        E_uniq = E_uniq[E_uniq != 0]\n",
    "\n",
    "        X_all = label_to_one_hot(X_all, X_uniq)\n",
    "        E_all = label_to_one_hot(E_all, E_uniq)\n",
    "    elif mode == 'disjoint':\n",
    "        X_uniq = np.unique([v for x in X_all for v in np.unique(x)])\n",
    "        E_uniq = np.unique([v for e in E_all for v in np.unique(e)])\n",
    "        X_uniq = X_uniq[X_uniq != 0]\n",
    "        E_uniq = E_uniq[E_uniq != 0]\n",
    "\n",
    "        X_all = [label_to_one_hot(x, labels=X_uniq) for x in X_all]\n",
    "        E_all = [label_to_one_hot(e, labels=E_uniq) for e in E_all]\n",
    "    \n",
    "    return A_all, X_all, E_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_data(sample_size, A_all, X_all, E_all, y_all, mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")\n",
    "    if mode == 'batch':\n",
    "        indices = np.random.choice(X_all.shape[0], sample_size, replace=False)\n",
    "        A = A_all[indices, :, :]\n",
    "        X = X_all[indices, :, :]\n",
    "        E = E_all[indices, :, :, :]\n",
    "        y = y_all.iloc[indices, :].copy()\n",
    "        \n",
    "    if mode == 'disjoint':\n",
    "        indices = np.random.choice(len(X_all), sample_size, replace=False)\n",
    "        A = [A_all[i] for i in indices]\n",
    "        X = [X_all[i] for i in indices]\n",
    "        E = [E_all[i] for i in indices]\n",
    "        y = y_all.iloc[indices, :].copy()\n",
    "    \n",
    "    return A, X, E, y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(y, scaler='power_transformer'):\n",
    "    if scaler not in ['power_transformer', 'standard_scaler']:\n",
    "        raise ValueError(f'scaler {scaler} not recognized'\n",
    "                        'choose \"power_transformer\" or \"standard_scaler\"')\n",
    "    task_to_scaler = dict()\n",
    "    for task in list(y.columns)[1:]:\n",
    "        if scaler == 'standard_scaler':\n",
    "            scaler = StandardScaler()\n",
    "        else:\n",
    "            scaler = PowerTransformer()\n",
    "        y.loc[:, task] = scaler.fit_transform(y[[task]])\n",
    "        task_to_scaler[task] = scaler\n",
    "    return task_to_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_params(*, A, X, E, mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")\n",
    "    F = X[0].shape[-1]  # Dimension of node features\n",
    "    S = E[0].shape[-1]  # Dimension of edge features\n",
    "    if mode == 'batch':\n",
    "        N = X.shape[-2]       # Number of nodes in the graphs\n",
    "        return N, F, S\n",
    "    if mode == 'disjoint':\n",
    "        return F, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tensors(*, A, X, E, mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")\n",
    "    if mode == 'batch':\n",
    "        N, F, S = get_shape_params(A=A, X=X, E=E, mode=mode)\n",
    "        X_in = Input(shape=(N, F), name='X_in')\n",
    "        A_in = Input(shape=(N, N), name='A_in')\n",
    "        E_in = Input(shape=(N, N, S), name='E_in')\n",
    "\n",
    "        return X_in, A_in, E_in\n",
    "    \n",
    "    if mode == 'disjoint':\n",
    "        F, S = get_shape_params(A=A, X=X, E=E, mode=mode)\n",
    "        X_in = Input(shape=(F,), name='X_in')\n",
    "        A_in = Input(shape=(None,), sparse=True, name='A_in')\n",
    "        E_in = Input(shape=(S,), name='E_in')\n",
    "        I_in = Input(shape=(), name='segment_ids_in', dtype=tf.int32)\n",
    "        \n",
    "        return X_in, A_in, E_in, I_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_task_model(*, A, X, E, learning_rate=1e-3, \n",
    "                            conv='ecc', mode='batch',\n",
    "                            layer_sizes=[128, 256, 512, 512]):  \n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")  \n",
    "    if conv not in ['ecc', 'gin']:\n",
    "        raise ValueError(f\"convolution layer {conv} not recognized; \"\n",
    "                         \"choose 'ecc' or 'gin'\")\n",
    "    \n",
    "    if mode == 'batch':\n",
    "        X_in, A_in, E_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "    if mode == 'disjoint':\n",
    "        X_in, A_in, E_in, I_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "        \n",
    "    gc1_size,\n",
    "    gc2_size,\n",
    "    pool_size,\n",
    "    dense_size = *layer_sizes\n",
    "\n",
    "    if conv == 'ecc':    \n",
    "        gc1 = EdgeConditionedConv(gc1_size, activation='relu')([X_in, A_in, E_in])\n",
    "        gc2 = EdgeConditionedConv(gc2_size, activation='relu')([gc1, A_in, E_in])\n",
    "    if conv == 'gin':\n",
    "        assert mode == 'disjoint', 'cannot run GIN in batch mode'\n",
    "        gc1 = GINConv(gc1_size, activation='relu')([X_in, A_in, E_in])\n",
    "        gc2 = GINConv(gc2_size, activation='relu')([gc1, A_in, E_in])\n",
    "    if mode == 'batch':\n",
    "        pool = GlobalAttentionPool(pool_size)(gc2)\n",
    "    if mode == 'disjoint':\n",
    "        pool = GlobalAttentionPool(pool_size)([gc2, I_in])\n",
    "    dense = Dense(dense_size, activation='relu')(pool)\n",
    "    output = Dense(1)(dense)\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    loss_fn = MeanSquaredError()\n",
    "    if mode == 'batch':\n",
    "        model = Model(inputs=[X_in, A_in, E_in], outputs=output)\n",
    "        model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    if mode == 'disjoint':\n",
    "        model = Model(inputs=[X_in, A_in, E_in, I_in], outputs=output)\n",
    "    \n",
    "    return model, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hard_sharing_model(*, A, X, E, num_tasks, \n",
    "                             learning_rate=1e-3, conv='ecc', mode='batch', \n",
    "                             layer_sizes=[128, 256, 512, 512]):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")  \n",
    "    if conv not in ['ecc', 'gin']:\n",
    "        raise ValueError(f\"convolution layer {conv} not recognized; \"\n",
    "                         \"choose 'ecc' or 'gin'\")\n",
    "    if mode == 'batch':\n",
    "        X_in, A_in, E_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "    if mode == 'disjoint':\n",
    "        X_in, A_in, E_in, I_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "\n",
    "    gc1_size,\n",
    "    gc2_size,\n",
    "    pool_size,\n",
    "    dense_size = *layer_sizes\n",
    "    \n",
    "    if conv == 'ecc':    \n",
    "        gc1 = EdgeConditionedConv(gc1_size, activation='relu')([X_in, A_in, E_in])\n",
    "        gc2 = EdgeConditionedConv(gc2_size, activation='relu')([gc1, A_in, E_in])\n",
    "    if conv == 'gin':\n",
    "        assert mode == 'disjoint', 'cannot run GIN in batch mode'\n",
    "        gc1 = GINConv(gc1_size, activation='relu')([X_in, A_in, E_in])\n",
    "        gc2 = GINConv(gc2_size, activation='relu')([gc1, A_in, E_in])\n",
    "    if mode == 'batch':\n",
    "        pool = GlobalAttentionPool(pool_size)(gc2)\n",
    "    if mode == 'disjoint':\n",
    "        pool = GlobalAttentionPool(pool_size)([gc2, I_in])\n",
    "    dense_list = [Dense(dense_size, activation='relu')(pool) \n",
    "                  for i in range(num_tasks)]\n",
    "    output_list = [Dense(1)(dense_layer) for dense_layer in dense_list]\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    loss_fn = MeanSquaredError()\n",
    "    if mode == 'batch':\n",
    "        model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "        model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    if mode == 'disjoint':\n",
    "        model = Model(inputs=[X_in, A_in, E_in, I_in], outputs=output_list)\n",
    "    \n",
    "    return model, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_soft_sharing_model(*, A, X, E, num_tasks, share_param, \n",
    "                             learning_rate=1e-3, conv='ecc', mode='batch', \n",
    "                             layer_sizes=[128, 256, 512, 512]):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")  \n",
    "    if conv not in ['ecc', 'gin']:\n",
    "        raise ValueError(f\"convolution layer {conv} not recognized; \"\n",
    "                         \"choose 'ecc' or 'gin'\")\n",
    "    if mode == 'batch':\n",
    "        X_in, A_in, E_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "    if mode == 'disjoint':\n",
    "        X_in, A_in, E_in, I_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "        \n",
    "    if conv == 'ecc':\n",
    "        conv_layer = EdgeConditionedConv\n",
    "    if conv == 'gin':\n",
    "        conv_layer = GINConv\n",
    "        \n",
    "    gc1_size,\n",
    "    gc2_size,\n",
    "    pool_size,\n",
    "    dense_size = *layer_sizes\n",
    "\n",
    "    gc1_list = [conv_layer(gc1_size, activation='relu')([X_in, A_in, E_in]) \n",
    "                for i in range(num_tasks)]\n",
    "    gc2_list = [conv_layer(gc2_size, activation='relu')([gc1, A_in, E_in]) \n",
    "                for gc1 in gc1_list]\n",
    "    if mode == 'batch':\n",
    "        pool_list = [GlobalAttentionPool(pool_size)(gc2) for gc2 in gc2_list]\n",
    "    if mode == 'disjoint':\n",
    "        pool_list = [GlobalAttentionPool(pool_size)([gc2, I_in]) for gc2 in gc2_list]\n",
    "    dense_list = [Dense(dense_size, activation='relu')(pool) for pool in pool_list]\n",
    "    output_list = [Dense(1)(dense) for dense in dense_list]\n",
    "\n",
    "    def loss_fn(y_actual, y_pred):\n",
    "        avg_layer_diff = 0\n",
    "        for i, j in itertools.combinations(range(num_tasks), 2):\n",
    "            for gc in [gc1_list, gc2_list]:\n",
    "                diff = gc[i].trainable_weights - gc[j].trainable_weights\n",
    "                avg_layer_diff += mean(square(diff))\n",
    "        avg_layer_diff /= (num_tasks)*(num_tasks-1)/2  \n",
    "        return mean(square(y_actual - y_pred)) + share_param*avg_layer_diff\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    if mode == 'batch':\n",
    "        model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "        model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    if mode == 'disjoint':\n",
    "        model = Model(inputs=[X_in, A_in, E_in, I_in], outputs=output_list)\n",
    "    \n",
    "    return model, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_filename(tasks, conv='ecc', mode='batch', folder_path='demo_models'):\n",
    "    filename = \"\".join(sorted(tasks)) + '_' + conv + '_' + mode \n",
    "    return path.join(folder_path, f'{filename}.h5')\n",
    "\n",
    "def generate_task_scaler_filename(task, folder_path='demo_models'):\n",
    "    return path.join(folder_path, f'{task}_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tasks, task_to_scaler, mode='batch', conv='ecc'):\n",
    "    model.save_weights(generate_model_filename(tasks, conv=conv, mode=mode))\n",
    "    for task in tasks:\n",
    "        scaler_filename = generate_task_scaler_filename(task)\n",
    "        with open(scaler_filename, 'wb') as f:\n",
    "            scaler = task_to_scaler[task]\n",
    "            pickle.dump(obj=scaler, file=f)\n",
    "\n",
    "def load_hard_sharing_model(*, A, X, E, tasks, conv='ecc', \n",
    "                            mode='batch', task_to_scaler=dict()):\n",
    "    model, _ = build_hard_sharing_model(A=A, X=X, E=E, conv=conv, mode=mode,\n",
    "                                     num_tasks=len(tasks))\n",
    "    model.load_weights(generate_model_filename(tasks, conv=conv, mode=mode))\n",
    "    for task in tasks:\n",
    "        if task not in task_to_scaler:\n",
    "            with open(generate_task_scaler_filename(task), 'rb') as f:\n",
    "                task_to_scaler[task] = pickle.load(f)\n",
    "    return model, task_to_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask_disjoint(model, cluster, *, opt, loss_fn, batch_size, \n",
    "                             epochs, A_train, X_train, E_train, y_train, \n",
    "                             epoch_num=1, loss_logger=None):\n",
    "    F, S = get_shape_params(A=A_train, X=X_train, E=E_train, mode='disjoint')\n",
    "    @tf.function(\n",
    "        input_signature=(tf.TensorSpec((None, F), dtype=tf.float64),\n",
    "                         tf.SparseTensorSpec((None, None), dtype=tf.float64),\n",
    "                         tf.TensorSpec((None, S), dtype=tf.float64),\n",
    "                         tf.TensorSpec((None,), dtype=tf.int32),\n",
    "                         tf.TensorSpec((None, len(cluster)), dtype=tf.float64)),\n",
    "        experimental_relax_shapes=True)\n",
    "    def train_step(X_, A_, E_, I_, y_):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model([X_, A_, E_, I_], training=True)\n",
    "            loss = loss_fn(y_, predictions)\n",
    "            loss += sum(model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    current_batch = 0\n",
    "    model_loss = 0\n",
    "    batches_in_epoch = np.ceil(len(A_train) / batch_size)\n",
    "\n",
    "    print('Fitting model')\n",
    "    batches_train = batch_iterator([X_train, A_train, E_train, y_train[cluster].values],\n",
    "                                   batch_size=batch_size, epochs=epochs)\n",
    "    for b in batches_train:\n",
    "        X_, A_, E_, I_ = numpy_to_disjoint(*b[:-1])\n",
    "        A_ = ops.sp_matrix_to_sp_tensor(A_)\n",
    "        y_ = b[-1]\n",
    "        outs = train_step(X_, A_, E_, I_, y_)\n",
    "\n",
    "        model_loss += outs.numpy()\n",
    "        current_batch += 1\n",
    "        if current_batch == batches_in_epoch:\n",
    "            print('Loss: {}'.format(model_loss / batches_in_epoch))\n",
    "            if loss_logger is not None:\n",
    "                loss_logger.losses[epoch_num] = model_loss / batches_in_epoch\n",
    "            model_loss = 0\n",
    "            current_batch = 0\n",
    "            epoch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multitask_disjoint(model, cluster, *, loss_fn, batch_size, \n",
    "                            A_test, X_test, E_test, y_test,\n",
    "                            task_to_scaler=None, model_data=None):\n",
    "    print('Testing model')\n",
    "    model_loss = 0\n",
    "    batches_in_epoch = np.ceil(len(A_test) / batch_size)\n",
    "    batches_test = batch_iterator([X_test, A_test, E_test, y_test[cluster].values],\n",
    "                                  batch_size=batch_size)\n",
    "    for b in batches_test:\n",
    "        X_, A_, E_, I_ = numpy_to_disjoint(*b[:-1])\n",
    "        A_ = ops.sp_matrix_to_sp_tensor(A_)\n",
    "        y_ = b[3]\n",
    "\n",
    "        predictions = model([X_, A_, E_, I_], training=False)\n",
    "        model_loss += loss_fn(y_, predictions)\n",
    "        \n",
    "        if all([task_to_scaler, model_data]):\n",
    "            if len(cluster) == 1:\n",
    "                predictions = [predictions]\n",
    "            y_ = np.hsplit(y_, len(cluster))\n",
    "            for task, batch_pred, batch_actual in zip(cluster, predictions, y_):\n",
    "                scaler = task_to_scaler[task]\n",
    "                batch_pred = scaler.inverse_transform(batch_pred)\n",
    "                batch_actual = scaler.inverse_transform(batch_actual)\n",
    "                for pred, actual in zip(batch_pred, batch_actual):\n",
    "                    model_data.add_test(task, pred=pred[0], actual=actual[0])\n",
    "\n",
    "    model_loss /= batches_in_epoch\n",
    "    print('Done. Test loss: {}'.format(model_loss))\n",
    "    \n",
    "    if model_data:\n",
    "        model_data.test_loss = model_loss\n",
    "\n",
    "    return model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_property(prop, mol_id, clusters, *, X_all, A_all, E_all,\n",
    "                     mode='batch', conv='ecc', model=None, \n",
    "                     task_to_scaler=dict()):\n",
    "    \n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")  \n",
    "    if conv not in ['ecc', 'gin']:\n",
    "        raise ValueError(f\"convolution layer {conv} not recognized; \"\n",
    "                         \"choose 'ecc' or 'gin'\")\n",
    "    \n",
    "    cluster = [c for c in clusters if prop in c][0]\n",
    "    if model is None:\n",
    "        model, task_to_scaler = load_hard_sharing_model(A=A_all, \n",
    "                                                        X=X_all, \n",
    "                                                        E=E_all, \n",
    "                                                        tasks=cluster,\n",
    "                                                        mode=mode, \n",
    "                                                        conv=conv, \n",
    "                                                        task_to_scaler=task_to_scaler)\n",
    "    i = mol_id - 1\n",
    "\n",
    "    # convert shape for batch mode\n",
    "    if mode == 'batch':\n",
    "        def wrap(a):\n",
    "            return a.reshape([1] + list(a.shape))\n",
    "        x = list(map(wrap, [X_all[i], A_all[i], E_all[i]]))\n",
    "        cluster_prediction = model.predict(x)       \n",
    "    \n",
    "    if mode == 'disjoint':\n",
    "        X_, A_, E_, I_ = numpy_to_disjoint([X_all[i]], [A_all[i]], [E_all[i]])\n",
    "        A_ = ops.sp_matrix_to_sp_tensor(A_)\n",
    "        cluster_prediction = model([X_, A_, E_, I_], training=False)\n",
    "    \n",
    "    prediction = cluster_prediction[cluster.index(prop)]\n",
    "    prediction = task_to_scaler[prop].inverse_transform(prediction)\n",
    "    return prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLoggerCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = dict()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.losses[epoch] = logs[\"loss\"]\n",
    "\n",
    "class ModelData:\n",
    "    def __init__(self, params=dict()):\n",
    "        self.timestamp = datetime.now()\n",
    "        self.loss_logger = LossLoggerCallback()\n",
    "        \n",
    "        \"\"\"\n",
    "        Possible params keys:\n",
    "        mode: 'batch' or 'disjoint'\n",
    "        conv: 'ecc' or 'gin'\n",
    "        single_task: true or false\n",
    "        cluster: only if single_task is false\n",
    "        hard_sharing: true or false, only if single_task is false \n",
    "        soft_weight: only if hard_sharing is false and single_task is false\n",
    "        batch_size\n",
    "        epochs\n",
    "        num_sampled\n",
    "        learning_rate\n",
    "        model_summary\n",
    "        loss_fn: string of loss function name\n",
    "        optimizer: string of optimizer name\n",
    "        layer_sizes: List[Int] [gc1, gc2, pool, dense]\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        \n",
    "        self.actual = dict() # task -> List[Float]\n",
    "        self.pred = dict() # task -> List[Float]\n",
    "\n",
    "        self.test_loss = None\n",
    "        \n",
    "    def get_losses(self):\n",
    "        return self.loss_logger.losses\n",
    "    \n",
    "    def add_test(self, task, *, actual, pred):\n",
    "        if task not in self.actual:\n",
    "            self.actual[task] = list()\n",
    "            self.pred[task] = list()\n",
    "        \n",
    "        self.actual[task].append(actual)\n",
    "        self.pred[task].append(pred)\n",
    "    \n",
    "    def _make_picklable(self):\n",
    "        return {'params': self.params, \n",
    "                'actual': self.actual, \n",
    "                'pred': self.pred, \n",
    "                'losses': self.loss_logger.losses,\n",
    "                'test_loss': self.test_loss}\n",
    "    \n",
    "    def serialize(self, dirname='model_data', filename=''):\n",
    "        if filename == '':\n",
    "            dt_string = self.timestamp.strftime('%d-%m-%Y_%H-%M-%S')\n",
    "            filename = path.join(dirname, dt_string + '.pkl')\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(self._make_picklable(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    mode = 'disjoint'\n",
    "    conv = 'gin'\n",
    "    batch_size = 32\n",
    "    epochs = 40\n",
    "    num_sampled = 30000\n",
    "    learning_rate = 1e-2\n",
    "    learning_rate_scheduler = 2\n",
    "    epochs_per_schedule = 5\n",
    "    amount = None\n",
    "    scaler = 'power_transformer'\n",
    "    loss_fn = MeanAbsoluteError()\n",
    "    A_all, X_all, E_all, y_all = load_data(amount=amount, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals(): \n",
    "    A, X, E, y = sample_from_data(num_sampled, \n",
    "                                  A_all, \n",
    "                                  X_all, \n",
    "                                  E_all, \n",
    "                                  y_all, \n",
    "                                  mode=mode)\n",
    "    task_to_scaler = standardize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [['A', 'lumo', 'homo'], \n",
    "            ['B', 'r2', 'cv'], \n",
    "            ['alpha', 'zpve'], \n",
    "            ['C', 'u0', 'u298', 'mu'], \n",
    "            ['g298', 'h298']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():     \n",
    "    A_train, A_test, \\\n",
    "        X_train, X_test, \\\n",
    "        E_train, E_test, \\\n",
    "        y_train, y_test = train_test_split(A, X, E, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    print('begin training models')\n",
    "   \n",
    "    tasks = [[task] for cluster in clusters for task in cluster]\n",
    "    clusters_alt = [['B', 'g298'], ['alpha', 'zpve'], ['C', 'u0', 'u298'], ['r2', 'cv'], ['h298', 'mu']]\n",
    "    tasks_and_clusters = itertools.chain(tasks, clusters, clusters_alt)\n",
    "    for cluster, conv in itertools.product(tasks_and_clusters,\n",
    "                                           ['ecc', 'gin']):\n",
    "        print(f'training {cluster} with {mode} mode on {conv} conv')\n",
    "        \n",
    "        model, _ = build_hard_sharing_model(A=A_train, \n",
    "                                                  X=X_train, \n",
    "                                                  E=E_train, \n",
    "                                                  num_tasks=len(cluster),\n",
    "                                                  mode=mode,\n",
    "                                                  conv=conv)\n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "        loss_fn = MeanSquaredError()\n",
    "        \n",
    "        stream = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
    "        summary = stream.getvalue()\n",
    "        \n",
    "        params = {'mode': mode, \n",
    "                  'conv': conv,\n",
    "                  'batch_size': batch_size,\n",
    "                  'epochs': epochs,\n",
    "                  'num_sampled': num_sampled,\n",
    "                  'learning_rate': learning_rate,\n",
    "                  'cluster': cluster,\n",
    "                  'hard_sharing': True,\n",
    "                  'model_summary': summary,\n",
    "                  'loss_fn': type(loss_fn).__name__,\n",
    "                  'optimizer': type(optimizer).__name__, \n",
    "                  'learning_rate_scheduler': learning_rate_scheduler, \n",
    "                  'epochs_per_schedule': epochs_per_schedule}\n",
    "        model_data = ModelData(params=params)\n",
    "                \n",
    "        if mode == 'batch':\n",
    "            # training\n",
    "            y_train_cluster = np.hsplit(y_train[cluster].values, len(cluster))\n",
    "            model.compile(optimizer=optimizer, \n",
    "                          loss=loss_fn)\n",
    "            model.fit(x=[X_train, A_train, E_train], \n",
    "                      y=y_train_cluster,\n",
    "                      batch_size=batch_size,\n",
    "                      validation_split=0.1,\n",
    "                      epochs=epochs,\n",
    "                      callbacks=[model_data.loss_logger])\n",
    "            \n",
    "            # testing\n",
    "            y_test_cluster = np.hsplit(y_test[cluster].values, len(cluster))\n",
    "            model_loss = model.evaluate(x=[X_test, A_test, E_test],\n",
    "                                        y=y_test_cluster)\n",
    "            print(f\"Test loss on {cluster}: {model_loss}\")\n",
    "            cluster_pred = model.predict([X_test, A_test, E_test])\n",
    "\n",
    "        if mode == 'disjoint':\n",
    "            # training with learning rate decay\n",
    "            for i in range(epochs//epochs_per_schedule):\n",
    "                if i > 0:\n",
    "                    learning_rate /= learning_rate_scheduler\n",
    "                optimizer = Adam(lr=learning_rate)\n",
    "                train_multitask_disjoint(model,\n",
    "                                         cluster,\n",
    "                                         opt=optimizer,\n",
    "                                         loss_fn=loss_fn,\n",
    "                                         batch_size=batch_size,\n",
    "                                         epochs=epochs_per_schedule,\n",
    "                                         epoch_num=i*epochs_per_schedule+1,\n",
    "                                         A_train=A_train,\n",
    "                                         X_train=X_train,\n",
    "                                         E_train=E_train,\n",
    "                                         y_train=y_train, \n",
    "                                         loss_logger=model_data.loss_logger)\n",
    "            # testing\n",
    "            model_loss = test_multitask_disjoint(model,\n",
    "                                                cluster,\n",
    "                                                loss_fn=loss_fn,\n",
    "                                                batch_size=batch_size,\n",
    "                                                A_test=A_test,\n",
    "                                                X_test=X_test,\n",
    "                                                E_test=E_test,\n",
    "                                                y_test=y_test)\n",
    "            X_, A_, E_, I_ = numpy_to_disjoint(X_test, A_test, E_test)\n",
    "            A_ = ops.sp_matrix_to_sp_tensor(A_)\n",
    "            cluster_pred = model([X_, A_, E_, I_], training=False)\n",
    "        \n",
    "        if len(cluster) == 1:\n",
    "            cluster_pred = [cluster_pred]\n",
    "            \n",
    "        for prop, batch_pred in zip(cluster, cluster_pred):\n",
    "            batch_pred = task_to_scaler[prop].inverse_transform(batch_pred)\n",
    "            errors = list()\n",
    "            for index, pred in zip(y_test.index.values, batch_pred):\n",
    "                actual = y_all.loc[index, prop]\n",
    "                model_data.add_test(prop, actual, pred[0])\n",
    "        \n",
    "        # save_model(model, cluster, task_to_scaler, mode=mode, conv=conv)\n",
    "        model_data.serialize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
