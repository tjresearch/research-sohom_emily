{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.backend import mean, square\n",
    "\n",
    "from spektral.datasets import qm9\n",
    "from spektral.layers import EdgeConditionedConv, GlobalSumPool, GlobalAttentionPool\n",
    "from spektral.utils import label_to_one_hot\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A_all, X_all, E_all, y_all = qm9.load_data(return_type='numpy',\n",
    "                           nf_keys='atomic_num',\n",
    "                           ef_keys='type',\n",
    "                           self_loops=True,\n",
    "                           amount=None) # load entire dataset\n",
    "# Preprocessing\n",
    "X_uniq = np.unique(X_all)\n",
    "X_uniq = X_uniq[X_uniq != 0]\n",
    "E_uniq = np.unique(E_all)\n",
    "E_uniq = E_uniq[E_uniq != 0]\n",
    "\n",
    "X_all = label_to_one_hot(X_all, X_uniq)\n",
    "E_all = label_to_one_hot(E_all, E_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N = X_all.shape[-2]       # Number of nodes in the graphs\n",
    "F = X_all[0].shape[-1]    # Dimension of node features\n",
    "S = E_all[0].shape[-1]    # Dimension of edge features\n",
    "n_out = y_all.shape[-1]   # Dimension of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we don't want to train only on the lightest molecules\n",
    "# we randomly sample from the dataset\n",
    "indices = np.random.choice(X_all.shape[0], 10000, replace=False)\n",
    "X = X_all[indices, :, :]\n",
    "A = A_all[indices, :, :]\n",
    "E = E_all[indices, :, :, :]\n",
    "y = y_all.iloc[indices, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Yeo-Johnson transformation\n",
    "# to reduce skew and standardize data\n",
    "task_to_scaler = dict()\n",
    "for task in list(y.columns)[1:]:\n",
    "    # scaler = StandardScaler()\n",
    "    scaler = PowerTransformer()\n",
    "    y.loc[:, task] = scaler.fit_transform(y[[task]])\n",
    "    task_to_scaler[task] = scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [['A', 'B', 'alpha'], \n",
    "            ['C', 'r2', 'u0'],\n",
    "            ['zpve', 'g298', 'cv'],\n",
    "            ['lumo', 'u298', 'h298'],\n",
    "            ['mu', 'homo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train, A_test, \\\n",
    "    X_train, X_test, \\\n",
    "    E_train, E_test, \\\n",
    "    y_train, y_test = train_test_split(A, X, E, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_task_model(*, N, F, S):\n",
    "    X_in = Input(shape=(N, F))\n",
    "    A_in = Input(shape=(N, N))\n",
    "    E_in = Input(shape=(N, N, S))\n",
    "\n",
    "    gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "    gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    pool = GlobalAttentionPool(256)(gc2)\n",
    "    dense = Dense(256, activation='relu')(pool)\n",
    "    output = Dense(1)(dense)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=[X_in, A_in, E_in], outputs=output)\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hard_sharing_model(*, N, F, S, num_tasks):\n",
    "    X_in = Input(shape=(N, F))\n",
    "    A_in = Input(shape=(N, N))\n",
    "    E_in = Input(shape=(N, N, S))\n",
    "\n",
    "    gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "    gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    pool = GlobalAttentionPool(256)(gc2)\n",
    "    dense_list = [Dense(256, activation='relu')(pool) for i in range(num_tasks)]\n",
    "    output_list = [Dense(1)(dense_layer) for dense_layer in dense_list]\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_soft_sharing_model(*, N, F, S, num_tasks, share_param):\n",
    "    X_in = Input(shape=(N, F))\n",
    "    A_in = Input(shape=(N, N))\n",
    "    E_in = Input(shape=(N, N, S))\n",
    "\n",
    "    gc1_list = [EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in]) for i in range(num_tasks)]\n",
    "    gc2_list = [EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in]) for gc1 in gc1_list]\n",
    "    pool_list = [GlobalAttentionPool(256)(gc2) for gc2 in gc2_list]\n",
    "    dense_list = [Dense(256, activation='relu')(pool) for pool in pool_list]\n",
    "    output_list = [Dense(1)(dense) for dense in dense_list]\n",
    "\n",
    "    def loss(y_actual, y_pred):\n",
    "        avg_layer_diff = 0\n",
    "        for i in range(num_tasks):\n",
    "            for j in range(i):\n",
    "                for gc in [gc1_list, gc2_list]:\n",
    "                      avg_layer_diff += mean(square(gc[i].trainable_weights - gc[j].trainable_weights))\n",
    "        avg_layer_diff /= (num_tasks)*(num_tasks-1)/2  \n",
    "        return mean(square(y_actual - y_pred)) + share_param*avg_layer_diff\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOLDER_PATH = '/content/drive/My Drive/Colab Notebooks/demo_models'\n",
    "FOLDER_PATH = 'demo_models'\n",
    "\n",
    "def generate_model_filename(tasks):\n",
    "    filename = \"\".join(sorted(tasks))\n",
    "    return path.join(FOLDER_PATH, f'{filename}.h5')\n",
    "\n",
    "def generate_task_scaler_filename(task):\n",
    "    return path.join(FOLDER_PATH, f'{task}_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tasks):\n",
    "    model.save_weights(generate_model_filename(tasks))\n",
    "    for task in tasks:\n",
    "        scaler_filename = generate_task_scaler_filename(task)\n",
    "        with open(scaler_filename, 'wb') as f:\n",
    "#       print(task_to_scaler[task].mean_[0], file=f)\n",
    "#       print(task_to_scaler[task].scale_[0], file=f)\n",
    "            scaler = task_to_scaler[task]\n",
    "            pickle.dump(obj=scaler, file=f)\n",
    "\n",
    "def load_hard_sharing_model(*, N, F, S, tasks, task_to_scaler=dict()):\n",
    "    model = build_hard_sharing_model(N=N, F=F, S=S, num_tasks=len(tasks))\n",
    "    model.load_weights(generate_model_filename(tasks))\n",
    "    for task in tasks:\n",
    "        if task not in task_to_scaler:\n",
    "            with open(generate_task_scaler_filename(task), 'rb') as f:\n",
    "                task_to_scaler[task] = pickle.load(f)\n",
    "    return model, task_to_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_property(prop, mol_id, clusters, N=N, F=F, S=S):\n",
    "    for cluster in clusters:\n",
    "        if prop in cluster:\n",
    "            model, task_to_scaler = load_hard_sharing_model(N=N, F=F, S=S, tasks=cluster, task_to_scaler=task_to_scaler)\n",
    "            i = mol_id - 1\n",
    "\n",
    "            # convert shape for batch mode\n",
    "            def wrap(a):\n",
    "                return a.reshape([1] + list(a.shape))\n",
    "            x = list(map(wrap, [X_all[i], A_all[i], E_all[i]]))\n",
    "            \n",
    "            cluster_prediction = model.predict(x)\n",
    "            prediction = cluster_prediction[cluster.index(prop)]\n",
    "            prediction = task_to_scaler[prop].inverse_transform(prediction)\n",
    "            return prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    print('begin training models')\n",
    "    for cluster in clusters:\n",
    "        print(f'training {cluster}')\n",
    "        model = build_hard_sharing_model(N=N, F=F, S=S, num_tasks=len(cluster))\n",
    "        y_train_cluster = np.hsplit(y_train[cluster].values, len(cluster))\n",
    "        model.fit(x=[X_train, A_train, E_train], \n",
    "                  y=y_train_cluster,\n",
    "                  batch_size=128,\n",
    "                  validation_split=0.1,\n",
    "                  epochs=3)\n",
    "        save_model(model, cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():   \n",
    "    for cluster in clusters:\n",
    "        model, _ = load_hard_sharing_model(N=N, F=F, S=S, tasks=clusters[0])\n",
    "        y_test_cluster = np.hsplit(y_test[cluster].values, len(cluster))\n",
    "        model_loss = model.evaluate(x=[X_test, A_test, E_test],\n",
    "                                    y=y_test_cluster)\n",
    "        print(f\"Test loss: {model_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    print(predict_property('A', 1, clusters, N=N, F=F, S=S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
