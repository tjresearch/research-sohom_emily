{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017 DeepCrystal Technologies - Patrick Hop\n",
    "\n",
    "Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs\n",
    "\n",
    "MIT License - have fun!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n",
      "3.6.8 (default, Jan 14 2019, 11:02:34) \n",
      "[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\n",
      "sys.version_info(major=3, minor=6, micro=8, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepchem'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-98a8c6c342ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdeepchem\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataStructs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAllChem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepchem'"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5d111a111359>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-350a373df33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import OrderedDict\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import donkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2)\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'az_ppb.csv'\n",
    "print(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3\n",
    "BATCH_SIZE = 48\n",
    "MAXITER = 40000\n",
    "LIMIT = 0\n",
    "LR = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = nn.Linear(150, 128)\n",
    "U = {0: nn.Linear(156, 75), 1: nn.Linear(156, 75), 2: nn.Linear(156, 75)}\n",
    "V = {0: nn.Linear(75, 75), 1: nn.Linear(75, 75), 2: nn.Linear(75, 75)}\n",
    "E = nn.Linear(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "  \"\"\"Sets the learning rate to the initial LR decayed by .8 every 5 epochs\"\"\"\n",
    "  lr = LR * (0.9 ** (epoch // 10))\n",
    "  print('new lr [%.5f]' % lr)\n",
    "  for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "  train_features, train_labels, val_features, val_labels = donkey.load_dataset(DATASET)\n",
    "\n",
    "  scaler = preprocessing.StandardScaler().fit(train_labels)\n",
    "  train_labels = scaler.transform(train_labels)\n",
    "  val_labels = scaler.transform(val_labels)\n",
    "\n",
    "  train_labels = Variable(torch.FloatTensor(train_labels), requires_grad=False)\n",
    "  val_labels = Variable(torch.FloatTensor(val_labels), requires_grad=False)\n",
    "  \n",
    "  return train_features, train_labels, val_features, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout(h, h2):\n",
    "  catted_reads = map(lambda x: torch.cat([h[x[0]], h2[x[1]]], 1), zip(h2.keys(), h.keys()))\n",
    "  activated_reads = map(lambda x: F.selu( R(x) ), catted_reads)\n",
    "  readout = Variable(torch.zeros(1, 128))\n",
    "  for read in activated_reads:\n",
    "    readout = readout + read\n",
    "  return F.tanh( readout )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_pass(g, h, k):\n",
    "  for v in g.keys():\n",
    "    neighbors = g[v]\n",
    "    for neighbor in neighbors:\n",
    "      e_vw = neighbor[0] # feature variable\n",
    "      w = neighbor[1]\n",
    "      \n",
    "      m_w = V[k](h[w])\n",
    "      m_e_vw = E(e_vw)\n",
    "      reshaped = torch.cat( (h[v], m_w, m_e_vw), 1)\n",
    "      h[v] = F.selu(U[k](reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_multigraph(smile):\n",
    "  g = OrderedDict({})\n",
    "  h = OrderedDict({})\n",
    "\n",
    "  molecule = Chem.MolFromSmiles(smile)\n",
    "  for i in xrange(0, molecule.GetNumAtoms()):\n",
    "    atom_i = molecule.GetAtomWithIdx(i)\n",
    "    h[i] = Variable(torch.FloatTensor(dc.feat.graph_features.atom_features(atom_i))).view(1, 75)\n",
    "    for j in xrange(0, molecule.GetNumAtoms()):\n",
    "      e_ij = molecule.GetBondBetweenAtoms(i, j)\n",
    "      if e_ij != None:\n",
    "        e_ij =  map(lambda x: 1 if x == True else 0, dc.feat.graph_features.bond_features(e_ij)) # ADDED edge feat\n",
    "        e_ij = Variable(torch.FloatTensor(e_ij).view(1, 6))\n",
    "        atom_j = molecule.GetAtomWithIdx(j)\n",
    "        if i not in g:\n",
    "          g[i] = []\n",
    "        g[i].append( (e_ij, j) )\n",
    "\n",
    "  return g, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smiles, train_labels, val_smiles, val_labels = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(128, 1)\n",
    "params = [{'params': R.parameters()},\n",
    "         {'params': U[0].parameters()},\n",
    "         {'params': U[1].parameters()},\n",
    "         {'params': U[2].parameters()},\n",
    "         {'params': E.parameters()},\n",
    "         {'params': V[0].parameters()},\n",
    "         {'params': V[1].parameters()},\n",
    "         {'params': V[2].parameters()},\n",
    "         {'params': linear.parameters()}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 0\n",
    "optimizer = optim.Adam(params, lr=LR, weight_decay=1e-4)\n",
    "for i in xrange(0, MAXITER):\n",
    "  optimizer.zero_grad()\n",
    "  train_loss = Variable(torch.zeros(1, 1))\n",
    "  y_hats_train = []\n",
    "  for j in xrange(0, BATCH_SIZE):\n",
    "    sample_index = random.randint(0, len(train_smiles) - 2)\n",
    "    smile = train_smiles[sample_index]\n",
    "    g, h = construct_multigraph(smile) # TODO: cache this\n",
    "\n",
    "    g2, h2 = construct_multigraph(smile)\n",
    "    \n",
    "    for k in xrange(0, T):\n",
    "      message_pass(g, h, k)\n",
    "\n",
    "    x = readout(h, h2)\n",
    "    #x = F.selu( fc(x) )\n",
    "    y_hat = linear(x)\n",
    "    y = train_labels[sample_index]\n",
    "\n",
    "    y_hats_train.append(y_hat)\n",
    "\n",
    "    error = (y_hat - y)*(y_hat - y) / Variable(torch.FloatTensor([BATCH_SIZE])).view(1, 1)\n",
    "    train_loss = train_loss + error\n",
    "\n",
    "  train_loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  if i % int(len(train_smiles) / BATCH_SIZE) == 0:\n",
    "    val_loss = Variable(torch.zeros(1, 1), requires_grad=False)\n",
    "    y_hats_val = []\n",
    "    for j in xrange(0, len(val_smiles)):\n",
    "      g, h = construct_multigraph(val_smiles[j])\n",
    "      g2, h2 = construct_multigraph(val_smiles[j])\n",
    "\n",
    "      for k in xrange(0, T):\n",
    "        message_pass(g, h, k)\n",
    "\n",
    "      x = readout(h, h2)\n",
    "      #x = F.selu( fc(x) )\n",
    "      y_hat = linear(x)\n",
    "      y = val_labels[j]\n",
    "\n",
    "      y_hats_val.append(y_hat)\n",
    "\n",
    "      error = (y_hat - y)*(y_hat - y) / Variable(torch.FloatTensor([len(val_smiles)])).view(1, 1)\n",
    "      val_loss = val_loss + error\n",
    "\n",
    "    y_hats_val = np.array(map(lambda x: x.data.numpy(), y_hats_val))\n",
    "    y_val = np.array(map(lambda x: x.data.numpy(), val_labels))\n",
    "    y_hats_val = y_hats_val.reshape(-1, 1)\n",
    "    y_val = y_val.reshape(-1, 1)\n",
    "    \n",
    "    r2_val_old = r2_score(y_val, y_hats_val)\n",
    "    r2_val_new = pearsonr(y_val, y_hats_val)[0]**2\n",
    "  \n",
    "    train_loss_ = train_loss.data.numpy()[0]\n",
    "    val_loss_ = val_loss.data.numpy()[0]\n",
    "    print 'epoch [%i/%i] train_loss [%f] val_loss [%f] r2_val_old [%.4f], r2_val_new [%.4f]' \\\n",
    "                  % (num_epoch, 100, train_loss_, val_loss_, r2_val_old, r2_val_new)\n",
    "    num_epoch += 1\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
