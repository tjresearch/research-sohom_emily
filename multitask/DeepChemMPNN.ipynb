{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "RDKit WARNING: [09:21:13] Enabling RDKit 2019.09.3 jupyter extensions\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sohompaul/psi4conda/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.utils import ScaffoldGenerator\n",
    "from deepchem.utils.save import log\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import OrderedDict\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fee6089fb90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(2)\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scaffold(smiles, include_chirality=False):\n",
    "    \"\"\"Compute the Bemis-Murcko scaffold for a SMILES string.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    engine = ScaffoldGenerator(include_chirality=include_chirality)\n",
    "    scaffold = engine.get_scaffold(mol)\n",
    "    return scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataset,\n",
    "          frac_train=.80,\n",
    "          frac_valid=.10,\n",
    "          frac_test=.10,\n",
    "          log_every_n=1000):\n",
    "    \"\"\"\n",
    "    Splits internal compounds into train/validation/test by scaffold.\n",
    "    \"\"\"\n",
    "    np.testing.assert_almost_equal(frac_train + frac_valid + frac_test, 1.)\n",
    "    scaffolds = {}\n",
    "    log(\"About to generate scaffolds\", True)\n",
    "    data_len = len(dataset)\n",
    "\n",
    "    for ind, smiles in enumerate(dataset):\n",
    "        if ind % log_every_n == 0:\n",
    "            log(\"Generating scaffold %d/%d\" % (ind, data_len), True)\n",
    "        scaffold = generate_scaffold(smiles)\n",
    "        if scaffold not in scaffolds:\n",
    "            scaffolds[scaffold] = [ind]\n",
    "        else:\n",
    "            scaffolds[scaffold].append(ind)\n",
    "\n",
    "    scaffolds = {key: sorted(value) for key, value in scaffolds.items()}\n",
    "    scaffold_sets = [\n",
    "        scaffold_set\n",
    "        for (scaffold, scaffold_set) in sorted(\n",
    "            scaffolds.items(), key=lambda x: (len(x[1]), x[1][0]), reverse=True)\n",
    "    ]\n",
    "    train_cutoff = frac_train * len(dataset)\n",
    "    valid_cutoff = (frac_train + frac_valid) * len(dataset)\n",
    "    train_inds, valid_inds, test_inds = [], [], []\n",
    "    log(\"About to sort in scaffold sets\", True)\n",
    "    for scaffold_set in scaffold_sets:\n",
    "        if len(train_inds) + len(scaffold_set) > train_cutoff:\n",
    "            if len(train_inds) + len(valid_inds) + len(scaffold_set) > valid_cutoff:\n",
    "                test_inds += scaffold_set    \n",
    "            else:\n",
    "                valid_inds += scaffold_set\n",
    "        else:\n",
    "            train_inds += scaffold_set\n",
    "    return train_inds, valid_inds, test_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donkey_load_dataset(filename, whiten=False):\n",
    "    f = open(filename, 'r')\n",
    "    features = []\n",
    "    labels = []\n",
    "    tracer = 0\n",
    "    for line in f:\n",
    "        if tracer == 0:\n",
    "            tracer += 1\n",
    "            continue\n",
    "        splits =  line[:-1].split(',')\n",
    "        features.append(splits[-1])\n",
    "        labels.append(float(splits[-2]))\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels, dtype='float32').reshape(-1, 1)\n",
    "\n",
    "    train_ind, val_ind, test_ins = split(features)\n",
    "\n",
    "    train_features = np.take(features, train_ind)\n",
    "    train_labels = np.take(labels, train_ind)\n",
    "    val_features = np.take(features, val_ind)\n",
    "    val_labels = np.take(labels, val_ind)\n",
    "\n",
    "    return train_features, train_labels, val_features, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nci_1.csv\n"
     ]
    }
   ],
   "source": [
    "# DATASET = 'az_ppb.csv'\n",
    "# DATASET = 'az_ppb_last_col_dropped.csv'\n",
    "DATASET = 'nci_1.csv'\n",
    "print(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3\n",
    "BATCH_SIZE = 48\n",
    "MAXITER = 40000\n",
    "LIMIT = 0\n",
    "LR = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = nn.Linear(150, 128)\n",
    "U = {0: nn.Linear(156, 75), 1: nn.Linear(156, 75), 2: nn.Linear(156, 75)}\n",
    "V = {0: nn.Linear(75, 75), 1: nn.Linear(75, 75), 2: nn.Linear(75, 75)}\n",
    "E = nn.Linear(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by .8 every 5 epochs\"\"\"\n",
    "    lr = LR * (0.9 ** (epoch // 10))\n",
    "    print('new lr [%.5f]' % lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_features, train_labels, val_features, val_labels = donkey_load_dataset(DATASET)\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(train_labels)\n",
    "    train_labels = scaler.transform(train_labels)\n",
    "    val_labels = scaler.transform(val_labels)\n",
    "\n",
    "    train_labels = Variable(torch.FloatTensor(train_labels), requires_grad=False)\n",
    "    val_labels = Variable(torch.FloatTensor(val_labels), requires_grad=False)\n",
    "\n",
    "    return train_features, train_labels, val_features, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout(h, h2):\n",
    "    catted_reads = map(lambda x: torch.cat([h[x[0]], h2[x[1]]], 1), zip(h2.keys(), h.keys()))\n",
    "    activated_reads = map(lambda x: F.selu( R(x) ), catted_reads)\n",
    "    readout = Variable(torch.zeros(1, 128))\n",
    "    for read in activated_reads:\n",
    "        readout = readout + read\n",
    "    return F.tanh(readout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_pass(g, h, k):\n",
    "    for v in g.keys():\n",
    "        neighbors = g[v]\n",
    "        for neighbor in neighbors:\n",
    "            e_vw = neighbor[0] # feature variable\n",
    "            w = neighbor[1]\n",
    "\n",
    "            m_w = V[k](h[w])\n",
    "            m_e_vw = E(e_vw)\n",
    "            reshaped = torch.cat( (h[v], m_w, m_e_vw), 1)\n",
    "            h[v] = F.selu(U[k](reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_multigraph(smile):\n",
    "    g = OrderedDict({})\n",
    "    h = OrderedDict({})\n",
    "    molecule = Chem.MolFromSmiles(smile)\n",
    "    for i in xrange(0, molecule.GetNumAtoms()):\n",
    "        atom_i = molecule.GetAtomWithIdx(i)\n",
    "        h[i] = Variable(torch.FloatTensor(dc.feat.graph_features.atom_features(atom_i))).view(1, 75)\n",
    "        for j in xrange(0, molecule.GetNumAtoms()):\n",
    "            e_ij = molecule.GetBondBetweenAtoms(i, j)\n",
    "            if e_ij != None:\n",
    "                e_ij =  map(lambda x: 1 if x == True else 0, dc.feat.graph_features.bond_features(e_ij)) # ADDED edge feat\n",
    "                e_ij = Variable(torch.FloatTensor(e_ij).view(1, 6))\n",
    "                atom_j = molecule.GetAtomWithIdx(j)\n",
    "                if i not in g:\n",
    "                    g[i] = []\n",
    "                g[i].append( (e_ij, j) )\n",
    "    return g, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to generate scaffolds\n",
      "Generating scaffold 0/528685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [09:25:53] SMILES Parse Error: syntax error while parsing: 1.06\n",
      "RDKit ERROR: [09:25:53] SMILES Parse Error: Failed parsing SMILES '1.06' for input: '1.06'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No molecule provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c02ea85634e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_smiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_smiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-1a6d7445e363>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdonkey_load_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6fd10c5aa895>\u001b[0m in \u001b[0;36mdonkey_load_dataset\u001b[0;34m(filename, whiten)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-1ef9f92bb942>\u001b[0m in \u001b[0;36msplit\u001b[0;34m(dataset, frac_train, frac_valid, frac_test, log_every_n)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_every_n\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating scaffold %d/%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mscaffold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_scaffold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscaffold\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscaffolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mscaffolds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscaffold\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3a4061a7a1dd>\u001b[0m in \u001b[0;36mgenerate_scaffold\u001b[0;34m(smiles, include_chirality)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMolFromSmiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScaffoldGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_chirality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_chirality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mscaffold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scaffold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscaffold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/psi4conda/envs/deepchem/lib/python3.6/site-packages/deepchem/utils/__init__.py\u001b[0m in \u001b[0;36mget_scaffold\u001b[0;34m(self, mol)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScaffolds\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMurckoScaffold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     return MurckoScaffold.MurckoScaffoldSmiles(\n\u001b[0;32m--> 154\u001b[0;31m         mol=mol, includeChirality=self.include_chirality)\n\u001b[0m",
      "\u001b[0;32m~/psi4conda/envs/deepchem/lib/python3.6/site-packages/rdkit/Chem/Scaffolds/MurckoScaffold.py\u001b[0m in \u001b[0;36mMurckoScaffoldSmiles\u001b[0;34m(smiles, mol, includeChirality)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMolFromSmiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No molecule provided'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m   \u001b[0mscaffold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetScaffoldForMol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscaffold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No molecule provided"
     ]
    }
   ],
   "source": [
    "train_smiles, train_labels, val_smiles, val_labels = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(128, 1)\n",
    "params = [{'params': R.parameters()},\n",
    "         {'params': U[0].parameters()},\n",
    "         {'params': U[1].parameters()},\n",
    "         {'params': U[2].parameters()},\n",
    "         {'params': E.parameters()},\n",
    "         {'params': V[0].parameters()},\n",
    "         {'params': V[1].parameters()},\n",
    "         {'params': V[2].parameters()},\n",
    "         {'params': linear.parameters()}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 0\n",
    "optimizer = optim.Adam(params, lr=LR, weight_decay=1e-4)\n",
    "for i in xrange(0, MAXITER):\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = Variable(torch.zeros(1, 1))\n",
    "    y_hats_train = []\n",
    "    for j in xrange(0, BATCH_SIZE):\n",
    "        sample_index = random.randint(0, len(train_smiles) - 2)\n",
    "        smile = train_smiles[sample_index]\n",
    "        g, h = construct_multigraph(smile) # TODO: cache this\n",
    "\n",
    "        g2, h2 = construct_multigraph(smile)\n",
    "\n",
    "        for k in xrange(0, T):\n",
    "            message_pass(g, h, k)\n",
    "\n",
    "        x = readout(h, h2)\n",
    "        #x = F.selu( fc(x) )\n",
    "        y_hat = linear(x)\n",
    "        y = train_labels[sample_index]\n",
    "\n",
    "        y_hats_train.append(y_hat)\n",
    "\n",
    "        error = (y_hat - y)*(y_hat - y) / Variable(torch.FloatTensor([BATCH_SIZE])).view(1, 1)\n",
    "        train_loss = train_loss + error\n",
    "\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % int(len(train_smiles) / BATCH_SIZE) == 0:\n",
    "        val_loss = Variable(torch.zeros(1, 1), requires_grad=False)\n",
    "        y_hats_val = []\n",
    "        for j in xrange(0, len(val_smiles)):\n",
    "            g, h = construct_multigraph(val_smiles[j])\n",
    "            g2, h2 = construct_multigraph(val_smiles[j])\n",
    "\n",
    "            for k in xrange(0, T):\n",
    "                message_pass(g, h, k)\n",
    "\n",
    "            x = readout(h, h2)\n",
    "            #x = F.selu( fc(x) )\n",
    "            y_hat = linear(x)\n",
    "            y = val_labels[j]\n",
    "\n",
    "            y_hats_val.append(y_hat)\n",
    "\n",
    "            error = (y_hat - y)*(y_hat - y) / Variable(torch.FloatTensor([len(val_smiles)])).view(1, 1)\n",
    "            val_loss = val_loss + error\n",
    "\n",
    "    y_hats_val = np.array(map(lambda x: x.data.numpy(), y_hats_val))\n",
    "    y_val = np.array(map(lambda x: x.data.numpy(), val_labels))\n",
    "    y_hats_val = y_hats_val.reshape(-1, 1)\n",
    "    y_val = y_val.reshape(-1, 1)\n",
    "\n",
    "    r2_val_old = r2_score(y_val, y_hats_val)\n",
    "    r2_val_new = pearsonr(y_val, y_hats_val)[0]**2\n",
    "\n",
    "    train_loss_ = train_loss.data.numpy()[0]\n",
    "    val_loss_ = val_loss.data.numpy()[0]\n",
    "    print 'epoch [%i/%i] train_loss [%f] val_loss [%f] r2_val_old [%.4f], r2_val_new [%.4f]' \\\n",
    "                  % (num_epoch, 100, train_loss_, val_loss_, r2_val_old, r2_val_new)\n",
    "    num_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
