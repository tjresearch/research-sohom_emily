{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Python 3 port of the donkey.py file in the DeepChem contrib/mpnn folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.utils import ScaffoldGenerator\n",
    "from deepchem.utils.save import log\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc9c1fc6b70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(2)\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scaffold(smiles, include_chirality=False):\n",
    "    \"\"\"Compute the Bemis-Murcko scaffold for a SMILES string.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    engine = ScaffoldGenerator(include_chirality=include_chirality)\n",
    "    scaffold = engine.get_scaffold(mol)\n",
    "    \n",
    "    return scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataset,\n",
    "         frac_train=0.80,\n",
    "         frac_valid=0.10,\n",
    "         frac_test=0.10,\n",
    "         log_every_n=1000):\n",
    "    \"\"\"Splits internal compounds into train/validation/test by scaffold.\"\"\"\n",
    "    np.testing.assert_almost_equal(frac_train+frac_valid+frac_test, 1.)\n",
    "    scaffolds = {}\n",
    "    log(\"About to generate scaffolds\", True)\n",
    "    data_len = len(dataset)\n",
    "    for ind, smiles in enumerate(dataset):\n",
    "        if ind % log_every_n == 0:\n",
    "            log(f\"Generating scaffold {ind}/{data_len}\", True)\n",
    "        scaffold = generate_scaffold(smiles)\n",
    "        if scaffold not in scaffolds:\n",
    "            scaffolds[scaffold] = [ind]\n",
    "        else:\n",
    "            scaffolds[scaffold].append(ind)\n",
    "    scaffolds = {key: sorted(value) for key, value in scaffold.items()}\n",
    "    scaffold_sets = [\n",
    "        scaffold_set for (scaffold, scaffold_set) in sorted(\n",
    "            scaffold.items(), key=lambda x: (len(x[1]), x[1][0]), reverse=True\n",
    "        )\n",
    "    ]\n",
    "    train_cutoff = frac_train * data_len\n",
    "    valid_cutoff = (frac_train+frac_valid)*data_len\n",
    "    train_inds, valid_inds, test_inds = [], [], []\n",
    "    log(\"About to sort in scaffold sets\", True)\n",
    "    for scaffold_set in scaffold_sets:\n",
    "        if len(train_inds) + len(scaffold_set) > train_cutoff:\n",
    "            if len(train_inds) + len(valid_inds) + len(scaffold_set) > valid_cutoff:\n",
    "                test_inds += scaffold_set\n",
    "            else:\n",
    "                valid_inds += scaffold_set\n",
    "        else:\n",
    "            train_inds += scaffold_set\n",
    "    return train_inds, valid_inds, test_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filenam, whiten=False):\n",
    "    f = open(filename, 'r')\n",
    "    features = []\n",
    "    labels = []\n",
    "    tracer = 0\n",
    "    for line in f:\n",
    "        if trace == 0:\n",
    "            tracer += 1\n",
    "            continue\n",
    "        splits = line[:-1].split(',')\n",
    "        features.append(splits[-1])\n",
    "        labels.append(float(splits[-2]))\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels, dtype='float32').reshape(-1, 1)\n",
    "    \n",
    "    train_ind, val_ind, test_ind = split(features)\n",
    "    \n",
    "    train_features = np.take(features, train_ind)\n",
    "    train_labels = np.take(labels, train_ind)\n",
    "    val_features = np.take(features, val_ind)\n",
    "    val_labels = np.take(labels, val_ind)\n",
    "    \n",
    "    return train_features, train_labels, val_features, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
