{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from tensorflow.keras.backend import mean, square\n",
    "\n",
    "from spektral.datasets import qm9\n",
    "from spektral.layers import EdgeConditionedConv, GINConv, GatedGraphConv\n",
    "from spektral.layers import ops, GlobalSumPool, GlobalAttentionPool\n",
    "from spektral.utils import batch_iterator, numpy_to_disjoint\n",
    "from spektral.utils import label_to_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(amount=None, mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")\n",
    "    \n",
    "    A_all, X_all, E_all, y_all = qm9.load_data(return_type='numpy',\n",
    "                               nf_keys='atomic_num',\n",
    "                               ef_keys='type',\n",
    "                               self_loops=True,\n",
    "                               amount=amount) # None for entire dataset\n",
    "    # Preprocessing\n",
    "    if mode == 'batch':\n",
    "        X_uniq = np.unique(X_all)\n",
    "        X_uniq = X_uniq[X_uniq != 0]\n",
    "        E_uniq = np.unique(E_all)\n",
    "        E_uniq = E_uniq[E_uniq != 0]\n",
    "\n",
    "        X_all = label_to_one_hot(X_all, X_uniq)\n",
    "        E_all = label_to_one_hot(E_all, E_uniq)\n",
    "    elif mode == 'disjoint':\n",
    "        X_uniq = np.unique([v for x in X_all for v in np.unique(x)])\n",
    "        E_uniq = np.unique([v for e in E_all for v in np.unique(e)])\n",
    "        X_uniq = X_uniq[X_uniq != 0]\n",
    "        E_uniq = E_uniq[E_uniq != 0]\n",
    "\n",
    "        X_all = [label_to_one_hot(x, labels=X_uniq) for x in X_all]\n",
    "        E_all = [label_to_one_hot(e, labels=E_uniq) for e in E_all]\n",
    "    \n",
    "    return A_all, X_all, E_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_data(sample_size, A_all, X_all, E_all, y_all, mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")\n",
    "    if mode == 'batch':\n",
    "        indices = np.random.choice(X_all.shape[0], sample_size, replace=False)\n",
    "        A = A_all[indices, :, :]\n",
    "        X = X_all[indices, :, :]\n",
    "        E = E_all[indices, :, :, :]\n",
    "        y = y_all.iloc[indices, :].copy()\n",
    "        \n",
    "    if mode == 'disjoint':\n",
    "        indices = np.random.choice(len(X_all), sample_size, replace=False)\n",
    "        A = [A_all[i] for i in indices]\n",
    "        X = [X_all[i] for i in indices]\n",
    "        E = [E_all[i] for i in indices]\n",
    "        y = y_all.iloc[indices, :].copy()\n",
    "    \n",
    "    return A, X, E, y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(y):\n",
    "    task_to_scaler = dict()\n",
    "    for task in list(y.columns)[1:]:\n",
    "        scaler = PowerTransformer()\n",
    "        y.loc[:, task] = scaler.fit_transform(y[[task]])\n",
    "        task_to_scaler[task] = scaler\n",
    "    return task_to_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_params(*, A, X, E, mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")\n",
    "    F = X[0].shape[-1]  # Dimension of node features\n",
    "    S = E[0].shape[-1]  # Dimension of edge features\n",
    "    if mode == 'batch':\n",
    "        N = X.shape[-2]       # Number of nodes in the graphs\n",
    "        return N, F, S\n",
    "    if mode == 'disjoint':\n",
    "        return F, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tensors(*, A, X, E, mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")\n",
    "    if mode == 'batch':\n",
    "        N, F, S = get_shape_params(A=A, X=X, E=E, mode=mode)\n",
    "        X_in = Input(shape=(N, F), name='X_in')\n",
    "        A_in = Input(shape=(N, N), name='A_in')\n",
    "        E_in = Input(shape=(N, N, S), name='E_in')\n",
    "\n",
    "        return X_in, A_in, E_in\n",
    "    \n",
    "    if mode == 'disjoint':\n",
    "        F, S = get_shape_params(A=A, X=X, E=E, mode=mode)\n",
    "        X_in = Input(shape=(F,), name='X_in')\n",
    "        A_in = Input(shape=(None,), sparse=True, name='A_in')\n",
    "        E_in = Input(shape=(S,), name='E_in')\n",
    "        I_in = Input(shape=(), name='segment_ids_in', dtype=tf.int32)\n",
    "        \n",
    "        return X_in, A_in, E_in, I_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_task_model(*, A, X, E, learning_rate=1e-3, conv='ecc', mode='batch'):  \n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")  \n",
    "    if conv not in ['ecc', 'gin']:\n",
    "        raise ValueError(f\"convolution layer {conv} not recognized; \"\n",
    "                         \"choose 'ecc' or 'gin'\")\n",
    "    \n",
    "    if mode == 'batch':\n",
    "        X_in, A_in, E_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "    if mode == 'disjoint':\n",
    "        X_in, A_in, E_in, I_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "\n",
    "    if conv == 'ecc':    \n",
    "        gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "        gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    if conv == 'gin':\n",
    "        assert mode == 'disjoint', 'cannot run GIN in batch mode'\n",
    "        gc1 = GINConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "        gc2 = GINConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    if mode == 'batch':\n",
    "        pool = GlobalAttentionPool(256)(gc2)\n",
    "    if mode == 'disjoint':\n",
    "        pool = GlobalAttentionPool(256)([gc2, I_in])\n",
    "    dense = Dense(256, activation='relu')(pool)\n",
    "    output = Dense(1)(dense)\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    loss_fn = MeanSquaredError()\n",
    "    if mode == 'batch':\n",
    "        model = Model(inputs=[X_in, A_in, E_in], outputs=output)\n",
    "        model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    if mode == 'disjoint':\n",
    "        model = Model(inputs=[X_in, A_in, E_in, I_in], outputs=output)\n",
    "    \n",
    "    return model, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hard_sharing_model(*, A, X, E, num_tasks, \n",
    "                             learning_rate=1e-3, conv='ecc', mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")  \n",
    "    if conv not in ['ecc', 'gin']:\n",
    "        raise ValueError(f\"convolution layer {conv} not recognized; \"\n",
    "                         \"choose 'ecc' or 'gin'\")\n",
    "    if mode == 'batch':\n",
    "        X_in, A_in, E_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "    if mode == 'disjoint':\n",
    "        X_in, A_in, E_in, I_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "\n",
    "    \n",
    "    if conv == 'ecc':    \n",
    "        gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "        gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    if conv == 'gin':\n",
    "        assert mode == 'disjoint', 'cannot run GIN in batch mode'\n",
    "        gc1 = GINConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "        gc2 = GINConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    if mode == 'batch':\n",
    "        pool = GlobalAttentionPool(256)(gc2)\n",
    "    if mode == 'disjoint':\n",
    "        pool = GlobalAttentionPool(256)([gc2, I_in])\n",
    "    dense_list = [Dense(256, activation='relu')(pool) \n",
    "                  for i in range(num_tasks)]\n",
    "    output_list = [Dense(1)(dense_layer) for dense_layer in dense_list]\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    loss_fn = MeanSquaredError()\n",
    "    if mode == 'batch':\n",
    "        model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "        model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    if mode == 'disjoint':\n",
    "        model = Model(inputs=[X_in, A_in, E_in, I_in], outputs=output_list)\n",
    "    \n",
    "    return model, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_soft_sharing_model(*, A, X, E, num_tasks, share_param, \n",
    "                             learning_rate=1e-3, conv='ecc', mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")  \n",
    "    if conv not in ['ecc', 'gin']:\n",
    "        raise ValueError(f\"convolution layer {conv} not recognized; \"\n",
    "                         \"choose 'ecc' or 'gin'\")\n",
    "    if mode == 'batch':\n",
    "        X_in, A_in, E_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "    if mode == 'disjoint':\n",
    "        X_in, A_in, E_in, I_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "        \n",
    "    if conv == 'ecc':\n",
    "        conv_layer = EdgeConditionedConv\n",
    "    if conv == 'gin':\n",
    "        conv_layer = GINConv\n",
    "\n",
    "    gc1_list = [conv_layer(64, activation='relu')([X_in, A_in, E_in]) \n",
    "                for i in range(num_tasks)]\n",
    "    gc2_list = [conv_layer(128, activation='relu')([gc1, A_in, E_in]) \n",
    "                for gc1 in gc1_list]\n",
    "    if mode == 'batch':\n",
    "        pool_list = [GlobalAttentionPool(256)(gc2) for gc2 in gc2_list]\n",
    "    if mode == 'disjoint':\n",
    "        pool_list = [GlobalAttentionPool(256)([gc2, I_in]) for gc2 in gc2_list]\n",
    "    dense_list = [Dense(256, activation='relu')(pool) for pool in pool_list]\n",
    "    output_list = [Dense(1)(dense) for dense in dense_list]\n",
    "\n",
    "    def loss_fn(y_actual, y_pred):\n",
    "        avg_layer_diff = 0\n",
    "        for i, j in itertools.combinations(range(num_tasks), 2):\n",
    "            for gc in [gc1_list, gc2_list]:\n",
    "                diff = gc[i].trainable_weights - gc[j].trainable_weights\n",
    "                avg_layer_diff += mean(square(diff))\n",
    "        avg_layer_diff /= (num_tasks)*(num_tasks-1)/2  \n",
    "        return mean(square(y_actual - y_pred)) + share_param*avg_layer_diff\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    if mode == 'batch':\n",
    "        model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "        model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    if mode == 'disjoint':\n",
    "        model = Model(inputs=[X_in, A_in, E_in, I_in], outputs=output_list)\n",
    "    \n",
    "    return model, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_filename(tasks, conv='ecc', mode='batch', folder_path='demo_models'):\n",
    "    filename = \"\".join(sorted(tasks)) + '_' + conv + '_' + mode \n",
    "    return path.join(folder_path, f'{filename}.h5')\n",
    "\n",
    "def generate_task_scaler_filename(task, folder_path='demo_models'):\n",
    "    return path.join(folder_path, f'{task}_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tasks, task_to_scaler, mode='batch', conv='ecc'):\n",
    "    model.save_weights(generate_model_filename(tasks, conv=conv, mode=mode))\n",
    "    for task in tasks:\n",
    "        scaler_filename = generate_task_scaler_filename(task)\n",
    "        with open(scaler_filename, 'wb') as f:\n",
    "            scaler = task_to_scaler[task]\n",
    "            pickle.dump(obj=scaler, file=f)\n",
    "\n",
    "def load_hard_sharing_model(*, A, X, E, tasks, conv='ecc', \n",
    "                            mode='batch', task_to_scaler=dict()):\n",
    "    model, _ = build_hard_sharing_model(A=A, X=X, E=E, conv=conv, mode=mode,\n",
    "                                     num_tasks=len(tasks))\n",
    "    model.load_weights(generate_model_filename(tasks, conv=conv, mode=mode))\n",
    "    for task in tasks:\n",
    "        if task not in task_to_scaler:\n",
    "            with open(generate_task_scaler_filename(task), 'rb') as f:\n",
    "                task_to_scaler[task] = pickle.load(f)\n",
    "    return model, task_to_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask_disjoint(model, cluster, *, opt, loss_fn, batch_size, \n",
    "                             epochs, A_train, X_train, E_train, y_train, \n",
    "                             loss_logger=None):\n",
    "    F, S = get_shape_params(A=A_train, X=X_train, E=E_train, mode='disjoint')\n",
    "    @tf.function(\n",
    "        input_signature=(tf.TensorSpec((None, F), dtype=tf.float64),\n",
    "                         tf.SparseTensorSpec((None, None), dtype=tf.float64),\n",
    "                         tf.TensorSpec((None, S), dtype=tf.float64),\n",
    "                         tf.TensorSpec((None,), dtype=tf.int32),\n",
    "                         tf.TensorSpec((None, len(cluster)), dtype=tf.float64)),\n",
    "        experimental_relax_shapes=True)\n",
    "    def train_step(X_, A_, E_, I_, y_):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model([X_, A_, E_, I_], training=True)\n",
    "            loss = loss_fn(y_, predictions)\n",
    "            loss += sum(model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    current_batch = 0\n",
    "    model_loss = 0\n",
    "    batches_in_epoch = np.ceil(len(A_train) / batch_size)\n",
    "\n",
    "    print('Fitting model')\n",
    "    batches_train = batch_iterator([X_train, A_train, E_train, y_train[cluster].values],\n",
    "                                   batch_size=batch_size, epochs=epochs)\n",
    "    epoch_num = 1\n",
    "    for b in batches_train:\n",
    "        X_, A_, E_, I_ = numpy_to_disjoint(*b[:-1])\n",
    "        A_ = ops.sp_matrix_to_sp_tensor(A_)\n",
    "        y_ = b[-1]\n",
    "        outs = train_step(X_, A_, E_, I_, y_)\n",
    "\n",
    "        model_loss += outs.numpy()\n",
    "        current_batch += 1\n",
    "        if current_batch == batches_in_epoch:\n",
    "            print('Loss: {}'.format(model_loss / batches_in_epoch))\n",
    "            if loss_logger is not None:\n",
    "                loss_logger.losses[epoch_num] = model_loss / batches_in_epoch\n",
    "            model_loss = 0\n",
    "            current_batch = 0\n",
    "            epoch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multitask_disjoint(model, cluster, *, loss_fn, batch_size, A_test, X_test, E_test, y_test):\n",
    "    print('Testing model')\n",
    "    model_loss = 0\n",
    "    batches_in_epoch = np.ceil(len(A_test) / batch_size)\n",
    "    batches_test = batch_iterator([X_test, A_test, E_test, y_test[cluster].values], batch_size=batch_size)\n",
    "    for b in batches_test:\n",
    "        X_, A_, E_, I_ = numpy_to_disjoint(*b[:-1])\n",
    "        A_ = ops.sp_matrix_to_sp_tensor(A_)\n",
    "        y_ = b[3]\n",
    "\n",
    "        predictions = model([X_, A_, E_, I_], training=False)\n",
    "        model_loss += loss_fn(y_, predictions)\n",
    "    model_loss /= batches_in_epoch\n",
    "    print('Done. Test loss: {}'.format(model_loss))\n",
    "    return model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_property(prop, mol_id, clusters, *, X_all, A_all, E_all,\n",
    "                     mode='batch', conv='ecc', model=None, \n",
    "                     task_to_scaler=dict()):\n",
    "    \n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")  \n",
    "    if conv not in ['ecc', 'gin']:\n",
    "        raise ValueError(f\"convolution layer {conv} not recognized; \"\n",
    "                         \"choose 'ecc' or 'gin'\")\n",
    "    \n",
    "    cluster = [c for c in clusters if prop in c][0]\n",
    "    if model is None:\n",
    "        model, task_to_scaler = load_hard_sharing_model(\n",
    "            A=A_all, X=X_all, E=E_all, tasks=cluster, \n",
    "            mode=mode, conv=conv, task_to_scaler=task_to_scaler\n",
    "        )\n",
    "    i = mol_id - 1\n",
    "\n",
    "    # convert shape for batch mode\n",
    "    if mode == 'batch':\n",
    "        def wrap(a):\n",
    "            return a.reshape([1] + list(a.shape))\n",
    "        x = list(map(wrap, [X_all[i], A_all[i], E_all[i]]))\n",
    "        cluster_prediction = model.predict(x)       \n",
    "    \n",
    "    if mode == 'disjoint':\n",
    "        X_, A_, E_, I_ = numpy_to_disjoint([X_all[i]], [A_all[i]], [E_all[i]])\n",
    "        A_ = ops.sp_matrix_to_sp_tensor(A_)\n",
    "        cluster_prediction = model([X_, A_, E_, I_], training=False)\n",
    "    \n",
    "    prediction = cluster_prediction[cluster.index(prop)]\n",
    "    prediction = task_to_scaler[prop].inverse_transform(prediction)\n",
    "    return prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLoggerCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = dict()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.losses[epoch] = logs[\"loss\"]\n",
    "\n",
    "class ModelData:\n",
    "    def __init__(self, params=dict()):\n",
    "        self.timestamp = datetime.now()\n",
    "        self.loss_logger = LossLoggerCallback()\n",
    "        \n",
    "        \"\"\"\n",
    "        Possible params keys:\n",
    "        mode: 'batch' or 'disjoint'\n",
    "        conv: 'ecc' or 'gin'\n",
    "        single_task: true or false\n",
    "        cluster: only if single_task is false\n",
    "        hard_sharing: true or false, only if single_task is false \n",
    "        soft_weight: only if hard_sharing is false and single_task is false\n",
    "        batch_size\n",
    "        epochs\n",
    "        num_sampled\n",
    "        learning_rate\n",
    "        model_summary\n",
    "        loss_fn: string of loss function name\n",
    "        optimizer: string of optimizer name\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        \n",
    "        # not using actual/pred dict in case values collide\n",
    "        self.actual = list()\n",
    "        self.pred = list()\n",
    "        \n",
    "    def get_losses(self):\n",
    "        return self.loss_logger.losses\n",
    "    \n",
    "    def add_test(self, actual, pred):\n",
    "        self.actual.append(actual)\n",
    "        self.pred.append(pred)\n",
    "    \n",
    "    def _make_picklable(self):\n",
    "        return {'params': params, \n",
    "                'actual': actual, \n",
    "                'pred': pred, \n",
    "                'losses': self.loss_logger.losses}\n",
    "    \n",
    "    def serialize(self, dirname='model_data', filename=''):\n",
    "        if filename == '':\n",
    "            dt_string = self.timestamp.strftime('%d-%m-%Y_%H-%M-%S')\n",
    "            filename = path.join(dirname, dt_string + '.pkl')\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(self._make_picklable(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    mode = 'disjoint'\n",
    "    conv = 'gin'\n",
    "    batch_size = 32\n",
    "    epochs = 40\n",
    "    num_sampled = 100000\n",
    "    learning_rate = 1e-3\n",
    "    amount = None\n",
    "    A_all, X_all, E_all, y_all = load_data(amount=amount, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals(): \n",
    "    A, X, E, y = sample_from_data(num_sampled, A_all, X_all, E_all, \n",
    "                                  y_all, mode=mode)\n",
    "    task_to_scaler = standardize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [['A', 'lumo', 'homo'], \n",
    "            ['B', 'r2', 'cv'], \n",
    "            ['alpha', 'zpve'], \n",
    "            ['C', 'u0', 'u298', 'mu'], \n",
    "            ['g298', 'h298']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():     \n",
    "    A_train, A_test, \\\n",
    "        X_train, X_test, \\\n",
    "        E_train, E_test, \\\n",
    "        y_train, y_test = train_test_split(A, X, E, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    print('begin training models')\n",
    "   \n",
    "    tasks = [[task] for cluster in clusters for task in cluster]\n",
    "    tasks_and_clusters = itertools.chain(tasks, clusters)\n",
    "    for cluster, conv in itertools.product(tasks_and_clusters,\n",
    "                                           ['ecc', 'gin']):\n",
    "        print(f'training {cluster} with {mode} mode on {conv} conv')\n",
    "        \n",
    "        model, loss_fn = build_hard_sharing_model(A=A_train, \n",
    "                                                  X=X_train, \n",
    "                                                  E=E_train, \n",
    "                                                  num_tasks=len(cluster),\n",
    "                                                  mode=mode,\n",
    "                                                  conv=conv)\n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "        \n",
    "        stream = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
    "        summary = stream.getvalue()\n",
    "        \n",
    "        params = {'mode': mode, \n",
    "                  'conv': conv,\n",
    "                  'batch_size': batch_size,\n",
    "                  'epochs': epochs,\n",
    "                  'num_sampled': num_sampled,\n",
    "                  'learning_rate': learning_rate,\n",
    "                  'cluster': cluster,\n",
    "                  'hard_sharing': True,\n",
    "                  'model_summary': summary,\n",
    "                  'loss_fn': type(loss_fn).__name__,\n",
    "                  'optimizer': type(optimizer).__name__}\n",
    "        model_data = ModelData(params=params)\n",
    "                \n",
    "        if mode == 'batch':\n",
    "            # training\n",
    "            y_train_cluster = np.hsplit(y_train[cluster].values, len(cluster))\n",
    "            model.compile(optimizer=optimizer, \n",
    "                          loss=loss_fn)\n",
    "            model.fit(x=[X_train, A_train, E_train], \n",
    "                      y=y_train_cluster,\n",
    "                      batch_size=batch_size,\n",
    "                      validation_split=0.1,\n",
    "                      epochs=epochs,\n",
    "                      callbacks=[model_data.loss_logger])\n",
    "            \n",
    "            # testing\n",
    "            y_test_cluster = np.hsplit(y_test[cluster].values, len(cluster))\n",
    "            model_loss = model.evaluate(x=[X_test, A_test, E_test],\n",
    "                                        y=y_test_cluster)\n",
    "            print(f\"Test loss on {cluster}: {model_loss}\")\n",
    "            cluster_pred = model.predict([X_test, A_test, E_test])\n",
    "\n",
    "        if mode == 'disjoint':\n",
    "            # training\n",
    "            train_multitask_disjoint(model,\n",
    "                                     cluster,\n",
    "                                     opt=Adam(lr=1e-3),\n",
    "                                     loss_fn=loss_fn,\n",
    "                                     batch_size=batch_size,\n",
    "                                     epochs=epochs,\n",
    "                                     A_train=A_train,\n",
    "                                     X_train=X_train,\n",
    "                                     E_train=E_train,\n",
    "                                     y_train=y_train, \n",
    "                                     loss_logger=model_data.loss_logger)\n",
    "            # testing\n",
    "            model_loss = test_multitask_disjoint(model,\n",
    "                                                cluster,\n",
    "                                                loss_fn=loss_fn,\n",
    "                                                batch_size=batch_size,\n",
    "                                                A_test=A_test,\n",
    "                                                X_test=X_test,\n",
    "                                                E_test=E_test,\n",
    "                                                y_test=y_test)\n",
    "            X_, A_, E_, I_ = numpy_to_disjoint(X_test, A_test, E_test)\n",
    "            A_ = ops.sp_matrix_to_sp_tensor(A_)\n",
    "            cluster_pred = model([X_, A_, E_, I_], training=False)\n",
    "            \n",
    "        if len(cluster) == 1:\n",
    "            cluster_pred = [cluster_pred]\n",
    "\n",
    "        for prop, batch_pred in zip(cluster, cluster_pred):\n",
    "            batch_pred = task_to_scaler[prop].inverse_transform(batch_pred)\n",
    "            errors = list()\n",
    "            for index, pred in zip(y_test.index.values, batch_pred):\n",
    "                actual = y_all.loc[index, prop]\n",
    "                model_data.add_test(actual, pred[0])\n",
    "        \n",
    "        save_model(model, cluster, task_to_scaler, mode=mode, conv=conv)\n",
    "        model_data.serialize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
