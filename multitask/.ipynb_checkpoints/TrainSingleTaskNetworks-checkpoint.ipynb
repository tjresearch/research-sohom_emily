{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.backend import mean, square\n",
    "\n",
    "from spektral.datasets import qm9\n",
    "from spektral.layers import EdgeConditionedConv, GlobalAttentionPool\n",
    "from spektral.utils import label_to_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QM9 dataset.\n",
      "Reading SDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2081.63it/s]\n"
     ]
    }
   ],
   "source": [
    "A, X, E, y = qm9.load_data(return_type='numpy',\n",
    "                          nf_keys='atomic_num',\n",
    "                          ef_keys='type',\n",
    "                          self_loops=True,\n",
    "                          amount=10000)\n",
    "uniq_X = np.unique(X)\n",
    "X = label_to_one_hot(X, uniq_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = list(y.columns)[1:]\n",
    "y_list = []\n",
    "for task in tasks:\n",
    "    y_list.append(y[[task]].values)\n",
    "for i in range(len(y_list)):\n",
    "    y_list[i] = StandardScaler().fit_transform(y_list[i]).reshape(-1, y_list[0].shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[-2]\n",
    "F = X.shape[-1] \n",
    "S = E.shape[-1]\n",
    "n_out = y_list[0].shape[-1]\n",
    "learning_rate = 1e-3\n",
    "epochs = 25\n",
    "batch_size = 64\n",
    "es_patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train, A_test, \\\n",
    "X_train, X_test, \\\n",
    "E_train, E_test, \\\n",
    "*y_train_test_list = train_test_split(A, X, E, *y_list, test_size = 0.1)\n",
    "\n",
    "y_train_list = y_train_test_list[::2]\n",
    "y_test_list = y_train_test_list[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sohompaul/psi4conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_in = Input(shape=(N, F))\n",
    "A_in = Input(shape=(N, N))\n",
    "E_in = Input(shape=(N, N, S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_task_model(X_in, A_in, E_in):\n",
    "    gc1 = EdgeConditionedConv(64, activation='relu')([X_in, A_in, E_in])\n",
    "    gc2 = EdgeConditionedConv(128, activation='relu')([gc1, A_in, E_in])\n",
    "    pool = GlobalAttentionPool(256)(gc2)\n",
    "    dense = Dense(256, activation='relu')(pool)\n",
    "    output = Dense(n_out)(dense)\n",
    "    return Model(inputs=[X_in, A_in, E_in], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning A\n",
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/25\n",
      "8100/8100 [==============================] - 274s 34ms/step - loss: 1.2356 - val_loss: 2.9712e-04\n",
      "Epoch 2/25\n",
      "8100/8100 [==============================] - 272s 34ms/step - loss: 1.2370 - val_loss: 2.1448e-05\n",
      "Epoch 3/25\n",
      "8100/8100 [==============================] - 272s 34ms/step - loss: 1.2372 - val_loss: 0.0016\n",
      "Epoch 4/25\n",
      "2560/8100 [========>.....................] - ETA: 3:04 - loss: 1.8617e-04"
     ]
    }
   ],
   "source": [
    "for i in range(len(tasks)):\n",
    "    print('learning', tasks[i])\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model = create_single_task_model(X_in, A_in, E_in)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    es_callback = EarlyStopping(monitor='val_loss', patience=es_patience)\n",
    "    model.fit([X_train, A_train, E_train],\n",
    "             y_train_list[i],\n",
    "             batch_size=batch_size,\n",
    "             validation_split=0.1,\n",
    "             epochs=epochs,\n",
    "             callbacks=[es_callback])\n",
    "    model.save(path.join('single_task_trained_models', tasks[i], '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transfer_coefficient_from_file(task_learned, task_transferred, y_transferred_test):\n",
    "    model_learned = load_model(path.join('single_task_trained_models', task_learned, '.h5'))\n",
    "    model_transferred = load_model(path.join('single_task_trained_models', task_transferred, '.h5'))\n",
    "    layers_learned = model_learned.get_weights()\n",
    "    layers_transferred = model_transferred.get_weights()\n",
    "    layer_transferred = learned_layers[:10] + transferred_layers[10:]\n",
    "    model_transferred.set_weights(layers_transferred)\n",
    "    eval_results = model_transferred.evaluate([X_test, A_test, E_test],\n",
    "                                              y_transferred_test,\n",
    "                                              batch_size=batch_size)\n",
    "    return eval_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
