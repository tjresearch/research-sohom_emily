{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "from os import path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, LogCosh\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from tensorflow.keras.backend import mean, square\n",
    "\n",
    "from spektral.datasets import qm9\n",
    "from spektral.layers import EdgeConditionedConv, GINConv, GatedGraphConv\n",
    "from spektral.layers import ops, GlobalSumPool, GlobalAttentionPool\n",
    "from spektral.utils import batch_iterator, numpy_to_disjoint\n",
    "from spektral.utils import label_to_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(amount=None, mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")\n",
    "    \n",
    "    A_all, X_all, E_all, y_all = qm9.load_data(return_type='numpy',\n",
    "                               nf_keys='atomic_num',\n",
    "                               ef_keys='type',\n",
    "                               self_loops=True,\n",
    "                               amount=amount) # None for entire dataset\n",
    "    # Preprocessing\n",
    "    if mode == 'batch':\n",
    "        X_uniq = np.unique(X_all)\n",
    "        X_uniq = X_uniq[X_uniq != 0]\n",
    "        E_uniq = np.unique(E_all)\n",
    "        E_uniq = E_uniq[E_uniq != 0]\n",
    "\n",
    "        X_all = label_to_one_hot(X_all, X_uniq)\n",
    "        E_all = label_to_one_hot(E_all, E_uniq)\n",
    "    elif mode == 'disjoint':\n",
    "        X_uniq = np.unique([v for x in X_all for v in np.unique(x)])\n",
    "        E_uniq = np.unique([v for e in E_all for v in np.unique(e)])\n",
    "        X_uniq = X_uniq[X_uniq != 0]\n",
    "        E_uniq = E_uniq[E_uniq != 0]\n",
    "\n",
    "        X_all = [label_to_one_hot(x, labels=X_uniq) for x in X_all]\n",
    "        E_all = [label_to_one_hot(e, labels=E_uniq) for e in E_all]\n",
    "    \n",
    "    return A_all, X_all, E_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_data(sample_size, A_all, X_all, E_all, y_all, mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")\n",
    "    if mode == 'batch':\n",
    "        indices = np.random.choice(X_all.shape[0], sample_size, replace=False)\n",
    "        A = A_all[indices, :, :]\n",
    "        X = X_all[indices, :, :]\n",
    "        E = E_all[indices, :, :, :]\n",
    "        y = y_all.iloc[indices, :].copy()\n",
    "        \n",
    "    if mode == 'disjoint':\n",
    "        indices = np.random.choice(len(X_all), sample_size, replace=False)\n",
    "        A = [A_all[i] for i in indices]\n",
    "        X = [X_all[i] for i in indices]\n",
    "        E = [E_all[i] for i in indices]\n",
    "        y = y_all.iloc[indices, :].copy()\n",
    "    \n",
    "    return A, X, E, y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(y, scaler='power_transformer'):\n",
    "    if scaler not in ['power_transformer', 'standard_scaler']:\n",
    "        raise ValueError(f'scaler {scaler} not recognized'\n",
    "                        'choose \"power_transformer\" or \"standard_scaler\"')\n",
    "    task_to_scaler = dict()\n",
    "    for task in list(y.columns)[1:]:\n",
    "        if scaler == 'standard_scaler':\n",
    "            scaler = StandardScaler()\n",
    "        else:\n",
    "            scaler = PowerTransformer()\n",
    "        y.loc[:, task] = scaler.fit_transform(y[[task]])\n",
    "        task_to_scaler[task] = scaler\n",
    "    return task_to_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_params(*, A, X, E, mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")\n",
    "    F = X[0].shape[-1]  # Dimension of node features\n",
    "    S = E[0].shape[-1]  # Dimension of edge features\n",
    "    if mode == 'batch':\n",
    "        N = X.shape[-2]       # Number of nodes in the graphs\n",
    "        return N, F, S\n",
    "    if mode == 'disjoint':\n",
    "        return F, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tensors(*, A, X, E, mode='batch'):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")\n",
    "    if mode == 'batch':\n",
    "        N, F, S = get_shape_params(A=A, X=X, E=E, mode=mode)\n",
    "        X_in = Input(shape=(N, F), name='X_in')\n",
    "        A_in = Input(shape=(N, N), name='A_in')\n",
    "        E_in = Input(shape=(N, N, S), name='E_in')\n",
    "\n",
    "        return X_in, A_in, E_in\n",
    "    \n",
    "    if mode == 'disjoint':\n",
    "        F, S = get_shape_params(A=A, X=X, E=E, mode=mode)\n",
    "        X_in = Input(shape=(F,), name='X_in')\n",
    "        A_in = Input(shape=(None,), sparse=True, name='A_in')\n",
    "        E_in = Input(shape=(S,), name='E_in')\n",
    "        I_in = Input(shape=(), name='segment_ids_in', dtype=tf.int32)\n",
    "        \n",
    "        return X_in, A_in, E_in, I_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_task_model(*, A, X, E, learning_rate=1e-3, \n",
    "                            conv='ecc', mode='batch',\n",
    "                            layer_sizes=[128, 256, 512, 512]):  \n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")  \n",
    "    if conv not in ['ecc', 'gin']:\n",
    "        raise ValueError(f\"convolution layer {conv} not recognized; \"\n",
    "                         \"choose 'ecc' or 'gin'\")\n",
    "    \n",
    "    if mode == 'batch':\n",
    "        X_in, A_in, E_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "    if mode == 'disjoint':\n",
    "        X_in, A_in, E_in, I_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "        \n",
    "    gc1_size, gc2_size, pool_size, dense_size = layer_sizes\n",
    "\n",
    "    if conv == 'ecc':    \n",
    "        gc1 = EdgeConditionedConv(gc1_size, activation='relu')([X_in, A_in, E_in])\n",
    "        gc2 = EdgeConditionedConv(gc2_size, activation='relu')([gc1, A_in, E_in])\n",
    "    if conv == 'gin':\n",
    "        assert mode == 'disjoint', 'cannot run GIN in batch mode'\n",
    "        gc1 = GINConv(gc1_size, activation='relu')([X_in, A_in, E_in])\n",
    "        gc2 = GINConv(gc2_size, activation='relu')([gc1, A_in, E_in])\n",
    "    if mode == 'batch':\n",
    "        pool = GlobalAttentionPool(pool_size)(gc2)\n",
    "    if mode == 'disjoint':\n",
    "        pool = GlobalAttentionPool(pool_size)([gc2, I_in])\n",
    "    dense = Dense(dense_size, activation='relu')(pool)\n",
    "    output = Dense(1)(dense)\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    loss_fn = MeanSquaredError()\n",
    "    if mode == 'batch':\n",
    "        model = Model(inputs=[X_in, A_in, E_in], outputs=output)\n",
    "        model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    if mode == 'disjoint':\n",
    "        model = Model(inputs=[X_in, A_in, E_in, I_in], outputs=output)\n",
    "    \n",
    "    return model, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hard_sharing_model(*, A, X, E, num_tasks, \n",
    "                             learning_rate=1e-3, conv='ecc', mode='batch', \n",
    "                             layer_sizes=[128, 256, 512, 512]):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")  \n",
    "    if conv not in ['ecc', 'gin']:\n",
    "        raise ValueError(f\"convolution layer {conv} not recognized; \"\n",
    "                         \"choose 'ecc' or 'gin'\")\n",
    "    if mode == 'batch':\n",
    "        X_in, A_in, E_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "    if mode == 'disjoint':\n",
    "        X_in, A_in, E_in, I_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "\n",
    "    gc1_size, gc2_size, pool_size, dense_size = layer_sizes\n",
    "    \n",
    "    if conv == 'ecc':    \n",
    "        gc1 = EdgeConditionedConv(gc1_size, activation='relu')([X_in, A_in, E_in])\n",
    "        gc2 = EdgeConditionedConv(gc2_size, activation='relu')([gc1, A_in, E_in])\n",
    "    if conv == 'gin':\n",
    "        assert mode == 'disjoint', 'cannot run GIN in batch mode'\n",
    "        gc1 = GINConv(gc1_size, activation='relu')([X_in, A_in, E_in])\n",
    "        gc2 = GINConv(gc2_size, activation='relu')([gc1, A_in, E_in])\n",
    "    if mode == 'batch':\n",
    "        pool = GlobalAttentionPool(pool_size)(gc2)\n",
    "    if mode == 'disjoint':\n",
    "        pool = GlobalAttentionPool(pool_size)([gc2, I_in])\n",
    "    dense_list = [Dense(dense_size, activation='relu')(pool) \n",
    "                  for i in range(num_tasks)]\n",
    "    output_list = [Dense(1)(dense_layer) for dense_layer in dense_list]\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    loss_fn = MeanSquaredError()\n",
    "    if mode == 'batch':\n",
    "        model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "        model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    if mode == 'disjoint':\n",
    "        model = Model(inputs=[X_in, A_in, E_in, I_in], outputs=output_list)\n",
    "    \n",
    "    return model, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_soft_sharing_model(*, A, X, E, num_tasks, share_param, \n",
    "                             learning_rate=1e-3, conv='ecc', mode='batch', \n",
    "                             layer_sizes=[128, 256, 512, 512]):\n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")  \n",
    "    if conv not in ['ecc', 'gin']:\n",
    "        raise ValueError(f\"convolution layer {conv} not recognized; \"\n",
    "                         \"choose 'ecc' or 'gin'\")\n",
    "    if mode == 'batch':\n",
    "        X_in, A_in, E_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "    if mode == 'disjoint':\n",
    "        X_in, A_in, E_in, I_in = get_input_tensors(A=A, X=X, E=E, mode=mode)\n",
    "        \n",
    "    if conv == 'ecc':\n",
    "        conv_layer = EdgeConditionedConv\n",
    "    if conv == 'gin':\n",
    "        conv_layer = GINConv\n",
    "        \n",
    "    gc1_size, gc2_size, pool_size, dense_size = layer_sizes\n",
    "\n",
    "    gc1_list = [conv_layer(gc1_size, activation='relu')([X_in, A_in, E_in]) \n",
    "                for i in range(num_tasks)]\n",
    "    gc2_list = [conv_layer(gc2_size, activation='relu')([gc1, A_in, E_in]) \n",
    "                for gc1 in gc1_list]\n",
    "    if mode == 'batch':\n",
    "        pool_list = [GlobalAttentionPool(pool_size)(gc2) for gc2 in gc2_list]\n",
    "    if mode == 'disjoint':\n",
    "        pool_list = [GlobalAttentionPool(pool_size)([gc2, I_in]) for gc2 in gc2_list]\n",
    "    dense_list = [Dense(dense_size, activation='relu')(pool) for pool in pool_list]\n",
    "    output_list = [Dense(1)(dense) for dense in dense_list]\n",
    "\n",
    "    def loss_fn(y_actual, y_pred):\n",
    "        avg_layer_diff = 0\n",
    "        for i, j in itertools.combinations(range(num_tasks), 2):\n",
    "            for gc in [gc1_list, gc2_list]:\n",
    "                diff = gc[i].trainable_weights - gc[j].trainable_weights\n",
    "                avg_layer_diff += mean(square(diff))\n",
    "        avg_layer_diff /= (num_tasks)*(num_tasks-1)/2  \n",
    "        return mean(square(y_actual - y_pred)) + share_param*avg_layer_diff\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    if mode == 'batch':\n",
    "        model = Model(inputs=[X_in, A_in, E_in], outputs=output_list)\n",
    "        model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    if mode == 'disjoint':\n",
    "        model = Model(inputs=[X_in, A_in, E_in, I_in], outputs=output_list)\n",
    "    \n",
    "    return model, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_filename(tasks, conv='ecc', mode='batch', folder_path='demo_models'):\n",
    "    filename = \"\".join(sorted(tasks)) + '_' + conv + '_' + mode \n",
    "    return path.join(folder_path, f'{filename}.h5')\n",
    "\n",
    "def generate_task_scaler_filename(task, folder_path='demo_models'):\n",
    "    return path.join(folder_path, f'{task}_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tasks, task_to_scaler, mode='batch', conv='ecc'):\n",
    "    model.save_weights(generate_model_filename(tasks, conv=conv, mode=mode))\n",
    "    for task in tasks:\n",
    "        scaler_filename = generate_task_scaler_filename(task)\n",
    "        with open(scaler_filename, 'wb') as f:\n",
    "            scaler = task_to_scaler[task]\n",
    "            pickle.dump(obj=scaler, file=f)\n",
    "\n",
    "def load_hard_sharing_model(*, A, X, E, tasks, conv='ecc', \n",
    "                            mode='batch', task_to_scaler=dict()):\n",
    "    model, _ = build_hard_sharing_model(A=A, X=X, E=E, conv=conv, mode=mode,\n",
    "                                     num_tasks=len(tasks))\n",
    "    model.load_weights(generate_model_filename(tasks, conv=conv, mode=mode))\n",
    "    for task in tasks:\n",
    "        if task not in task_to_scaler:\n",
    "            with open(generate_task_scaler_filename(task), 'rb') as f:\n",
    "                task_to_scaler[task] = pickle.load(f)\n",
    "    return model, task_to_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask_disjoint(model, cluster, *, opt, loss_fn, batch_size, \n",
    "                             epochs, A_train, X_train, E_train, y_train, \n",
    "                             epoch_num=1, loss_logger=None):\n",
    "    F, S = get_shape_params(A=A_train, X=X_train, E=E_train, mode='disjoint')\n",
    "    @tf.function(\n",
    "        input_signature=(tf.TensorSpec((None, F), dtype=tf.float64),\n",
    "                         tf.SparseTensorSpec((None, None), dtype=tf.float64),\n",
    "                         tf.TensorSpec((None, S), dtype=tf.float64),\n",
    "                         tf.TensorSpec((None,), dtype=tf.int32),\n",
    "                         tf.TensorSpec((None, len(cluster)), dtype=tf.float64)),\n",
    "        experimental_relax_shapes=True)\n",
    "    def train_step(X_, A_, E_, I_, y_):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model([X_, A_, E_, I_], training=True)\n",
    "            loss = loss_fn(y_, predictions)\n",
    "            loss += sum(model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    current_batch = 0\n",
    "    model_loss = 0\n",
    "    batches_in_epoch = np.ceil(len(A_train) / batch_size)\n",
    "\n",
    "    print('Fitting model')\n",
    "    batches_train = batch_iterator([X_train, A_train, E_train, y_train[cluster].values],\n",
    "                                   batch_size=batch_size, epochs=epochs)\n",
    "    for b in batches_train:\n",
    "        X_, A_, E_, I_ = numpy_to_disjoint(*b[:-1])\n",
    "        A_ = ops.sp_matrix_to_sp_tensor(A_)\n",
    "        y_ = b[-1]\n",
    "        outs = train_step(X_, A_, E_, I_, y_)\n",
    "\n",
    "        model_loss += outs.numpy()\n",
    "        current_batch += 1\n",
    "        if current_batch == batches_in_epoch:\n",
    "            print('Loss: {}'.format(model_loss / batches_in_epoch))\n",
    "            if loss_logger is not None:\n",
    "                loss_logger.losses[epoch_num] = model_loss / batches_in_epoch\n",
    "            model_loss = 0\n",
    "            current_batch = 0\n",
    "            epoch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multitask_disjoint(model, cluster, *, loss_fn, batch_size, \n",
    "                            A_test, X_test, E_test, y_test,\n",
    "                            task_to_scaler=None, model_data=None):\n",
    "    print('Testing model')\n",
    "    model_loss = 0\n",
    "    batches_in_epoch = np.ceil(len(A_test) / batch_size)\n",
    "    batches_test = batch_iterator([X_test, A_test, E_test, y_test[cluster].values],\n",
    "                                  batch_size=batch_size)\n",
    "    for b in batches_test:\n",
    "        X_, A_, E_, I_ = numpy_to_disjoint(*b[:-1])\n",
    "        A_ = ops.sp_matrix_to_sp_tensor(A_)\n",
    "        y_ = b[3]\n",
    "\n",
    "        predictions = model([X_, A_, E_, I_], training=False)\n",
    "        model_loss += loss_fn(y_, predictions)\n",
    "        \n",
    "        if all([task_to_scaler, model_data]):\n",
    "            if len(cluster) == 1:\n",
    "                predictions = [predictions]\n",
    "            y_ = np.hsplit(y_, len(cluster))\n",
    "            for task, batch_pred, batch_actual in zip(cluster, predictions, y_):\n",
    "                scaler = task_to_scaler[task]\n",
    "                batch_pred = scaler.inverse_transform(batch_pred)\n",
    "                batch_actual = scaler.inverse_transform(batch_actual)\n",
    "                for pred, actual in zip(batch_pred, batch_actual):\n",
    "                    model_data.add_test(task, pred=pred[0], actual=actual[0])\n",
    "\n",
    "    model_loss /= batches_in_epoch\n",
    "    print('Done. Test loss: {}'.format(model_loss))\n",
    "    \n",
    "    if model_data:\n",
    "        model_data.test_loss = model_loss\n",
    "\n",
    "    return model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_property(prop, mol_id, clusters, *, X_all, A_all, E_all,\n",
    "                     mode='batch', conv='ecc', model=None, \n",
    "                     task_to_scaler=dict()):\n",
    "    \n",
    "    if mode not in ['batch', 'disjoint']:\n",
    "        raise ValueError(f\"mode {mode} not recognized; \"\n",
    "                         \"choose 'batch' or 'disjoint'\")  \n",
    "    if conv not in ['ecc', 'gin']:\n",
    "        raise ValueError(f\"convolution layer {conv} not recognized; \"\n",
    "                         \"choose 'ecc' or 'gin'\")\n",
    "    \n",
    "    cluster = [c for c in clusters if prop in c][0]\n",
    "    if model is None:\n",
    "        model, task_to_scaler = load_hard_sharing_model(A=A_all, \n",
    "                                                        X=X_all, \n",
    "                                                        E=E_all, \n",
    "                                                        tasks=cluster,\n",
    "                                                        mode=mode, \n",
    "                                                        conv=conv, \n",
    "                                                        task_to_scaler=task_to_scaler)\n",
    "    i = mol_id - 1\n",
    "\n",
    "    # convert shape for batch mode\n",
    "    if mode == 'batch':\n",
    "        def wrap(a):\n",
    "            return a.reshape([1] + list(a.shape))\n",
    "        x = list(map(wrap, [X_all[i], A_all[i], E_all[i]]))\n",
    "        cluster_prediction = model.predict(x)       \n",
    "    \n",
    "    if mode == 'disjoint':\n",
    "        X_, A_, E_, I_ = numpy_to_disjoint([X_all[i]], [A_all[i]], [E_all[i]])\n",
    "        A_ = ops.sp_matrix_to_sp_tensor(A_)\n",
    "        cluster_prediction = model([X_, A_, E_, I_], training=False)\n",
    "    \n",
    "    prediction = cluster_prediction[cluster.index(prop)]\n",
    "    prediction = task_to_scaler[prop].inverse_transform(prediction)\n",
    "    return prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLoggerCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = dict()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.losses[epoch] = logs[\"loss\"]\n",
    "\n",
    "class ModelData:\n",
    "    def __init__(self, params=dict()):\n",
    "        self.timestamp = datetime.now()\n",
    "        self.loss_logger = LossLoggerCallback()\n",
    "        \n",
    "        \"\"\"\n",
    "        Possible params keys:\n",
    "        mode: 'batch' or 'disjoint'\n",
    "        conv: 'ecc' or 'gin'\n",
    "        single_task: true or false\n",
    "        cluster: only if single_task is false\n",
    "        hard_sharing: true or false, only if single_task is false \n",
    "        soft_weight: only if hard_sharing is false and single_task is false\n",
    "        batch_size\n",
    "        epochs\n",
    "        num_sampled\n",
    "        learning_rate\n",
    "        model_summary\n",
    "        loss_fn: string of loss function name\n",
    "        optimizer: string of optimizer name\n",
    "        layer_sizes: List[Int] [gc1, gc2, pool, dense]\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        \n",
    "        self.actual = dict() # task -> List[Float]\n",
    "        self.pred = dict() # task -> List[Float]\n",
    "\n",
    "        self.test_loss = None\n",
    "        \n",
    "    def get_losses(self):\n",
    "        return self.loss_logger.losses\n",
    "    \n",
    "    def add_test(self, task, *, actual, pred):\n",
    "        if task not in self.actual:\n",
    "            self.actual[task] = list()\n",
    "            self.pred[task] = list()\n",
    "        \n",
    "        self.actual[task].append(actual)\n",
    "        self.pred[task].append(pred)\n",
    "    \n",
    "    def _make_picklable(self):\n",
    "        return {'params': self.params, \n",
    "                'actual': self.actual, \n",
    "                'pred': self.pred, \n",
    "                'losses': self.loss_logger.losses,\n",
    "                'test_loss': self.test_loss}\n",
    "    \n",
    "    def serialize(self, dirname='model_data', filename=''):\n",
    "        if filename == '':\n",
    "            dt_string = self.timestamp.strftime('%d-%m-%Y_%H-%M-%S')\n",
    "            filename = path.join(dirname, dt_string + '.pkl')\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(self._make_picklable(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__' and '__file__' not in globals():\n",
    "#     mode = 'disjoint'\n",
    "#     conv = 'gin'\n",
    "#     batch_size = 32\n",
    "#     epochs = 40\n",
    "#     num_sampled = 30000\n",
    "#     learning_rate = 1e-2\n",
    "#     learning_rate_scheduler = 2\n",
    "#     epochs_per_schedule = 5\n",
    "#     amount = None\n",
    "#     scaler = 'power_transformer'\n",
    "#     loss_fn = MeanAbsoluteError()\n",
    "#     A_all, X_all, E_all, y_all = load_data(amount=amount, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__' and '__file__' not in globals(): \n",
    "#     A, X, E, y = sample_from_data(num_sampled, \n",
    "#                                   A_all, \n",
    "#                                   X_all, \n",
    "#                                   E_all, \n",
    "#                                   y_all, \n",
    "#                                   mode=mode)\n",
    "#     task_to_scaler = standardize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [['A', 'lumo', 'homo'], \n",
    "            ['B', 'r2', 'cv'], \n",
    "            ['alpha', 'zpve'], \n",
    "            ['C', 'u0', 'u298', 'mu'], \n",
    "            ['g298', 'h298']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__' and '__file__' not in globals():     \n",
    "#     A_train, A_test, \\\n",
    "#         X_train, X_test, \\\n",
    "#         E_train, E_test, \\\n",
    "#         y_train, y_test = train_test_split(A, X, E, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__' and '__file__' not in globals():\n",
    "#     print('begin training models')\n",
    "   \n",
    "#     tasks = [[task] for cluster in clusters for task in cluster]\n",
    "#     clusters_alt = [['B', 'g298'], ['alpha', 'zpve'], ['C', 'u0', 'u298'], ['r2', 'cv'], ['h298', 'mu']]\n",
    "#     tasks_and_clusters = itertools.chain(tasks, clusters, clusters_alt)\n",
    "#     for cluster, conv in itertools.product(tasks_and_clusters,\n",
    "#                                            ['ecc', 'gin']):\n",
    "#         print(f'training {cluster} with {mode} mode on {conv} conv')\n",
    "        \n",
    "#         model, _ = build_hard_sharing_model(A=A_train, \n",
    "#                                                   X=X_train, \n",
    "#                                                   E=E_train, \n",
    "#                                                   num_tasks=len(cluster),\n",
    "#                                                   mode=mode,\n",
    "#                                                   conv=conv)\n",
    "#         optimizer = Adam(lr=learning_rate)\n",
    "#         loss_fn = MeanSquaredError()\n",
    "        \n",
    "#         stream = io.StringIO()\n",
    "#         model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
    "#         summary = stream.getvalue()\n",
    "        \n",
    "#         params = {'mode': mode, \n",
    "#                   'conv': conv,\n",
    "#                   'batch_size': batch_size,\n",
    "#                   'epochs': epochs,\n",
    "#                   'num_sampled': num_sampled,\n",
    "#                   'learning_rate': learning_rate,\n",
    "#                   'cluster': cluster,\n",
    "#                   'hard_sharing': True,\n",
    "#                   'model_summary': summary,\n",
    "#                   'loss_fn': type(loss_fn).__name__,\n",
    "#                   'optimizer': type(optimizer).__name__, \n",
    "#                   'learning_rate_scheduler': learning_rate_scheduler, \n",
    "#                   'epochs_per_schedule': epochs_per_schedule}\n",
    "#         model_data = ModelData(params=params)\n",
    "                \n",
    "#         if mode == 'batch':\n",
    "#             # training\n",
    "#             y_train_cluster = np.hsplit(y_train[cluster].values, len(cluster))\n",
    "#             model.compile(optimizer=optimizer, \n",
    "#                           loss=loss_fn)\n",
    "#             model.fit(x=[X_train, A_train, E_train], \n",
    "#                       y=y_train_cluster,\n",
    "#                       batch_size=batch_size,\n",
    "#                       validation_split=0.1,\n",
    "#                       epochs=epochs,\n",
    "#                       callbacks=[model_data.loss_logger])\n",
    "            \n",
    "#             # testing\n",
    "#             y_test_cluster = np.hsplit(y_test[cluster].values, len(cluster))\n",
    "#             model_loss = model.evaluate(x=[X_test, A_test, E_test],\n",
    "#                                         y=y_test_cluster)\n",
    "#             print(f\"Test loss on {cluster}: {model_loss}\")\n",
    "#             cluster_pred = model.predict([X_test, A_test, E_test])\n",
    "\n",
    "#         if mode == 'disjoint':\n",
    "#             # training with learning rate decay\n",
    "#             for i in range(epochs//epochs_per_schedule):\n",
    "#                 if i > 0:\n",
    "#                     learning_rate /= learning_rate_scheduler\n",
    "#                 optimizer = Adam(lr=learning_rate)\n",
    "#                 train_multitask_disjoint(model,\n",
    "#                                          cluster,\n",
    "#                                          opt=optimizer,\n",
    "#                                          loss_fn=loss_fn,\n",
    "#                                          batch_size=batch_size,\n",
    "#                                          epochs=epochs_per_schedule,\n",
    "#                                          epoch_num=i*epochs_per_schedule+1,\n",
    "#                                          A_train=A_train,\n",
    "#                                          X_train=X_train,\n",
    "#                                          E_train=E_train,\n",
    "#                                          y_train=y_train, \n",
    "#                                          loss_logger=model_data.loss_logger)\n",
    "#             # testing\n",
    "#             model_loss = test_multitask_disjoint(model,\n",
    "#                                                 cluster,\n",
    "#                                                 loss_fn=loss_fn,\n",
    "#                                                 batch_size=batch_size,\n",
    "#                                                 A_test=A_test,\n",
    "#                                                 X_test=X_test,\n",
    "#                                                 E_test=E_test,\n",
    "#                                                 y_test=y_test)\n",
    "#             X_, A_, E_, I_ = numpy_to_disjoint(X_test, A_test, E_test)\n",
    "#             A_ = ops.sp_matrix_to_sp_tensor(A_)\n",
    "#             cluster_pred = model([X_, A_, E_, I_], training=False)\n",
    "        \n",
    "#         if len(cluster) == 1:\n",
    "#             cluster_pred = [cluster_pred]\n",
    "            \n",
    "#         for prop, batch_pred in zip(cluster, cluster_pred):\n",
    "#             batch_pred = task_to_scaler[prop].inverse_transform(batch_pred)\n",
    "#             errors = list()\n",
    "#             for index, pred in zip(y_test.index.values, batch_pred):\n",
    "#                 actual = y_all.loc[index, prop]\n",
    "#                 model_data.add_test(prop, actual, pred[0])\n",
    "        \n",
    "#         # save_model(model, cluster, task_to_scaler, mode=mode, conv=conv)\n",
    "#         model_data.serialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QM9 dataset.\n",
      "Reading SDF\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_8/root_kernel:0', 'edge_conditioned_conv_9/root_kernel:0', 'edge_conditioned_conv_10/root_kernel:0', 'edge_conditioned_conv_11/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_8/root_kernel:0', 'edge_conditioned_conv_9/root_kernel:0', 'edge_conditioned_conv_10/root_kernel:0', 'edge_conditioned_conv_11/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_8/root_kernel:0', 'edge_conditioned_conv_9/root_kernel:0', 'edge_conditioned_conv_10/root_kernel:0', 'edge_conditioned_conv_11/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_8/root_kernel:0', 'edge_conditioned_conv_9/root_kernel:0', 'edge_conditioned_conv_10/root_kernel:0', 'edge_conditioned_conv_11/root_kernel:0'] when minimizing the loss.\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 1.9597 - dense_14_loss: 0.9697 - dense_15_loss: 0.9899 - val_loss: 3.7859 - val_dense_14_loss: 2.0743 - val_dense_15_loss: 1.7116\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 0s 198ms/step - loss: 1.9413 - dense_14_loss: 0.9637 - dense_15_loss: 0.9776 - val_loss: 3.8419 - val_dense_14_loss: 2.0918 - val_dense_15_loss: 1.7500\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 0s 173ms/step - loss: 1.9185 - dense_14_loss: 0.9557 - dense_15_loss: 0.9629 - val_loss: 3.7864 - val_dense_14_loss: 2.0728 - val_dense_15_loss: 1.7136\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 0s 250ms/step - loss: 1.9051 - dense_14_loss: 0.9527 - dense_15_loss: 0.9524 - val_loss: 3.7597 - val_dense_14_loss: 2.0640 - val_dense_15_loss: 1.6956\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 0s 226ms/step - loss: 1.8831 - dense_14_loss: 0.9470 - dense_15_loss: 0.9361 - val_loss: 3.7385 - val_dense_14_loss: 2.0505 - val_dense_15_loss: 1.6881\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8185 - dense_14_loss: 0.3186 - dense_15_loss: 0.4999\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_12/root_kernel:0', 'edge_conditioned_conv_13/root_kernel:0', 'edge_conditioned_conv_14/root_kernel:0', 'edge_conditioned_conv_15/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_12/root_kernel:0', 'edge_conditioned_conv_13/root_kernel:0', 'edge_conditioned_conv_14/root_kernel:0', 'edge_conditioned_conv_15/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_12/root_kernel:0', 'edge_conditioned_conv_13/root_kernel:0', 'edge_conditioned_conv_14/root_kernel:0', 'edge_conditioned_conv_15/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_12/root_kernel:0', 'edge_conditioned_conv_13/root_kernel:0', 'edge_conditioned_conv_14/root_kernel:0', 'edge_conditioned_conv_15/root_kernel:0'] when minimizing the loss.\n",
      "3/3 [==============================] - 0s 153ms/step - loss: 3.3993 - dense_18_loss: 1.8442 - dense_19_loss: 1.5551 - val_loss: 0.7656 - val_dense_18_loss: 0.4348 - val_dense_19_loss: 0.3308\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 2.5363 - dense_18_loss: 1.4955 - dense_19_loss: 1.0408 - val_loss: 1.8900 - val_dense_18_loss: 0.6306 - val_dense_19_loss: 1.2595\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 2.4752 - dense_18_loss: 1.2801 - dense_19_loss: 1.1951 - val_loss: 0.7709 - val_dense_18_loss: 0.4650 - val_dense_19_loss: 0.3059\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 2.6130 - dense_18_loss: 1.2892 - dense_19_loss: 1.3238 - val_loss: 0.9580 - val_dense_18_loss: 0.4581 - val_dense_19_loss: 0.4999\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 2.2470 - dense_18_loss: 1.1536 - dense_19_loss: 1.0934 - val_loss: 0.8716 - val_dense_18_loss: 0.4306 - val_dense_19_loss: 0.4410\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4728 - dense_18_loss: 0.8734 - dense_19_loss: 0.5994\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_16/root_kernel:0', 'edge_conditioned_conv_17/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_16/root_kernel:0', 'edge_conditioned_conv_17/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_16/root_kernel:0', 'edge_conditioned_conv_17/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_16/root_kernel:0', 'edge_conditioned_conv_17/root_kernel:0'] when minimizing the loss.\n",
      "2/2 [==============================] - 1s 733ms/step - loss: 1.8777 - dense_22_loss: 0.9614 - dense_23_loss: 0.9163 - val_loss: 2.7405 - val_dense_22_loss: 1.4064 - val_dense_23_loss: 1.3341\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 1s 600ms/step - loss: 1.8745 - dense_22_loss: 0.9589 - dense_23_loss: 0.9156 - val_loss: 2.7387 - val_dense_22_loss: 1.4061 - val_dense_23_loss: 1.3326\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 1s 614ms/step - loss: 1.8723 - dense_22_loss: 0.9579 - dense_23_loss: 0.9144 - val_loss: 2.7396 - val_dense_22_loss: 1.4064 - val_dense_23_loss: 1.3332\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 1s 595ms/step - loss: 1.8708 - dense_22_loss: 0.9571 - dense_23_loss: 0.9137 - val_loss: 2.7403 - val_dense_22_loss: 1.4063 - val_dense_23_loss: 1.3340\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 1s 614ms/step - loss: 1.8697 - dense_22_loss: 0.9563 - dense_23_loss: 0.9133 - val_loss: 2.7372 - val_dense_22_loss: 1.4039 - val_dense_23_loss: 1.3333\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3194 - dense_22_loss: 0.9509 - dense_23_loss: 1.3685\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_18/root_kernel:0', 'edge_conditioned_conv_19/root_kernel:0', 'edge_conditioned_conv_20/root_kernel:0', 'edge_conditioned_conv_21/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_18/root_kernel:0', 'edge_conditioned_conv_19/root_kernel:0', 'edge_conditioned_conv_20/root_kernel:0', 'edge_conditioned_conv_21/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_18/root_kernel:0', 'edge_conditioned_conv_19/root_kernel:0', 'edge_conditioned_conv_20/root_kernel:0', 'edge_conditioned_conv_21/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_18/root_kernel:0', 'edge_conditioned_conv_19/root_kernel:0', 'edge_conditioned_conv_20/root_kernel:0', 'edge_conditioned_conv_21/root_kernel:0'] when minimizing the loss.\n",
      "3/3 [==============================] - 2s 565ms/step - loss: 1.9159 - dense_26_loss: 0.9824 - dense_27_loss: 0.9335 - val_loss: 2.7587 - val_dense_26_loss: 1.2382 - val_dense_27_loss: 1.5205\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 1s 444ms/step - loss: 1.9099 - dense_26_loss: 0.9790 - dense_27_loss: 0.9308 - val_loss: 2.7520 - val_dense_26_loss: 1.2347 - val_dense_27_loss: 1.5173\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 1s 406ms/step - loss: 1.9050 - dense_26_loss: 0.9767 - dense_27_loss: 0.9283 - val_loss: 2.7477 - val_dense_26_loss: 1.2316 - val_dense_27_loss: 1.5161\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 1s 407ms/step - loss: 1.8993 - dense_26_loss: 0.9738 - dense_27_loss: 0.9255 - val_loss: 2.7427 - val_dense_26_loss: 1.2292 - val_dense_27_loss: 1.5135\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 1s 435ms/step - loss: 1.8933 - dense_26_loss: 0.9709 - dense_27_loss: 0.9224 - val_loss: 2.7359 - val_dense_26_loss: 1.2257 - val_dense_27_loss: 1.5102\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9722 - dense_26_loss: 0.9271 - dense_27_loss: 1.0451\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_22/root_kernel:0', 'edge_conditioned_conv_23/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_22/root_kernel:0', 'edge_conditioned_conv_23/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_22/root_kernel:0', 'edge_conditioned_conv_23/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_22/root_kernel:0', 'edge_conditioned_conv_23/root_kernel:0'] when minimizing the loss.\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.9842 - dense_30_loss: 0.9988 - dense_31_loss: 0.9854 - val_loss: 2.6560 - val_dense_30_loss: 1.3006 - val_dense_31_loss: 1.3554\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.9784 - dense_30_loss: 0.9967 - dense_31_loss: 0.9817 - val_loss: 2.6472 - val_dense_30_loss: 1.2998 - val_dense_31_loss: 1.3474\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.9715 - dense_30_loss: 0.9939 - dense_31_loss: 0.9776 - val_loss: 2.6314 - val_dense_30_loss: 1.2940 - val_dense_31_loss: 1.3373\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.9637 - dense_30_loss: 0.9910 - dense_31_loss: 0.9727 - val_loss: 2.6099 - val_dense_30_loss: 1.2878 - val_dense_31_loss: 1.3221\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.9530 - dense_30_loss: 0.9878 - dense_31_loss: 0.9652 - val_loss: 2.5916 - val_dense_30_loss: 1.2833 - val_dense_31_loss: 1.3083\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5376 - dense_30_loss: 0.7483 - dense_31_loss: 0.7893\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_24/root_kernel:0', 'edge_conditioned_conv_25/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_24/root_kernel:0', 'edge_conditioned_conv_25/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_24/root_kernel:0', 'edge_conditioned_conv_25/root_kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_24/root_kernel:0', 'edge_conditioned_conv_25/root_kernel:0'] when minimizing the loss.\n",
      "2/2 [==============================] - 22s 11s/step - loss: 1.6617 - dense_34_loss: 0.8342 - dense_35_loss: 0.8275 - val_loss: 0.9989 - val_dense_34_loss: 0.4566 - val_dense_35_loss: 0.5423\n",
      "Epoch 2/5\n"
     ]
    }
   ],
   "source": [
    "def random_hyperparameter_search(trials=50):\n",
    "    mode = 'batch'\n",
    "    conv = 'ecc'\n",
    "    amount = 1000 # None\n",
    "    A_all, X_all, E_all, y_all = load_data(amount=amount, mode=mode)\n",
    "    \n",
    "    for _ in range(trials):\n",
    "        batch_size = random.choice([16, 32, 64])\n",
    "        learning_rate = random.choice([1e-2, 1e-3, 1e-4])\n",
    "        epochs = 5 # random.choice([20, 30, 40])\n",
    "        layer_sizes = [random.choice([16, 32, 64, 128, 256, 512]), \n",
    "               random.choice([16, 32, 64, 128, 256, 512]), \n",
    "               random.choice([16, 32, 64, 128, 256, 512]), \n",
    "               random.choice([64, 128, 256, 512])]\n",
    "        num_sampled = 100 # 30000\n",
    "        scaler = random.choice(['power_transformer', 'standard_scaler'])\n",
    "        loss_fn = random.choice([MeanAbsoluteError(), MeanSquaredError()])\n",
    "        soft_sharing = random.random() < 0.5\n",
    "        share_param = random.choice([1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3])\n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "        A, X, E, y = sample_from_data(num_sampled,\n",
    "                                     A_all,\n",
    "                                     X_all,\n",
    "                                     E_all,\n",
    "                                     y_all,\n",
    "                                     mode=mode)\n",
    "        task_to_scaler = standardize(y, scaler=scaler)\n",
    "        \n",
    "        cluster = ['r2', 'cv']\n",
    "        A_train, A_test, \\\n",
    "        X_train, X_test, \\\n",
    "        E_train, E_test, \\\n",
    "        y_train, y_test = train_test_split(A, X, E, y, test_size=0.1)\n",
    "        \n",
    "        if soft_sharing:\n",
    "            model, _ = build_soft_sharing_model(A=A_train,\n",
    "                                                X=X_train,\n",
    "                                                E=E_train,\n",
    "                                                num_tasks=len(cluster),\n",
    "                                                share_param=share_param,\n",
    "                                                mode=mode,\n",
    "                                                conv=conv,\n",
    "                                                layer_sizes=layer_sizes)\n",
    "        else:\n",
    "            model, _ = build_hard_sharing_model(A=A_train,\n",
    "                                                X=X_train,\n",
    "                                                E=E_train,\n",
    "                                                num_tasks=len(cluster),\n",
    "                                                mode=mode,\n",
    "                                                conv=conv,\n",
    "                                                layer_sizes=layer_sizes)\n",
    "        \n",
    "        stream = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
    "        summary = stream.getvalue()\n",
    "        \n",
    "        params = {'mode': mode,\n",
    "                 'conv': conv,\n",
    "                 'batch_size': batch_size,\n",
    "                 'epochs': epochs,\n",
    "                 'num_sampled': num_sampled,\n",
    "                 'learning_rate': learning_rate,\n",
    "                 'cluster': cluster,\n",
    "                 'hard_sharing': not soft_sharing,\n",
    "                 'share_param': share_param,\n",
    "                 'model_summary': summary,\n",
    "                 'loss_fn': type(loss_fn).__name__,\n",
    "                 'optimizer': type(optimizer).__name__}\n",
    "        model_data = ModelData(params=params)\n",
    "        \n",
    "        y_train_cluster = np.hsplit(y_train[cluster].values, len(cluster))\n",
    "        model.compile(optimizer=optimizer, \n",
    "                      loss=loss_fn)\n",
    "        model.fit(x=[X_train, A_train, E_train], \n",
    "                  y=y_train_cluster,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_split=0.1,\n",
    "                  epochs=epochs,\n",
    "                  callbacks=[model_data.loss_logger])\n",
    "        \n",
    "        y_test_cluster = np.hsplit(y_test[cluster].values, len(cluster))\n",
    "        model_loss = model.evaluate(x=[X_test, A_test, E_test],\n",
    "                                    y=y_test_cluster)\n",
    "        model_data.test_loss = model_loss\n",
    "        cluster_pred = model.predict([X_test, A_test, E_test])\n",
    "\n",
    "        for prop, batch_pred in zip(cluster, cluster_pred):\n",
    "            batch_pred = task_to_scaler[prop].inverse_transform(batch_pred)\n",
    "            errors = list()\n",
    "            for index, pred in zip(y_test.index.values, batch_pred):\n",
    "                actual = y_all.loc[index, prop]\n",
    "                model_data.add_test(prop, actual=actual, pred=pred[0])\n",
    "        \n",
    "        dirname = 'model_data'\n",
    "        dt_string = model_data.timestamp.strftime('%d-%m-%Y_%H-%M-%S')\n",
    "        filename = path.join(dirname, 'hyperparam_search' + dt_string + '.pkl')\n",
    "        # model_data.serialize(filename=filename)\n",
    "\n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    random_hyperparameter_search(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
