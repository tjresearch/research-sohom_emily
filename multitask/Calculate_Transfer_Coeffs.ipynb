{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading QM9 dataset.\nReading SDF\n"
    }
   ],
   "source": [
    "%run QM9GNN2_Multitask.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=learning_rate)\n",
    "loss = 'mse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_task_filename(task):\n",
    "    return path.join('single_task_trained_models', task + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "learning A\nTrain on 8100 samples, validate on 900 samples\nEpoch 1/25\nWARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_12/root_kernel:0', 'edge_conditioned_conv_13/root_kernel:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_12/root_kernel:0', 'edge_conditioned_conv_13/root_kernel:0'] when minimizing the loss.\n8100/8100 [==============================] - 140s 17ms/sample - loss: 0.9081 - val_loss: 0.9522\nEpoch 2/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.7268 - val_loss: 0.8841\nEpoch 3/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.7056 - val_loss: 0.8596\nEpoch 4/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.6971 - val_loss: 0.8593\nEpoch 5/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.6915 - val_loss: 0.9159\nEpoch 6/25\n8100/8100 [==============================] - 136s 17ms/sample - loss: 0.6886 - val_loss: 0.8231\nEpoch 7/25\n8100/8100 [==============================] - 136s 17ms/sample - loss: 0.6826 - val_loss: 0.8687\nEpoch 8/25\n8100/8100 [==============================] - 137s 17ms/sample - loss: 0.6741 - val_loss: 0.8454\nEpoch 9/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.6772 - val_loss: 0.8376\nEpoch 10/25\n8100/8100 [==============================] - 140s 17ms/sample - loss: 0.6879 - val_loss: 0.7939\nEpoch 11/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.6715 - val_loss: 0.8173\nEpoch 12/25\n8100/8100 [==============================] - 137s 17ms/sample - loss: 0.6652 - val_loss: 0.8127\nEpoch 13/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.6671 - val_loss: 0.7936\nEpoch 14/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.6615 - val_loss: 0.8078\nEpoch 15/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.6622 - val_loss: 0.7856\nEpoch 16/25\n8100/8100 [==============================] - 136s 17ms/sample - loss: 0.6718 - val_loss: 0.7716\nEpoch 17/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.6552 - val_loss: 0.7861\nEpoch 18/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.6400 - val_loss: 0.7483\nEpoch 19/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.6326 - val_loss: 0.7575\nEpoch 20/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.6339 - val_loss: 0.7615\nEpoch 21/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.6281 - val_loss: 0.7132\nEpoch 22/25\n8100/8100 [==============================] - 133s 16ms/sample - loss: 0.6139 - val_loss: 0.7389\nEpoch 23/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.6285 - val_loss: 0.7214\nEpoch 24/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.6102 - val_loss: 0.7994\nEpoch 25/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.6149 - val_loss: 0.7040\nlearning B\nTrain on 8100 samples, validate on 900 samples\nEpoch 1/25\nWARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_14/root_kernel:0', 'edge_conditioned_conv_15/root_kernel:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['edge_conditioned_conv_14/root_kernel:0', 'edge_conditioned_conv_15/root_kernel:0'] when minimizing the loss.\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.9206 - val_loss: 0.7735\nEpoch 2/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.7106 - val_loss: 0.6867\nEpoch 3/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.6291 - val_loss: 0.6010\nEpoch 4/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.5892 - val_loss: 0.5792\nEpoch 5/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.5436 - val_loss: 0.5404\nEpoch 6/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.5379 - val_loss: 0.5022\nEpoch 7/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.5126 - val_loss: 0.5071\nEpoch 8/25\n8100/8100 [==============================] - 140s 17ms/sample - loss: 0.4794 - val_loss: 0.4920\nEpoch 9/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.4609 - val_loss: 0.4600\nEpoch 10/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.4556 - val_loss: 0.4812\nEpoch 11/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.4454 - val_loss: 0.4282\nEpoch 12/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.4250 - val_loss: 0.4224\nEpoch 13/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.4241 - val_loss: 0.4088\nEpoch 14/25\n8100/8100 [==============================] - 142s 18ms/sample - loss: 0.4056 - val_loss: 0.4233\nEpoch 15/25\n8100/8100 [==============================] - 141s 17ms/sample - loss: 0.4038 - val_loss: 0.3752\nEpoch 16/25\n8100/8100 [==============================] - 140s 17ms/sample - loss: 0.3999 - val_loss: 0.4484\nEpoch 17/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.3957 - val_loss: 0.4361\nEpoch 18/25\n8100/8100 [==============================] - 140s 17ms/sample - loss: 0.3986 - val_loss: 0.3960\nEpoch 19/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.3863 - val_loss: 0.3608\nEpoch 20/25\n8100/8100 [==============================] - 136s 17ms/sample - loss: 0.3816 - val_loss: 0.3855\nEpoch 21/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.3706 - val_loss: 0.3595\nEpoch 22/25\n8100/8100 [==============================] - 139s 17ms/sample - loss: 0.3886 - val_loss: 0.3626\nEpoch 23/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.3709 - val_loss: 0.3751\nEpoch 24/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.3777 - val_loss: 0.4075\nEpoch 25/25\n8100/8100 [==============================] - 138s 17ms/sample - loss: 0.3790 - val_loss: 0.3552\n"
    }
   ],
   "source": [
    "for i in range(0, 2):\n",
    "    print('learning', tasks[i])\n",
    "    model = create_single_task_model(X_in, A_in, E_in, n_out)\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    es_callback = EarlyStopping(monitor='val_loss', patience=es_patience)\n",
    "    model.fit([X_train, A_train, E_train],\n",
    "             y_train_list[i],\n",
    "             batch_size=batch_size,\n",
    "             validation_split=0.1,\n",
    "             epochs=epochs,\n",
    "             callbacks=[es_callback])\n",
    "    model.save_weights(generate_single_task_filename(tasks[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_compile_model(task):\n",
    "    model = create_single_task_model(X_in=X_in, A_in=A_in, E_in=E_in, n_out=n_out)\n",
    "    model.load_weights(generate_single_task_filename(task))\n",
    "    model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transfer_coefficient_from_file(task_learned, task_transferred, y_transferred_test):\n",
    "    model_learned = create_single_task_model(X_in=X_in, A_in=A_in, E_in=E_in, n_out=n_out)\n",
    "    model_transferred = create_single_task_model(X_in=X_in, A_in=A_in, E_in=E_in, n_out=n_out)\n",
    "    model_learned.load_weights(generate_single_task_filename(task_learned))\n",
    "    model_transferred.load_weights(generate_single_task_filename(task_transferred))\n",
    "    \n",
    "    layers_learned = model_learned.get_weights()\n",
    "    layers_transferred = model_transferred.get_weights()    \n",
    "    \n",
    "    # 10 is a hard-coded value dependent on the architecture\n",
    "    layers_transferred = layers_learned[:10] + layers_transferred[10:]\n",
    "    model_transferred.set_weights(layers_transferred)\n",
    "    model_transferred.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    eval_results = model_transferred.evaluate([X_test, A_test, E_test],\n",
    "                                              y_transferred_test,\n",
    "                                              batch_size=batch_size)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_test_single_task_model(task_learned, y_test):\n",
    "    model = create_single_task_model(X_in, A_in, E_in, n_out)\n",
    "    model.load_weights(path.join('single_task_trained_models', task_learned + '.h5'))\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    eval_results = model.evaluate([X_test, A_test, E_test], y_test, batch_size=batch_size)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1000/1000 [==============================] - 7s 7ms/sample - loss: 0.6330\n0.6329849510192871\n1000/1000 [==============================] - 7s 7ms/sample - loss: 1.0046\n1.0045776519775391\n"
    }
   ],
   "source": [
    "print(load_and_test_single_task_model('A', y_test_list[0]))\n",
    "print(calculate_transfer_coefficient_from_file('A', 'B', y_test_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Layer #0 (named \"edge_conditioned_conv_2\" in the current model) was found to correspond to layer edge_conditioned_conv_1 in the save file. However the new layer edge_conditioned_conv_2 expects 4 weights, but the saved weights have 3 elements.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dbc8be999219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_and_test_single_task_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-0943cc7b4591>\u001b[0m in \u001b[0;36mload_and_test_single_task_model\u001b[0;34m(task_learned, y_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_test_single_task_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_learned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_single_task_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'single_task_trained_models'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_learned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   1221\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    695\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m                        \u001b[0;34m' weights, but the saved weights have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                        str(len(weight_values)) + ' elements.')\n\u001b[0m\u001b[1;32m    698\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer #0 (named \"edge_conditioned_conv_2\" in the current model) was found to correspond to layer edge_conditioned_conv_1 in the save file. However the new layer edge_conditioned_conv_2 expects 4 weights, but the saved weights have 3 elements."
     ]
    }
   ],
   "source": [
    "for task, test_data in zip(tasks, y_test_list):\n",
    "    print(task, load_and_test_single_task_model(task, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Layer #0 (named \"edge_conditioned_conv_4\" in the current model) was found to correspond to layer edge_conditioned_conv_1 in the save file. However the new layer edge_conditioned_conv_4 expects 4 weights, but the saved weights have 3 elements.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f17a0bfe0e65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_evaluate_multitask_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b6859bef08a7>\u001b[0m in \u001b[0;36mload_and_evaluate_multitask_model\u001b[0;34m(tasks, y_test_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_evaluate_multitask_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_hard_parameter_sharing_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_model_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtesting_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_test_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   1221\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    695\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m                        \u001b[0;34m' weights, but the saved weights have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                        str(len(weight_values)) + ' elements.')\n\u001b[0m\u001b[1;32m    698\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer #0 (named \"edge_conditioned_conv_4\" in the current model) was found to correspond to layer edge_conditioned_conv_1 in the save file. However the new layer edge_conditioned_conv_4 expects 4 weights, but the saved weights have 3 elements."
     ]
    }
   ],
   "source": [
    "for cluster in clusters:\n",
    "    losses = load_and_evaluate_multitask_model(cluster, y_test_list)\n",
    "    for task, loss in zip(cluster, losses):\n",
    "        print(task, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['A', 'B', 'C', 'mu', 'alpha', 'homo', 'lumo', 'gap', 'r2', 'zpve', 'u0', 'u298', 'h298', 'g298', 'cv', 'u0_atom', 'u298_atom', 'h298_atom', 'g298_atom']\nA B\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Layer #0 (named \"edge_conditioned_conv_6\" in the current model) was found to correspond to layer edge_conditioned_conv_1 in the save file. However the new layer edge_conditioned_conv_6 expects 4 weights, but the saved weights have 3 elements.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d76cc46c6280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtask_transferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_learned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_transferred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtransfer_coefficient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_transfer_coefficient_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_learned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_transferred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_coefficient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-e4391dd1e228>\u001b[0m in \u001b[0;36mcalculate_transfer_coefficient_from_file\u001b[0;34m(task_learned, task_transferred, y_transferred_test)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_learned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_single_task_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mE_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_transferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_single_task_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mE_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel_learned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_single_task_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_learned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_transferred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_single_task_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_transferred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   1221\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    695\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m                        \u001b[0;34m' weights, but the saved weights have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                        str(len(weight_values)) + ' elements.')\n\u001b[0m\u001b[1;32m    698\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer #0 (named \"edge_conditioned_conv_6\" in the current model) was found to correspond to layer edge_conditioned_conv_1 in the save file. However the new layer edge_conditioned_conv_6 expects 4 weights, but the saved weights have 3 elements."
     ]
    }
   ],
   "source": [
    "print(tasks)\n",
    "transfer_coefficient_dict = dict()\n",
    "for i, j in itertools.permutations(range(len(tasks)), 2):\n",
    "    task_learned = tasks[i]\n",
    "    task_transferred = tasks[j]\n",
    "    print(task_learned, task_transferred)\n",
    "    transfer_coefficient = calculate_transfer_coefficient_from_file(task_learned, task_transferred, y_test_list[j])\n",
    "    print(transfer_coefficient)\n",
    "    \n",
    "    if task_learned not in transfer_coefficient_dict.keys():\n",
    "        transfer_coefficient_dict[task_learned] = {task_transferred: transfer_coefficient}\n",
    "    else:\n",
    "        transfer_coefficient_dict[task_learned][task_transferred] = transfer_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('cross_task_transfer_coefficients.txt', 'w')\n",
    "for key1, value in transfer_coefficient_dict.items():\n",
    "    for key2, coef in value.items():\n",
    "        print(key1, key2, coef)\n",
    "        print(key1, key2, coef, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest of this file is comprised of mostly useless analysis code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_transfer_coefficients():\n",
    "    with open('cross_task_transfer_coefficients.txt', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        transfer_coefficient_dict = dict()\n",
    "        for line in lines:\n",
    "            task_learned, task_transferred, transfer_coefficient = line.strip().split()\n",
    "            if task_learned not in transfer_coefficient_dict.keys():\n",
    "                transfer_coefficient_dict[task_learned] = {task_transferred: float(transfer_coefficient)}\n",
    "            else:\n",
    "                transfer_coefficient_dict[task_learned][task_transferred] = float(transfer_coefficient)\n",
    "        return transfer_coefficient_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_coefficient = analyze_transfer_coefficients()\n",
    "coef_list = list()\n",
    "# relevant_tasks = ['alpha', 'homo', 'lumo', 'gap']\n",
    "relevant_tasks = ['mu', 'homo']\n",
    "for task_set in itertools.combinations(relevant_tasks, 3):\n",
    "    coef_sum = 0\n",
    "    for i, j in itertools.combinations(task_set, 2):\n",
    "        coef_sum += transfer_coefficient[i][j]**2 + transfer_coefficient[j][i]**2\n",
    "    coef_list.append((coef_sum, task_set))\n",
    "# coef_list = np.asarray(coef_list)\n",
    "# coef_list = np.sort(coef_list)\n",
    "coef_list.sort()\n",
    "for entry in coef_list:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in itertools.combinations(transfer_coefficient.keys(), 2):\n",
    "    if transfer_coefficient[i][j] < 0.9 and transfer_coefficient[j][i] < 0.9:\n",
    "        print(i, \n",
    "              j, \n",
    "              round(transfer_coefficient[i][j], 3), \n",
    "              round(transfer_coefficient[j][i], 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}