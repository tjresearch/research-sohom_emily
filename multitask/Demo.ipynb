{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading QM9 dataset.\nReading SDF\n"
    }
   ],
   "source": [
    "%run QM9GNN2_Multitask.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psi4\n",
    "import time\n",
    "from os import path, getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_property_neural(prop=None, mol_id=-1):\n",
    "    if mol_id == -1:\n",
    "        raise ValueError(\"ID must be between 1 and 133885\")\n",
    "    for cluster in clusters:\n",
    "        if prop in cluster:\n",
    "            model = create_hard_parameter_sharing_model(X_in, A_in, E_in, n_out, len(cluster))\n",
    "            model.load_weights(generate_model_filename(cluster))\n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "            predictions = model.predict([[X_complete[mol_id-1]], [A_complete[mol_id-1]], [E_complete[mol_id-1]]])\n",
    "            mean, std = 0, 1\n",
    "            with open(generate_model_helper_filename(prop), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                mean = float(lines[0].strip())\n",
    "                std = float(lines[1].strip())\n",
    "            prediction = mean + std * predictions[cluster.index(prop)]\n",
    "            return prediction[0][0]\n",
    "    if prop == 'gap':\n",
    "        lumo = predict_property_neural(prop='lumo', mol_id=mol_id)\n",
    "        homo = predict_property_neural(prop='homo', mol_id=mol_id)\n",
    "        return lumo - homo\n",
    "    raise ValueError(\"Property was not found in clusters list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_folder_path():\n",
    "    return path.join(getcwd(), '..', '..', 'dsgdb9nsd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_from_file(filenum):\n",
    "    filepath = path.join(get_data_folder_path(), \n",
    "                           'dsgdb9nsd_' + str(filenum).zfill(6) + '.xyz')\n",
    "    f = open(filepath, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    num_atoms = int(lines[0])\n",
    "    atom_list = lines[2:2+num_atoms]\n",
    "    for i in range(len(atom_list)):\n",
    "        atom_list[i] = atom_list[i][:atom_list[i].rfind(\"\\t\")] + \"\\n\"\n",
    "    return psi4.geometry(\"\".join(atom_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dft_output_file_path(filenum):\n",
    "    return path.join('psi4_output', 'output_'+str(filenum)+'.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_molecule(filenum, thermochemical=False):\n",
    "    psi4.core.set_output_file(generate_dft_output_file_path(filenum), False)\n",
    "    psi4.set_memory('2 GB')\n",
    "    molecule = get_molecule_from_file(filenum)\n",
    "    if thermochemical:\n",
    "        e, wfn = psi4.freq('b3lyp/cc-pvqz', molecule=molecule, return_wfn=True)\n",
    "    else:\n",
    "        e, wfn = psi4.energy('b3lyp/cc-pvqz', molecule=molecule, return_wfn=True)\n",
    "    return wfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rotational_constants(filenum, wfn):\n",
    "    f = open(generate_dft_output_file_path(filenum), 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for i in range(len(lines)-1, -1, -1):\n",
    "        if lines[i].find('Rotational constants:') > -1 and lines[i].find('[MHz]') > -1:\n",
    "            words = lines[i].split()\n",
    "            rot_constants = []\n",
    "            for const in [words[4], words[7], words[10]]:     \n",
    "                if const.isnumeric():\n",
    "                    rot_constants.append(float(const)/1000)\n",
    "                else:\n",
    "                    rot_constants.append(const)\n",
    "                    # rot_constants.append(float(const)/1000)\n",
    "            return rot_constants\n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dipole_moment(filenum, wfn):\n",
    "    f = open(generate_dft_output_file_path(filenum), 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for i in range(len(lines)-1, -1, -1):\n",
    "        if lines[i].find(\"Dipole Moment: [D]\") > -1:\n",
    "            return lines[i+1][lines[i+1].find(\"Total:\") + 6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_homo_lumo(filenum, wfn):\n",
    "    homo = wfn.epsilon_a_subset(\"AO\", \"ALL\").get(wfn.nalpha())\n",
    "    lumo = wfn.epsilon_a_subset(\"AO\", \"ALL\").get(wfn.nalpha() + 1)\n",
    "    return homo, lumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gap(filenum, wfn):\n",
    "    homo, lumo = extract_homo_lumo_gap(fileum, wfn)\n",
    "    return lumo - homo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zpve(filenum, wfn):\n",
    "    f = open(generate_dft_output_file_path(filenum), 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for i in range(len(lines)-1, -1, -1):\n",
    "        if lines[i].find(\"Total ZPE, Electronic energy at 0 [K]\") > -1:\n",
    "            words = lines[i].split()\n",
    "            return words[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zero_point_internal_energy(filenum, wfn):\n",
    "    f = open(generate_dft_output_file_path(filenum), 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for i in range(len(lines)-1, -1, -1):\n",
    "        if lines[i].find(\"Total E0, Electronic energy\") > -1:\n",
    "            words = lines[i].split()\n",
    "            return words[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_internal_energy(filenum, wfn):\n",
    "    f = open(generate_dft_output_file_path(filenum), 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for i in range(len(lines)-1, -1, -1):\n",
    "        if lines[i].find(\"Total E, Electronic energy at  298.15 [K]\") > -1:\n",
    "            words = lines[i].split()\n",
    "            return words[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enthalpy(filenum, wfn):\n",
    "    f = open(generate_dft_output_file_path(filenum), 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for i in range(len(lines)-1, -1, -1):\n",
    "        if lines[i].find(\"Total H, Enthalpy at  298.15 [K]\") > -1:\n",
    "            words = lines[i].split()\n",
    "            return words[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gibbs_free_energy(filenum, wfn):\n",
    "    f = open(generate_dft_output_file_path(filenum), 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for i in range(len(lines)-1, -1, -1):\n",
    "        if lines[i].find(\"Total G,\") > -1:\n",
    "            words = lines[i].split()\n",
    "            return words[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cv(filenum, wfn):\n",
    "    f = open(generate_dft_output_file_path(filenum), 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for i in range(len(lines)-1, -1, -1):\n",
    "        if lines[i].find('Total Cv') > -1:\n",
    "            words = lines[i].split()\n",
    "            return words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process(start_num, end_num, thermochemical=False):\n",
    "    f = open(\"output.csv\", \"w\")\n",
    "    output_header = \"Index,A,B,C,Dipole,HOMO,LUMO\"\n",
    "    if thermochemical:\n",
    "        output_header += \",zpve,H 298.15,G 298.15\"\n",
    "    output_header += \"\\n\"\n",
    "    f.write(output_header)\n",
    "    for filenum in range(start_num, end_num+1):\n",
    "        wfn = process_molecule(filenum, thermochemical=thermochemical)\n",
    "        a, b, c = extract_rotational_constants(filenum, wfn)\n",
    "        dipole = extract_dipole_moment(filenum, wfn)\n",
    "        homo, lumo = extract_homo_lumo(filenum, wfn)\n",
    "        output = str(filenum) + \",\" + str(a) + \",\" + str(b) + \",\" + str(c) + \",\" + str(dipole) + \",\" + str(homo) + \",\" + str(lumo)\n",
    "        if thermochemical:\n",
    "            zpve = extract_zpve(filenum, wfn)\n",
    "            enthalpy = extract_enthalpy(filenum, wfn)\n",
    "            gibbs_free_energy = extract_gibbs_free_energy(filenum, wfn)\n",
    "            output += \",\" + str(zpve) + \",\" + str(enthalpy) + \",\" + str(gibbs_free_energy)\n",
    "        output += \"\\n\" \n",
    "        f.write(output)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_properties_dft(filenum=-1, thermochemical=False):\n",
    "    if filenum == -1:\n",
    "        raise ValueError(\"Filenum must be between 1 and 133885\")\n",
    "    wfn = process_molecule(filenum, thermochemical=thermochemical)\n",
    "    a, b, c = extract_rotational_constants(filenum, wfn)\n",
    "    dipole = extract_dipole_moment(filenum, wfn)\n",
    "    homo, lumo = extract_homo_lumo(filenum, wfn)\n",
    "    ret_dict = {'A': a, 'B': b, 'C': c, 'mu': dipole, 'homo': homo, \n",
    "                'lumo': lumo, 'gap': lumo-homo}\n",
    "    if thermochemical:\n",
    "        zpve = extract_zpve(filenum, wfn)\n",
    "        internal_energy = extract_internal_energy(filenum, wfn)\n",
    "        u0 = extract_zero_point_internal_energy(filenum, wfn)\n",
    "        enthalpy = extract_enthalpy(filenum, wfn)\n",
    "        gibbs_free_energy = extract_gibbs_free_energy(filenum, wfn)\n",
    "        cv = extract_cv(filenum, wfn)\n",
    "        ret_dict.update({'zpve': zpve, 'u0': u0, 'u298': internal_energy, \n",
    "                           'h298': enthalpy, 'g298': gibbs_free_energy, 'cv': cv})\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_property(prop=None, mol_id=-1):\n",
    "    return y_complete.loc[mol_id-1, prop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_user_for_calculation():\n",
    "    while True:\n",
    "        num = -1\n",
    "        while num < 1 or num > 133885:\n",
    "            try:\n",
    "                num = int(input('Choose a molecule index (1-133885): '))\n",
    "            except ValueError:\n",
    "                print(\"Please provide a valid number\")\n",
    "                num = -1\n",
    "            \n",
    "        properties = ['A', 'B', 'C', 'mu', 'homo', 'lumo', 'gap', 'zpve', 'u0', 'u298', 'h298', 'g298', 'cv']\n",
    "        prop = None\n",
    "        while prop not in properties:\n",
    "            print(\"Choose an available property from the following:\")\n",
    "            print(\"A, B, C, mu, homo, lumo, gap, zpve, u0, u298, h298, g298, cv\")\n",
    "            prop = input('Choose a property: ')\n",
    "        \n",
    "        dft = None\n",
    "        while dft not in ['0', '1']:\n",
    "            print(\"Choose whether to use DFT or neural methods\")\n",
    "            print(\"0 for neural methods, 1 for DFT\")\n",
    "            dft = input('Calculation type: ')\n",
    "        \n",
    "        if dft == '1':\n",
    "            print('Beginning DFT calculation')\n",
    "            start = time.time()\n",
    "            thermochemical = prop in properties[7:]\n",
    "            ret_dict = predict_all_properties_dft(num, thermochemical=thermochemical)\n",
    "            print(ret_dict[prop])\n",
    "            end = time.time()\n",
    "            print('DFT calculation took', end-start, 's')\n",
    "        else:\n",
    "            print('Beginning neural calculation:')\n",
    "            start = time.time()\n",
    "            print(predict_property_neural(prop=prop, mol_id=num))\n",
    "            end = time.time()\n",
    "            print('Neural method took', end-start, 's')\n",
    "        print('Actual data:')\n",
    "        print(lookup_property(prop=prop, mol_id=num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_user_for_calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "A\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Layer #0 (named \"edge_conditioned_conv\" in the current model) was found to correspond to layer edge_conditioned_conv_1 in the save file. However the new layer edge_conditioned_conv expects 4 weights, but the saved weights have 3 elements.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a2a4a3b25854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m133885\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_property_neural\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f5cc5128699f>\u001b[0m in \u001b[0;36mpredict_property_neural\u001b[0;34m(prop, mol_id)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_hard_parameter_sharing_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_model_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_complete\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmol_id\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mA_complete\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmol_id\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mE_complete\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmol_id\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   1221\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    695\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m                        \u001b[0;34m' weights, but the saved weights have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                        str(len(weight_values)) + ' elements.')\n\u001b[0m\u001b[1;32m    698\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer #0 (named \"edge_conditioned_conv\" in the current model) was found to correspond to layer edge_conditioned_conv_1 in the save file. However the new layer edge_conditioned_conv expects 4 weights, but the saved weights have 3 elements."
     ]
    }
   ],
   "source": [
    "properties = ['A', 'B', 'C', 'mu', 'homo', 'lumo', 'gap', 'zpve', 'u0', 'u298', 'h298', 'g298', 'cv']\n",
    "\n",
    "for prop in properties:\n",
    "    print(prop)\n",
    "    errors = list()\n",
    "    for index in np.random.choice(133885, 10):\n",
    "        pred = predict_property_neural(prop=prop, mol_id=index+1)\n",
    "        actual = lookup_property(prop=prop, mol_id=index+1)\n",
    "        err = abs((pred-actual)/actual*100)\n",
    "        print(err)\n",
    "        errors.append(err)\n",
    "    print('total err', sum(errors)/len(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "A\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Layer #0 (named \"edge_conditioned_conv_2\" in the current model) was found to correspond to layer edge_conditioned_conv_1 in the save file. However the new layer edge_conditioned_conv_2 expects 4 weights, but the saved weights have 3 elements.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-cfe3e37e1c11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m133885\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_property_neural\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f5cc5128699f>\u001b[0m in \u001b[0;36mpredict_property_neural\u001b[0;34m(prop, mol_id)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_hard_parameter_sharing_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_model_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_complete\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmol_id\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mA_complete\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmol_id\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mE_complete\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmol_id\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   1221\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/senior_research/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    695\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m                        \u001b[0;34m' weights, but the saved weights have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                        str(len(weight_values)) + ' elements.')\n\u001b[0m\u001b[1;32m    698\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer #0 (named \"edge_conditioned_conv_2\" in the current model) was found to correspond to layer edge_conditioned_conv_1 in the save file. However the new layer edge_conditioned_conv_2 expects 4 weights, but the saved weights have 3 elements."
     ]
    }
   ],
   "source": [
    "properties = ['A', 'B', 'C', 'mu', 'homo', 'lumo', 'gap', 'zpve', 'u0', 'u298', 'h298', 'g298', 'cv']\n",
    "\n",
    "for prop in properties:\n",
    "    print(prop)\n",
    "    errors = list()\n",
    "    for index in np.random.choice(133885, 10):\n",
    "        pred = predict_property_neural(prop=prop, mol_id=index+1)\n",
    "        actual = lookup_property(prop=prop, mol_id=index+1)\n",
    "        err = abs(pred-actual)\n",
    "        print(err)\n",
    "        errors.append(err)\n",
    "    print('avg err', sum(errors)/len(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}